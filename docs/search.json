[
  {
    "objectID": "weekly-notes.html",
    "href": "weekly-notes.html",
    "title": "WEEKLY NOTES",
    "section": "",
    "text": "09-08-2025 Notes: Course Introduction"
  },
  {
    "objectID": "weekly-notes.html#week-1-course-introduction",
    "href": "weekly-notes.html#week-1-course-introduction",
    "title": "WEEKLY NOTES",
    "section": "",
    "text": "09-08-2025 Notes: Course Introduction"
  },
  {
    "objectID": "weekly-notes.html#week-2-dplyr-basics-and-census-data",
    "href": "weekly-notes.html#week-2-dplyr-basics-and-census-data",
    "title": "WEEKLY NOTES",
    "section": "WEEK 2: DPLYR BASICS AND CENSUS DATA",
    "text": "WEEK 2: DPLYR BASICS AND CENSUS DATA\n09/15/2025 Notes: dplyr Basics and Census Data"
  },
  {
    "objectID": "weekly-notes.html#week-3-exploratory-data-analysis-eda-visualization",
    "href": "weekly-notes.html#week-3-exploratory-data-analysis-eda-visualization",
    "title": "WEEKLY NOTES",
    "section": "WEEK 3: EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "text": "WEEK 3: EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION\n09/22/2025 Notes: Exploratory Data Analysis (EDA) & Visualization"
  },
  {
    "objectID": "weekly-notes.html#week-4-spatial-data-gis-operations-in-r",
    "href": "weekly-notes.html#week-4-spatial-data-gis-operations-in-r",
    "title": "WEEKLY NOTES",
    "section": "WEEK 4: SPATIAL DATA & GIS OPERATIONS IN R",
    "text": "WEEK 4: SPATIAL DATA & GIS OPERATIONS IN R\n09/29/2025 Notes: Spatial Data & GIS Operations in R"
  },
  {
    "objectID": "weekly-notes.html#week-5-predictive-modeling-with-linear-regression",
    "href": "weekly-notes.html#week-5-predictive-modeling-with-linear-regression",
    "title": "WEEKLY NOTES",
    "section": "WEEK 5: PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "text": "WEEK 5: PREDICTIVE MODELING WITH LINEAR REGRESSION\n10/06/2025 Notes: Predictive Modeling with Linear Regression"
  },
  {
    "objectID": "weekly-notes.html#week-6-spatial-machine-learning-advanced-regression",
    "href": "weekly-notes.html#week-6-spatial-machine-learning-advanced-regression",
    "title": "WEEKLY NOTES",
    "section": "WEEK 6: SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "text": "WEEK 6: SPATIAL MACHINE LEARNING & ADVANCED REGRESSION\n10/13/2025 Notes: Spatial Machine Learning & Advanced Regression"
  },
  {
    "objectID": "weekly-notes.html#week-7-model-diagnostics-spatial-autocorrelation",
    "href": "weekly-notes.html#week-7-model-diagnostics-spatial-autocorrelation",
    "title": "WEEKLY NOTES",
    "section": "WEEK 7: MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "text": "WEEK 7: MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION\n10/20/2025 Notes: Model Diagnostics & Spatial Autocorrelation"
  },
  {
    "objectID": "weekly-notes.html#week-8-midterm-project",
    "href": "weekly-notes.html#week-8-midterm-project",
    "title": "WEEKLY NOTES",
    "section": "WEEK 8: MIDTERM PROJECT",
    "text": "WEEK 8: MIDTERM PROJECT\n10/27/2025: Midterm Project"
  },
  {
    "objectID": "weekly-notes.html#week-9-critical-perspectives-on-predictive-policing",
    "href": "weekly-notes.html#week-9-critical-perspectives-on-predictive-policing",
    "title": "WEEKLY NOTES",
    "section": "WEEK 9: CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "text": "WEEK 9: CRITICAL PERSPECTIVES ON PREDICTIVE POLICING\n11/03/2025 Notes: Critical Perspectives on Predictive Policing"
  },
  {
    "objectID": "weekly-notes.html#week-10-logistic-regression-for-binary-outcomes",
    "href": "weekly-notes.html#week-10-logistic-regression-for-binary-outcomes",
    "title": "WEEKLY NOTES",
    "section": "WEEK 10: LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "text": "WEEK 10: LOGISTIC REGRESSION FOR BINARY OUTCOMES\n11/10/2025 Notes: Logistic Regression for Binary Outcomes"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-notes.html#coding-techniques",
    "href": "weekly-notes/week-10/week-10-notes.html#coding-techniques",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-notes.html#questions-challenges",
    "href": "weekly-notes/week-10/week-10-notes.html#questions-challenges",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-notes.html#reflection",
    "href": "weekly-notes/week-10/week-10-notes.html#reflection",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Name: ________________________ Date: _______________\n\n\nPatient ID: ________\nTruth (Red dot?): ⃝ YES (has disease) ⃝ NO (no disease)\nModel Prediction (Probability): _______\nDeadly Variant? (Star): ⃝ YES ⃝ NO\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.50 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\nBased on where you went and your truth:\n⃝ True Positive (TP) - Correctly quarantined (have disease, high prob)\n⃝ False Positive (FP) - Incorrectly quarantined (no disease, high prob)\n⃝ True Negative (TN) - Correctly not quarantined (no disease, low prob)\n⃝ False Negative (FN) - Incorrectly not quarantined (have disease, low prob)\n\n\n\nWork with your table to count:\n           Predicted Positive   Predicted Negative\n           (Quarantined)       (Not Quarantined)\n\nActually \nPositive   TP = _______         FN = _______\n(Red Dot)\n\nActually\nNegative   FP = _______         TN = _______\n(No Dot)\n\n\n\nSensitivity (True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: What % of actually sick people did we catch? __________\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: What % of healthy people did we correctly not quarantine? __________\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: Of those quarantined, what % actually had disease? __________\nAccuracy: \\[\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}} = \\frac{\\_\\_\\_ + \\_\\_\\_}{48} = \\_\\_\\_\\_\\]\nInterpretation: What % did we classify correctly overall? __________\n\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.30 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\nDid your classification change from Round 1? ⃝ YES ⃝ NO\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______ (Did this go up ↑ or down ↓ from Round 1?)\nSpecificity: _______ (Did this go up ↑ or down ↓ from Round 1?)\nPrecision: _______\nAccuracy: _______\n\n\n\nHow many people with stars (deadly variant) were caught? - Round 1 (threshold 0.50): _______ - Round 2 (threshold 0.30): _______\nDid lowering the threshold help catch deadly variants? ⃝ YES ⃝ NO\n\n\n\n\n\n\n\nDecision rule: If probability ≥ 0.70 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______\nSpecificity: _______\nPrecision: _______\nAccuracy: _______\n\n\n\n\n\nFill in the table below with your results:\n\n\n\nMetric\nThreshold = 0.30\nThreshold = 0.50\nThreshold = 0.70\n\n\n\n\nSensitivity\n\n\n\n\n\nSpecificity\n\n\n\n\n\nPrecision\n\n\n\n\n\nFalse Positives (FP)\n\n\n\n\n\nFalse Negatives (FN)\n\n\n\n\n\n\n\n\n\nAs threshold increases (0.30 → 0.50 → 0.70), what happens to sensitivity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nAs threshold increases, what happens to specificity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nCan we maximize BOTH sensitivity and specificity at the same time?\n⃝ YES ⃝ NO\nWhat is the fundamental trade-off?\n\n\n\n\n\n\n\n\n\n\nIf you were a False Positive (quarantined when healthy):\nHow did it feel to be quarantined unnecessarily?\n\n\nWhat if this meant missing work for 2 weeks without pay? Or being unable to attend an important event?\n\n\nIf you were a False Negative (missed when actually sick):\nHow did it feel to be sent back when you actually had the disease?\n\n\nWhat if you had the deadly variant (star)? What are the consequences of being missed?\n\n\n\n\n\nScenario A: Rare, extremely deadly disease (like Ebola) - Disease is rare but 70% fatality rate - Treatment is available and effective if caught early - Quarantine costs $1,000/person, treatment costs $5,000\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\nScenario B: Common, mild illness (like common cold) - Disease is common but not serious (just annoying) - No treatment available - Quarantine means missing work (costs $2,000 in lost wages)\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\n\n\n\nWhich metric is MOST important for Scenario A (deadly disease)?\n⃝ Sensitivity - Don’t miss any sick people\n⃝ Specificity - Don’t quarantine healthy people\n⃝ Accuracy - Overall correct\n⃝ Precision - When we say “sick,” be sure\nWhy?\n\n\nWhich metric is MOST important for Scenario B (mild illness)?\n⃝ Sensitivity\n⃝ Specificity\n⃝ Accuracy\n⃝ Precision\nWhy?\n\n\n\n\n\nThink about COVID-19 testing. Rapid tests had lower sensitivity than PCR tests.\nWhen might you prefer a rapid test (lower sensitivity)?\n\n\nWhen would you insist on PCR test (higher sensitivity)?\n\n\n\n\n\nWhat if the model’s predictions were systematically wrong for certain groups?\nFor example: The model gives lower probabilities for women even when they have the disease.\nWhat would happen?\n\n\nHow could we detect this problem?\n\n\nWhat should we do about it?\n\n\n\n\n\n\n\nWrite 3 key things you learned from this exercise:\n\n\n\n\n\n\n\n\nOne question you still have:"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#your-patient-information",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#your-patient-information",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Patient ID: ________\nTruth (Red dot?): ⃝ YES (has disease) ⃝ NO (no disease)\nModel Prediction (Probability): _______\nDeadly Variant? (Star): ⃝ YES ⃝ NO"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-1-threshold-0.50",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-1-threshold-0.50",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.50 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\nBased on where you went and your truth:\n⃝ True Positive (TP) - Correctly quarantined (have disease, high prob)\n⃝ False Positive (FP) - Incorrectly quarantined (no disease, high prob)\n⃝ True Negative (TN) - Correctly not quarantined (no disease, low prob)\n⃝ False Negative (FN) - Incorrectly not quarantined (have disease, low prob)\n\n\n\nWork with your table to count:\n           Predicted Positive   Predicted Negative\n           (Quarantined)       (Not Quarantined)\n\nActually \nPositive   TP = _______         FN = _______\n(Red Dot)\n\nActually\nNegative   FP = _______         TN = _______\n(No Dot)\n\n\n\nSensitivity (True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: What % of actually sick people did we catch? __________\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: What % of healthy people did we correctly not quarantine? __________\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\_\\_\\_}{\\_\\_\\_ + \\_\\_\\_} = \\_\\_\\_\\_\\]\nInterpretation: Of those quarantined, what % actually had disease? __________\nAccuracy: \\[\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}} = \\frac{\\_\\_\\_ + \\_\\_\\_}{48} = \\_\\_\\_\\_\\]\nInterpretation: What % did we classify correctly overall? __________"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-2-threshold-0.30",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-2-threshold-0.30",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.30 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\nDid your classification change from Round 1? ⃝ YES ⃝ NO\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______ (Did this go up ↑ or down ↓ from Round 1?)\nSpecificity: _______ (Did this go up ↑ or down ↓ from Round 1?)\nPrecision: _______\nAccuracy: _______\n\n\n\nHow many people with stars (deadly variant) were caught? - Round 1 (threshold 0.50): _______ - Round 2 (threshold 0.30): _______\nDid lowering the threshold help catch deadly variants? ⃝ YES ⃝ NO"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-3-threshold-0.70",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#round-3-threshold-0.70",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Decision rule: If probability ≥ 0.70 → QUARANTINE\nWhere should you go? ⃝ Quarantine Table\n⃝ Stay at regular table\n\n\n\n           Predicted Positive   Predicted Negative\n\nActually \nPositive   TP = _______         FN = _______\n\nActually\nNegative   FP = _______         TN = _______\n\n\n\nSensitivity: _______\nSpecificity: _______\nPrecision: _______\nAccuracy: _______"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#comparison-across-thresholds",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#comparison-across-thresholds",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Fill in the table below with your results:\n\n\n\nMetric\nThreshold = 0.30\nThreshold = 0.50\nThreshold = 0.70\n\n\n\n\nSensitivity\n\n\n\n\n\nSpecificity\n\n\n\n\n\nPrecision\n\n\n\n\n\nFalse Positives (FP)\n\n\n\n\n\nFalse Negatives (FN)\n\n\n\n\n\n\n\n\n\nAs threshold increases (0.30 → 0.50 → 0.70), what happens to sensitivity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nAs threshold increases, what happens to specificity?\n⃝ Increases ⃝ Decreases ⃝ Stays the same\nCan we maximize BOTH sensitivity and specificity at the same time?\n⃝ YES ⃝ NO\nWhat is the fundamental trade-off?"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#reflection-questions",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#reflection-questions",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "If you were a False Positive (quarantined when healthy):\nHow did it feel to be quarantined unnecessarily?\n\n\nWhat if this meant missing work for 2 weeks without pay? Or being unable to attend an important event?\n\n\nIf you were a False Negative (missed when actually sick):\nHow did it feel to be sent back when you actually had the disease?\n\n\nWhat if you had the deadly variant (star)? What are the consequences of being missed?\n\n\n\n\n\nScenario A: Rare, extremely deadly disease (like Ebola) - Disease is rare but 70% fatality rate - Treatment is available and effective if caught early - Quarantine costs $1,000/person, treatment costs $5,000\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\nScenario B: Common, mild illness (like common cold) - Disease is common but not serious (just annoying) - No treatment available - Quarantine means missing work (costs $2,000 in lost wages)\nWhich threshold would you choose? ⃝ 0.30 ⃝ 0.50 ⃝ 0.70\nWhy?\n\n\n\n\n\n\nWhich metric is MOST important for Scenario A (deadly disease)?\n⃝ Sensitivity - Don’t miss any sick people\n⃝ Specificity - Don’t quarantine healthy people\n⃝ Accuracy - Overall correct\n⃝ Precision - When we say “sick,” be sure\nWhy?\n\n\nWhich metric is MOST important for Scenario B (mild illness)?\n⃝ Sensitivity\n⃝ Specificity\n⃝ Accuracy\n⃝ Precision\nWhy?\n\n\n\n\n\nThink about COVID-19 testing. Rapid tests had lower sensitivity than PCR tests.\nWhen might you prefer a rapid test (lower sensitivity)?\n\n\nWhen would you insist on PCR test (higher sensitivity)?\n\n\n\n\n\nWhat if the model’s predictions were systematically wrong for certain groups?\nFor example: The model gives lower probabilities for women even when they have the disease.\nWhat would happen?\n\n\nHow could we detect this problem?\n\n\nWhat should we do about it?"
  },
  {
    "objectID": "weekly-notes/week-10/student_worksheet_classification_exercise.html#key-takeaways",
    "href": "weekly-notes/week-10/student_worksheet_classification_exercise.html#key-takeaways",
    "title": "Disease Testing Exercise - Student Worksheet",
    "section": "",
    "text": "Write 3 key things you learned from this exercise:\n\n\n\n\n\n\n\n\nOne question you still have:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "",
    "text": "Some midterm teams suggested:\n\n“To improve predictions, implement a spatial lag model (SAR) or spatial error model (SEM)”\n\nLet’s revisit why this doesn’t work for prediction\n\n(And what you should recommend instead)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#quick-clarification",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#quick-clarification",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "",
    "text": "Some midterm teams suggested:\n\n“To improve predictions, implement a spatial lag model (SAR) or spatial error model (SEM)”\n\nLet’s revisit why this doesn’t work for prediction\n\n(And what you should recommend instead)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#first-what-is-a-spatial-lag-model",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#first-what-is-a-spatial-lag-model",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "First: What IS a Spatial Lag Model?",
    "text": "First: What IS a Spatial Lag Model?\nStandard regression: \\[\\text{Price}_i = \\beta_0 + \\beta_1(\\text{Sqft}) + \\beta_2(\\text{Beds}) + \\varepsilon\\]\nSpatial lag regression: \\[\\text{Price}_i = \\beta_0 + \\rho \\times \\color{red}{\\text{Avg(Neighbor Prices)}} + \\beta_1(\\text{Sqft}) + \\beta_2(\\text{Beds}) + \\varepsilon\\]\n\nKey difference: Your price depends on your neighbors’ actual prices\nQuestion: “Do nearby house prices affect each other?” (spillover effects)\nUsed for: Understanding spatial processes, causal inference about neighborhood effects"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-problem-for-prediction",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-problem-for-prediction",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "The Problem for Prediction",
    "text": "The Problem for Prediction\nLet’s work through two concrete scenarios where this breaks down:\n\nTemporal: Training on 2024, predicting 2025\nTransfer: Philadelphia model → Orlando"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-1-estimate-model-on-2024-data",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-1-estimate-model-on-2024-data",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Step 1: Estimate Model on 2024 Data",
    "text": "Step 1: Estimate Model on 2024 Data\nYou fit this spatial lag model on 2024 Philadelphia sales:\n# Your estimated model from 2024\nmodel_2024 &lt;- spatialreg::lagsarlm(\n  log(price) ~ sqft + bedrooms + bathrooms,\n  data = sales_2024,\n  listw = neighbors_weights\n)\nResults:\nSpatial lag coefficient (ρ) = 0.65\nβ_sqft = 0.00015\nβ_beds = 0.12\n\nInterpretation: A 1% increase in neighbors’ prices → 0.65% increase in my price\nThis works for 2024 because all prices are known!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-2-three-houses-list-in-january-2025",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-2-three-houses-list-in-january-2025",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Step 2: Three Houses List in January 2025",
    "text": "Step 2: Three Houses List in January 2025\nYour prediction task:\n\n\n\nHouse\nSqft\nBeds\nBaths\n5 Nearest Neighbors\n\n\n\n\nA\n1,500\n3\n2\nB, C, D, E, F\n\n\nB\n1,800\n3\n2\nA, C, G, H, I\n\n\nC\n2,000\n4\n3\nA, B, J, K, L\n\n\n\n\nWhat you know:\n\n✓ Sqft, beds, baths for A, B, C\n✓ Locations of A, B, C\n✗ What A, B, C will actually sell for"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-3-try-to-predict-house-a",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-3-try-to-predict-house-a",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Step 3: Try to Predict House A",
    "text": "Step 3: Try to Predict House A\nYour model equation: \\[\\text{Price}_A = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_B, \\text{Price}_C, ...)} + 0.00015 \\times 1500 + 0.12 \\times 3\\]\n\nProblem: You need PriceB and PriceC to predict PriceA\n\n\nBut wait… PriceB and PriceC haven’t happened yet! They’re listed, not sold."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-4-realize-the-circular-dependency",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#step-4-realize-the-circular-dependency",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Step 4: Realize the Circular Dependency",
    "text": "Step 4: Realize the Circular Dependency\nTry to predict House B: \\[\\text{Price}_B = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_A, \\text{Price}_C, ...)} + ...\\]\nTry to predict House C: \\[\\text{Price}_C = \\beta_0 + 0.65 \\times \\color{red}{\\text{Avg}(\\text{Price}_A, \\text{Price}_B, ...)} + ...\\]\n\nPrice_A needs Price_B and Price_C\nPrice_B needs Price_A and Price_C  \nPrice_C needs Price_A and Price_B\nYou’re stuck in a circular dependency!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#visual-the-circular-dependency",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#visual-the-circular-dependency",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Visual: The Circular Dependency",
    "text": "Visual: The Circular Dependency\n\n\n\n\n\ngraph TD\n    A[House A&lt;br/&gt;Need to predict] --&gt;|needs price of| B[House B&lt;br/&gt;Need to predict]\n    B --&gt;|needs price of| C[House C&lt;br/&gt;Need to predict]\n    C --&gt;|needs price of| A\n    \n    style A fill:#e74c3c,stroke:#c0392b,color:#fff\n    style B fill:#e74c3c,stroke:#c0392b,color:#fff\n    style C fill:#e74c3c,stroke:#c0392b,color:#fff\n\n\n\n\n\n\nAll the unknowns depend on each other!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#but-cant-i-use-recent-sales",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#but-cant-i-use-recent-sales",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "“But Can’t I Use Recent Sales?”",
    "text": "“But Can’t I Use Recent Sales?”\nYou might think: “I’ll use recent sales from December 2024 as the spatial lag”\n\n\nProblem 1: House A’s neighbors (B, C) haven’t sold - they’re ALSO new listings\nProblem 2: If you use OLD sales (from months ago), you’re predicting based on stale prices in a changing market\nProblem 3: What if it’s a new development? No recent sales exist nearby\nProblem 4: Your spatial lag coefficient (ρ = 0.65) was estimated assuming SIMULTANEOUS prices, not lagged prices\n\n\n\nBottom line: Spatial lag models assume all observations exist simultaneously. Prediction is inherently sequential."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#your-sales-pitch-to-orlando",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#your-sales-pitch-to-orlando",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Your Sales Pitch to Orlando",
    "text": "Your Sales Pitch to Orlando\nYou: “We built an amazing spatial lag model for Philadelphia! R² = 0.85!”\nOrlando Chief Data Officer: “Great! We have 5,000 active listings. Can you predict their prices?”\n\nYou: “Sure! Just send me the data…”\n(You open the file)\norlando_listings &lt;- read_csv(\"orlando_new_listings.csv\")\n# Variables: address, sqft, beds, baths, lat, lon\n# Missing: SALE_PRICE (that's what we're predicting!)\n\n\nYou realize: “Wait… I need neighbors’ prices to predict each price…”\nOrlando: “That’s why we hired you - these HAVEN’T sold yet!”"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-parameter-transfer-problem",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-parameter-transfer-problem",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "The Parameter Transfer Problem",
    "text": "The Parameter Transfer Problem\nEven if you had some recent Orlando sales to use:\nYour Philadelphia model: \\[\\text{Price}_i = \\beta_0 + \\color{red}{0.65} \\times \\text{Avg}(\\text{Neighbor Prices}) + \\beta_1(\\text{Sqft}) + ...\\]\n\nQuestions:\n\nWas ρ = 0.65 estimated on Philadelphia’s price distribution (avg $350k)\nOrlando’s average is $280k - different scale\nOrlando is sprawling suburbs vs. Philadelphia’s dense rowhouses\nDoes ρ = 0.65 even mean the same thing in Orlando?\n\n\n\nAnswer: No! You’d have to re-estimate the entire model on Orlando data.\nSo your “model” isn’t actually transferable."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#contrast-spatial-features-transfer",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#contrast-spatial-features-transfer",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Contrast: Spatial Features Transfer",
    "text": "Contrast: Spatial Features Transfer\nIf you had built with spatial FEATURES:\n# Philadelphia model\nmodel &lt;- lm(log(price) ~ sqft + bedrooms + \n              dist_to_transit + parks_500ft + \n              median_income_tract,\n            data = philly)\n\nTo use in Orlando:\n# Get Orlando's spatial context (all observable!)\norlando$dist_to_transit &lt;- get_transit_distance(orlando)\norlando$parks_500ft &lt;- count_parks_buffer(orlando, 500)\norlando$median_income_tract &lt;- get_census_data(orlando)\n\n# Apply Philadelphia coefficients\norlando$predicted_price &lt;- predict(model, newdata = orlando)\nThis works because features are CONTEXT, not outcomes!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#visual-comparison",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#visual-comparison",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Visual Comparison",
    "text": "Visual Comparison\n\n\nSpatial Lag (Fails)\n\n\n\n\n\ngraph TD\n    A[Predict&lt;br/&gt;House A] --&gt;|needs| B[Price of B&lt;br/&gt;UNKNOWN]\n    A --&gt;|needs| C[Price of C&lt;br/&gt;UNKNOWN]\n    \n    style A fill:#e74c3c\n    style B fill:#e74c3c\n    style C fill:#e74c3c\n\n\n\n\n\n\nCircular dependency\n\nSpatial Features (Works)\n\n\n\n\n\ngraph TD\n    T[Transit: 0.3mi&lt;br/&gt;KNOWN] --&gt; A[Predict&lt;br/&gt;House A]\n    P[Parks: 2&lt;br/&gt;KNOWN] --&gt; A\n    I[Income: 65k&lt;br/&gt;KNOWN] --&gt; A\n    \n    style A fill:#27ae60\n    style T fill:#3498db\n    style P fill:#3498db\n    style I fill:#3498db\n\n\n\n\n\n\nAll inputs observable"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#so-when-are-spatial-lag-models-useful",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#so-when-are-spatial-lag-models-useful",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "So When ARE Spatial Lag Models Useful?",
    "text": "So When ARE Spatial Lag Models Useful?\nSpatial lag models are GREAT for:\n\n\nUnderstanding spillover effects: “Does gentrification in one neighborhood cause price increases in adjacent neighborhoods?”\nCausal inference: “Do nearby foreclosures depress my home value?”\nPolicy simulation: “If we build a park here, how will it affect the surrounding area?”\nCross-sectional analysis: Looking at ONE point in time where all prices exist\n\n\n\nBut NOT for:\n\nPredicting future sales\nTransferring models between cities\nReal-time valuation systems\nOut-of-sample forecasting"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#what-high-morans-i-actually-tells-you",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#what-high-morans-i-actually-tells-you",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "What High Moran’s I Actually Tells You",
    "text": "What High Moran’s I Actually Tells You\nIf your model errors have Moran’s I = 0.58 (high spatial clustering):\n\n❌ Wrong response: “Switch to spatial lag model”\n✓ Right response: “Add better spatial features!”"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#instead-of-implement-a-spatial-lag-model",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#instead-of-implement-a-spatial-lag-model",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Instead of:** “Implement a spatial lag model”",
    "text": "Instead of:** “Implement a spatial lag model”\nSay this:\n\n“The high Moran’s I (0.58) indicates spatial clustering in errors, suggesting our model is missing important location-based predictors. Recommendations:\n\nVary buffer distances - Currently using 500ft; try 250ft, 1000ft, 1500ft\nAdd more amenities - Coffee shops, grocery stores, restaurants, crime incidents\nRicher census data - Use block group instead of tract; add commute time variables\nNeighborhood interactions - sqft × neighborhood, age × distance_downtown\nTime-varying features - Recent building permits, development activity\nMore granular fixed effects - Census block group FE instead of neighborhood FE”"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-key-distinction",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#the-key-distinction",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "The Key Distinction",
    "text": "The Key Distinction\n\n\nSPATIAL FEATURES (What you observe about a location)\n\nDistance to transit\nParks within buffer\nMedian income\nCrime rate\nSchool quality\nWalkability score\n\nFor prediction: ✓ Always observable\n\nSPATIAL LAG (What neighbors’ outcomes are)\n\nAverage neighbor price\nNeighbor sale date\nNeighbor appreciation rate\n\nFor prediction: ✗ Creates circular dependency\n\n\n\nFix spatial autocorrelation by improving FEATURES, not by changing model structure"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#machine-learning-doesnt-fix-this",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#machine-learning-doesnt-fix-this",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Machine Learning Doesn’t Fix This",
    "text": "Machine Learning Doesn’t Fix This\nSome might think: “I’ll use a Random Forest with neighbors’ prices as a feature!”\n\n# This STILL doesn't work for prediction\nrf_model &lt;- randomForest(\n  price ~ sqft + bedrooms + avg_neighbor_price, # ⚠️ \n  data = train_2024\n)\n\n# Try to predict 2025\npredictions_2025 &lt;- predict(rf_model, newdata = new_listings)\n#                                      ↑\n#                               avg_neighbor_price is MISSING!\n\n\nThe problem isn’t the model TYPE, it’s the LOGIC:\nIf a feature requires knowing other predictions first, it’s not a valid predictor."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-spatial-lag-problem.html#final-thought",
    "href": "weekly-notes/week-09/week-09-spatial-lag-problem.html#final-thought",
    "title": "WHY SPATIAL LAG MODELS DON’T WORK FOR PREDICTING",
    "section": "Final Thought",
    "text": "Final Thought\nAlways understand what a method is designed for before recommending it."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09/week-09-notes.html#coding-techniques",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09/week-09-notes.html#questions-challenges",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09/week-09-notes.html#reflection",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07/week-07-notes.html#coding-techniques",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07/week-07-notes.html#questions-challenges",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07/week-07-notes.html#reflection",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html",
    "href": "weekly-notes/week-06/week-06-notes.html",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "",
    "text": "Code\n# Load packages and data\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# Load Boston housing data\nboston &lt;- read_csv(here(\"data/boston.csv\"))\n\n# Quick look at the data\nglimpse(boston)\n\n\nRows: 1,485\nColumns: 24\n$ ...1       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ Parcel_No  &lt;dbl&gt; 100032000, 100058000, 100073000, 100112000, 100137000, 1001…\n$ SalePrice  &lt;dbl&gt; 450000, 600000, 450000, 670000, 260000, 355000, 665000, 355…\n$ PricePerSq &lt;dbl&gt; 228.89, 164.34, 105.98, 291.94, 217.21, 190.96, 227.35, 120…\n$ LivingArea &lt;dbl&gt; 1966, 3840, 4246, 2295, 1197, 1859, 2925, 2904, 892, 1916, …\n$ Style      &lt;chr&gt; \"Conventional\", \"Semi?Det\", \"Decker\", \"Row\\xa0End\", \"Coloni…\n$ GROSS_AREA &lt;dbl&gt; 3111, 5603, 6010, 3482, 1785, 2198, 4341, 3892, 1658, 3318,…\n$ NUM_FLOORS &lt;dbl&gt; 2.0, 3.0, 3.0, 3.0, 2.0, 1.5, 3.0, 3.0, 2.0, 2.0, 1.5, 2.0,…\n$ R_BDRMS    &lt;dbl&gt; 4, 8, 9, 6, 2, 3, 8, 6, 2, 2, 4, 3, 3, 3, 6, 8, 4, 4, 3, 2,…\n$ R_FULL_BTH &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 1, 2,…\n$ R_HALF_BTH &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ R_KITCH    &lt;dbl&gt; 2, 3, 3, 3, 1, 2, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 2, 2,…\n$ R_AC       &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\",…\n$ R_FPLACE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ LU         &lt;chr&gt; \"R2\", \"R3\", \"R3\", \"R3\", \"R1\", \"R2\", \"R3\", \"E\", \"R1\", \"R2\", …\n$ OWN_OCC    &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\",…\n$ R_BLDG_STY &lt;chr&gt; \"CV\", \"SD\", \"DK\", \"RE\", \"CL\", \"CV\", \"DK\", \"DK\", \"RE\", \"TF\",…\n$ R_ROOF_TYP &lt;chr&gt; \"H\", \"F\", \"F\", \"F\", \"F\", \"G\", \"F\", \"F\", \"G\", \"F\", \"G\", \"F\",…\n$ R_EXT_FIN  &lt;chr&gt; \"M\", \"B\", \"M\", \"M\", \"P\", \"M\", \"M\", \"A\", \"A\", \"M\", \"W\", \"W\",…\n$ R_TOTAL_RM &lt;dbl&gt; 10, 17, 20, 14, 5, 8, 14, 14, 4, 9, 7, 7, 5, 5, 15, 14, 11,…\n$ R_HEAT_TYP &lt;chr&gt; \"W\", \"W\", \"W\", \"W\", \"E\", \"E\", \"W\", \"W\", \"W\", \"W\", \"W\", \"W\",…\n$ YR_BUILT   &lt;dbl&gt; 1900, 1910, 1910, 1905, 1860, 1905, 1900, 1890, 1900, 1900,…\n$ Latitude   &lt;dbl&gt; 42.37963, 42.37877, 42.37940, 42.38014, 42.37967, 42.37953,…\n$ Longitude  &lt;dbl&gt; -71.03076, -71.02943, -71.02846, -71.02859, -71.02903, -71.…\n\n\n\n\nCode\n# Simple model: Predict price from living area\n# SalePrice is y (response) and LivingArea is x (predictor).\nbaseline_model &lt;- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,    Adjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation\n\nIntercept’s Estimate: Base Sale Price at 0 sq. ft. is ~$157,968.32. Not useful in practice.\nLiving Area’s Estimate: Each additional square foot of Living Area adds ~$216 to Sale Price.\nRelationship is statistically significant (p &lt; 0.001). If p is small, reject the null hypothesis.\nR^2 is 0.13, so 13% of variation is explained by regression model output.\n\nMost of the 87% variation is unexplained, so what’s the problem?\n\nLimitations\n\nIgnores location (North End vs. Roxbury vs. Back Bay); proximity to downtown, waterfront, parks, etc.; nearby crime levels; school quality; neighborhood characteristics.\nMight fail because 1,000 sq ft in Back Bay is not equivalent to 1,000 sq ft in Roxbury; same house, different locations, means very different prices; location!\nCould improve by adding spatial features like crime and distance to amenities; control for neighborhood (fixed effects); include interactions (does size matter more in wealthy areas)?\nJust doing a linear regression is a limit, so that’s where spatial features come in.\n\n\n\n\n\n\n\nCode\n# Add number of bathrooms\nbetter_model &lt;- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\nsummary(better_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-783019 -230756  -65737   75367 9193133 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 107683.96   37340.88   2.884  0.00399 ** \nLivingArea     145.68      21.33   6.828 1.25e-11 ***\nR_FULL_BTH  106978.13   23800.30   4.495 7.50e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 560200 on 1482 degrees of freedom\nMultiple R-squared:  0.1429,    Adjusted R-squared:  0.1418 \nF-statistic: 123.6 on 2 and 1482 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# Compare models\ncat(\"Baseline R²:\", summary(baseline_model)$r.squared, \"\\n\")\n\n\nBaseline R²: 0.1312636 \n\n\n\n\nCode\ncat(\"With bathrooms R²:\", summary(better_model)$r.squared, \"\\n\")\n\n\nWith bathrooms R²: 0.1429474 \n\n\n\n\nCode\n# Convert boston data to sf object\nboston.sf &lt;- boston %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n# Check it worked\nhead(boston.sf)\n\n\nSimple feature collection with 6 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 238643.6 ymin: 903246 xmax: 238833.2 ymax: 903399.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 23\n   ...1 Parcel_No SalePrice PricePerSq LivingArea Style    GROSS_AREA NUM_FLOORS\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1     1 100032000    450000       229.       1966 \"Conven…       3111        2  \n2     2 100058000    600000       164.       3840 \"Semi?D…       5603        3  \n3     3 100073000    450000       106.       4246 \"Decker\"       6010        3  \n4     4 100112000    670000       292.       2295 \"Row\\xa…       3482        3  \n5     5 100137000    260000       217.       1197 \"Coloni…       1785        2  \n6     6 100138000    355000       191.       1859 \"Conven…       2198        1.5\n# ℹ 15 more variables: R_BDRMS &lt;dbl&gt;, R_FULL_BTH &lt;dbl&gt;, R_HALF_BTH &lt;dbl&gt;,\n#   R_KITCH &lt;dbl&gt;, R_AC &lt;chr&gt;, R_FPLACE &lt;dbl&gt;, LU &lt;chr&gt;, OWN_OCC &lt;chr&gt;,\n#   R_BLDG_STY &lt;chr&gt;, R_ROOF_TYP &lt;chr&gt;, R_EXT_FIN &lt;chr&gt;, R_TOTAL_RM &lt;dbl&gt;,\n#   R_HEAT_TYP &lt;chr&gt;, YR_BUILT &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\nCode\nclass(boston.sf)  # Should show \"sf\" and \"data.frame\"\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nCode\n# Load neighborhood boundaries\nnhoods &lt;- read_sf(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %&gt;%\n  st_transform('ESRI:102286')  # Match CRS!\n\n# Check the neighborhoods\nhead(nhoods)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229049.1 ymin: 890856 xmax: 236590.9 ymax: 900302.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 8\n  sqmiles name         neighborhood_id  acres SHAPE__Length objectid SHAPE__Area\n    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\n1    2.51 Roslindale   15              1606.         53564.       53   69938273.\n2    3.94 Jamaica Pla… 11              2519.         56350.       54  109737890.\n3    0.55 Mission Hill 13               351.         17919.       55   15283120.\n4    0.29 Longwood     28               189.         11909.       56    8215904.\n5    0.04 Bay Village  33                26.5         4651.       57    1156071.\n6    0.02 Leather Dis… 27                15.6         3237.       58     681272.\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n\nCode\nnrow(nhoods)  # How many neighborhoods?\n\n\n[1] 26\n\n\n\n\nCode\n# Spatial join: Assign each house to its neighborhood\nboston.sf &lt;- boston.sf %&gt;%\n  st_join(nhoods, join = st_intersects)\n\n# Check results\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(`name`) %&gt;%\n  arrange(desc(n))\n\n\n# A tibble: 19 × 2\n   name              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 Dorchester      344\n 2 West Roxbury    242\n 3 Hyde Park       152\n 4 East Boston     149\n 5 Roslindale      142\n 6 Jamaica Plain   113\n 7 South Boston     80\n 8 Charlestown      65\n 9 Mattapan         65\n10 Roxbury          63\n11 South End        20\n12 Beacon Hill      17\n13 Mission Hill     14\n14 Allston           6\n15 Brighton          6\n16 Back Bay          3\n17 Fenway            2\n18 Bay Village       1\n19 Downtown          1\n\n\n\nWhy do you think certain neighborhoods command higher prices? Proximity to downtown? Historical character? School quality? Safety? All of the above? This is why we need spatial features and neighborhood controls!\n\n\n\n\n\n\n\nContinuous Variables\n\nSquare footage\nAge of house\nIncome levels\nDistance to downtown\n\nCategorical Variables\n\nNeighborhood\n\nn-1 Rule: One neighborhood is chosen as the reference category (omitted). R picks the first alphabetically unless specified otherwise.\n\nSchool district\nBuilding type\nHas garage\n\n\n\n\nCode\n# Ensure name is a factor\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(name = as.factor(name))\n\n# Check which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\n\n\n[1] \"Allston\"\n\n\n\n\nCode\n# Fit model with neighborhood fixed effects\nmodel_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, \n                          data = boston.sf)\n\n# Show just first 10 coefficients\nsummary(model_neighborhoods)$coef[1:10, ]\n\n\n                    Estimate   Std. Error    t value      Pr(&gt;|t|)\n(Intercept)      704306.0751 9.022210e+04  7.8063584  1.112892e-14\nLivingArea          138.3206 6.468186e+00 21.3847638  1.605175e-88\nnameBack Bay    7525585.0560 1.533714e+05 49.0677253 1.522921e-311\nnameBay Village 1284057.5326 2.320256e+05  5.5341206  3.700442e-08\nnameBeacon Hill 1767805.2297 1.020211e+05 17.3278366  2.465874e-61\nnameBrighton    -168941.2113 1.239972e+05 -1.3624597  1.732623e-01\nnameCharlestown   -2556.8667 9.208989e+04 -0.0277649  9.778534e-01\nnameDorchester  -576819.4858 8.846990e+04 -6.5199517  9.653393e-11\nnameDowntown      58553.1237 2.322150e+05  0.2521505  8.009601e-01\nnameEast Boston -534060.4110 8.962950e+04 -5.9585340  3.182615e-09\n\n\n\nInterperetation\n\nReference Category: Allston was automatically chosen as the intercept because it’s the first alphabetically.\nStructural Variables\n\nLiving Area: Each additional sq. ft. adds this amount (same for all neighborhoods).\nBedrooms: Effect of one or more full bathroom (same for all neighborhoods).\n\nNeighborhood Dummies\n\nPositive Coefficient: This neighborhood is more expensive than Allston.\nNegative Coefficient: This neighborhood is less expensive than Allston.\nAll other variables constant and held equal, same size and same bathrooms.\n\nHouse A in Back Bay\n\nLiving Area: 1,500 sq. ft.\nBaths: 2\nNeighborhood: Back Bay\nPredicted Price: $8,458,873\n\nHouse B in Roxbury\n\nLiving Area: 1,500 sq. ft.\nBaths: 2\nNeighborhood: Roxbury\nPredicted Price: $311,066\n\nNeighborhood Effect Price Difference: $8,127,807\n\nSame house, different location, but huge price difference. This is what neighborhood dummies capture.\n\nR automatically handles dummy variables as booleans, so 1 is true for Back Bay and 0 is false for Back Bay when creating neighborhood variables.\n\n\n\n\n\n\n\n\n\n\nThe Question: Doess the effect of one variable depend on the level of another variable?\n\nExample Scenarios\n\nHousing: Does square footage matter more in wealthy neighborhoods?\nEducation: Do tutoring effects vary by initial skill level?\nPublic Health: Do pollution effects differ by age?\n\nExample for this study: Is the value of square footage the same across all Boston neighborhoods?\n\nSalePrice = β₀ + β₁(LivingArea) + β₂(WealthyNeighborhood) + β₃(LivingArea × WealthyNeighborhood) + ε\n\n\nTheory: Luxury Premium Hypothesis\n\nIn Wealthy Neighborhoods (Back Bay, Beacon Hill, South End)\n\nHigh-end buyers pay premium for space.\nLuxury finishes, location prestige.\nEach sq. ft. adds substantial value.\nSteep slope.\nHypothesis: $300+ per sq. ft.\n\nIn Working-Class Neighborhoods (Dorchester, Mattapan, East Boston)\n\nBuyers value function over luxury.\nMore price-sensitive market.\nSpace matters, but less premium.\nFlatter slope.\nHypothesis: $100-150 per sq. ft.\n\nKey Question: If we assume one slope for all neighborhoods, are we misunderstanding the market?\n\n\n\n\nCode\n# Define wealthy neighborhoods based on median prices\nwealthy_hoods &lt;- c(\"Back Bay\", \"Beacon Hill\", \"South End\", \"Bay Village\")\n\n# Create binary indicator\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n# Check the split\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n# A tibble: 2 × 2\n  wealthy_neighborhood     n\n  &lt;fct&gt;                &lt;int&gt;\n1 Not Wealthy           1444\n2 Wealthy                 41\n\n\nModel 1: No Interaction (Parallel Slopes)\n\n\nCode\n# Model assumes same slope everywhere\nmodel_no_interact &lt;- lm(SalePrice ~ LivingArea + wealthy_neighborhood, \n                        data = boston.sf)\n\nsummary(model_no_interact)$coef\n\n\n                                Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)                  213842.7642 23893.619394  8.949785  1.039392e-18\nLivingArea                      160.2357     9.713346 16.496442  2.936087e-56\nwealthy_neighborhoodWealthy 2591012.0471 59958.794614 43.213211 1.103579e-264\n\n\n\nAssumes living area has the same effect in all neighborhoods. Only the intercept differs (wealthy areas start higher). Parallel lines on a plot.\n\nModel 2: With Interaction (Different Slopes)\n\n\nCode\n# Model allows different slopes\nmodel_interact &lt;- lm(SalePrice ~ LivingArea * wealthy_neighborhood, \n                     data = boston.sf)\n\nsummary(model_interact)$coef\n\n\n                                            Estimate   Std. Error   t value\n(Intercept)                             358542.41696 1.863997e+04 19.235144\nLivingArea                                  95.63947 7.620382e+00 12.550483\nwealthy_neighborhoodWealthy            -377937.38372 1.005554e+05 -3.758498\nLivingArea:wealthy_neighborhoodWealthy     985.12500 2.975904e+01 33.103383\n                                            Pr(&gt;|t|)\n(Intercept)                             8.875710e-74\nLivingArea                              2.079700e-34\nwealthy_neighborhoodWealthy             1.775774e-04\nLivingArea:wealthy_neighborhoodWealthy 2.445570e-180\n\n\n\nAllows living are to have different effects in different neighborhoods. Both intercept and slope differ. Non-parallel lines on a plot.\nWe get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive at what sq. ft. = 384 (358,542 / 985).\n\nNot Wealthy Areas Equation: \\(Price = 358,542 + 96 × LivingArea\\)\n\nInterpretation is at base price $358,542, each sq. ft. adds $96.\n\nWealthy Areas Equation: \\(Price = -19,395 + 1,081 × LivingArea\\)\n\nInterpretation is at base price -$19,395, each sq. ft. adds $1,081.\n\nThe Interaction Effect. Wealthy areas value each sq ft $985 more than non-wealthy areas!\n\nKey Observation: The lines are NOT parallel when plotted - that’s the interaction!\n\nComparing Model Performance\n\n\nCode\n# Compare R-squared\ncat(\"Model WITHOUT interaction R²:\", round(summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITHOUT interaction R²: 0.6156 \n\n\n\n\nCode\ncat(\"Model WITH interaction R²:\", round(summary(model_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITH interaction R²: 0.7791 \n\n\n\n\nCode\ncat(\"Improvement:\", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.1635 \n\n\n\nModel Improvement: Adding the interaction improves R² by 0.1635 (a 26.6% relative improvement)\n\nInterpretation: We explain 16.35% more variation in prices by allowing different slopes!\n\n\n\n\n\n\nSigns of Non-Linearity:\n\nCurved residual plots.\nDiminishing returns.\nAccelerating effects.\nU-shaped or inverted-U patterns.\nTheoretical reasons.\n\nExamples:\n\nHouse Age: Depreciation, then vintage premium.\nTest Scores: Plateau after studying.\nAdvertising: Diminishing returns.\nCrime Prevention: Early gains, then plateaus.\n\nPolynomial Regression\n\nSalePrice = β₀ + β₁(Age) + β₂(Age²) + ε\n\nAllows for curved relationship.\n\n\nAge’s Non-Linear Effect\n\nNew Houses (0-20 Years)\n\nModern amenities.\nMove-in ready.\nNo repairs needed.\nHigh value.\nSteep depreciation initially.\n\nMiddle-Aged (20-80 Years)\n\nNeeds updates.\nWear and tear.\nNot yet “historic”.\nLowest value.\nTrough of the curve.\n\nHistoric/Vintage (80+ Years)\n\nArchitectural character.\nHistoric districts.\nPrestige value.\nRising value.\n“Vintage premium”.\n\n\nBoston’s Context: The city has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?\n\n\n\nCode\n# Calculate age from year built\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2025 - YR_BUILT)%&gt;% filter(Age &lt;2000)\n\n\n# Check the distribution of age\nsummary(boston.sf$Age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    95.0   115.0   108.1   125.0   223.0 \n\n\n\n\nCode\n# Visualize age distribution\nggplot(boston.sf, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Distribution of House Age in Boston\",\n       x = \"Age (years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLinear Model (Baseline)\n\n\nCode\n# Simple linear relationship\nmodel_age_linear &lt;- lm(SalePrice ~ Age + LivingArea, data = boston.sf)\n\nsummary(model_age_linear)$coef\n\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -120808.8738 57836.98918 -2.088782 3.689911e-02\nAge            2834.0198   497.72013  5.694003 1.496246e-08\nLivingArea      202.8312    15.10373 13.429214 7.228187e-39\n\n\n\nEach additional year of age changes price by $2834.01 (assumed constant rate).\n\nAdd Polynomial Term: Age Squared\n\n\nCode\n# Quadratic model (Age²)\nmodel_age_quad &lt;- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)\n\nsummary(model_age_quad)$coef\n\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 570397.14406 97894.237962  5.826667 6.938345e-09\nAge         -13007.51918  1896.446156 -6.858892 1.019524e-11\nI(Age^2)        80.88988     9.360652  8.641479 1.425208e-17\nLivingArea     203.75007    14.739041 13.823835 5.975020e-41\n\n\n\nImportant: The I() Function Why I(Age^2) instead of just Age^2? In R formulas, ^ has special meaning. I() tells R: “interpret this literally, compute Age²” Without I(): R would interpret it differently in the formula.\nModel Equation: \\(Price = 570,397 + - 13,008 × Age + 80 × Age² + 204 × LivingArea\\)\n\nWarning: Can’t interpret coefficients directly. With Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of \\(Age = β₁ + 2 × β₂ × Age\\) This means the effect changes at every age!\n\n\nCompare Models\n\n\nCode\n# R-squared comparison\nr2_linear &lt;- summary(model_age_linear)$r.squared\nr2_quad &lt;- summary(model_age_quad)$r.squared\n\ncat(\"Linear model R²:\", round(r2_linear, 4), \"\\n\")\n\n\nLinear model R²: 0.1549 \n\n\n\n\nCode\ncat(\"Quadratic model R²:\", round(r2_quad, 4), \"\\n\")\n\n\nQuadratic model R²: 0.1959 \n\n\n\n\nCode\ncat(\"Improvement:\", round(r2_quad - r2_linear, 4), \"\\n\\n\")\n\n\nImprovement: 0.0409 \n\n\n\n\nCode\n# F-test: Is the Age² term significant?\nanova(model_age_linear, model_age_quad)\n\n\nAnalysis of Variance Table\n\nModel 1: SalePrice ~ Age + LivingArea\nModel 2: SalePrice ~ Age + I(Age^2) + LivingArea\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1469 4.5786e+14                                   \n2   1468 4.3569e+14  1 2.2163e+13 74.675 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck Residual Plot\n\n\nCode\n# Compare residual plots\npar(mfrow = c(1, 2))\n\n# Linear model residuals\nplot(fitted(model_age_linear), residuals(model_age_linear),\n     main = \"Linear Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Quadratic model residuals  \nplot(fitted(model_age_quad), residuals(model_age_quad),\n     main = \"Quadratic Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Everything is related to everything else, but near things are more related than distant things.”\n- Waldo Tobler, 1970\n\nFor house prices, crime nearby matters more than crime across the city.\nParks within walking distance affect value.\nImmediate neighborhood defines market.\n\n\n\n\n\n\n\n\n\n\n\nCount or sum events within a defined distance.\n\nExample: Number of crimes within 500’.\n\n\n\n\nCode\nneighborhood_boundaries &lt;- st_read(\"data/BPDA_Neighborhood_Boundaries.geojson\")\n\n\nReading layer `BPDA_Neighborhood_Boundaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-06\\data\\BPDA_Neighborhood_Boundaries.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 26 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -71.19125 ymin: 42.22792 xmax: -70.92278 ymax: 42.39699\nGeodetic CRS:  WGS 84\n\n\nCode\ncrimes.sf &lt;- read_csv(\"data/bostonCrimes.csv\")\ncrimes.sf &lt;- crimes.sf %&gt;%\n  na.omit()\n\nneighborhood_boundaries &lt;- st_transform(neighborhood_boundaries, 102286)\nboston.sf &lt;- st_transform(boston.sf, 102286)\ncrimes.sf &lt;- st_as_sf(crimes.sf, coords = c(\"Long\", \"Lat\"), crs = 4326)\ncrimes.sf &lt;- st_transform(crimes.sf, 102286)\n\n\n\n\nCode\n# Create buffer features - these will work now that CRS is correct\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crimes.Buffer = lengths(st_intersects(\n      st_buffer(geometry, 660),\n      crimes.sf\n    )),\n    crimes_500ft = lengths(st_intersects(\n      st_buffer(geometry, 500),\n      crimes.sf\n    ))\n  )\n\n\n\n\n\n\nAverage distance to k closest events.\n\nExample: Average distance to 3 nearest violent crimes.\n\n\n\n\nCode\n# Calculate distance matrix (houses to crimes)\ndist_matrix &lt;- st_distance(boston.sf, crimes.sf)\n\n# Function to get mean distance to k nearest neighbors\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    # Sort and take first k, then average\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create multiple kNN features\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crime_nn1 = get_knn_distance(dist_matrix, k = 1),\n    crime_nn3 = get_knn_distance(dist_matrix, k = 3),\n    crime_nn5 = get_knn_distance(dist_matrix, k = 5)\n  )\n\n# Check results\nsummary(boston.sf %&gt;% st_drop_geometry() %&gt;% select(starts_with(\"crime_nn\")))\n\n\n   crime_nn1         crime_nn3         crime_nn5      \n Min.   :  16.58   Min.   :  16.58   Min.   :  16.58  \n 1st Qu.: 299.19   1st Qu.: 331.19   1st Qu.: 382.47  \n Median : 585.10   Median : 611.03   Median : 679.62  \n Mean   : 757.88   Mean   : 791.84   Mean   : 857.06  \n 3rd Qu.:1043.77   3rd Qu.:1079.86   3rd Qu.:1174.81  \n Max.   :3558.73   Max.   :3558.73   Max.   :3630.10  \n\n\n\ncrime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet.\n\n\n\nCode\n# Which k value correlates most with price?\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(SalePrice)\n\n\n           SalePrice\nSalePrice 1.00000000\ncrime_nn1 0.03267382\ncrime_nn3 0.03117884\ncrime_nn5 0.06963100\n\n\n\nFinding the kNN feature with the strongest correlation tells us the relevant “zone of influence” for crime perception!\n\n\n\n\n\nStraight-line (Euclidian) distance to important locations.\n\nExample: Distance to downtown, nearest T-Station.\n\n\n\n\nCode\n# Define downtown Boston (Boston Common: 42.3551° N, 71.0656° W)\ndowntown &lt;- st_sfc(st_point(c(-71.0656, 42.3551)), crs = \"EPSG:4326\") %&gt;%\n  st_transform('ESRI:102286')\n\n# Calculate distance from each house to downtown\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),\n    dist_downtown_mi = dist_downtown_ft / 5280\n  )\n\n# Summary\nsummary(boston.sf$dist_downtown_mi)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.05467 0.80476 1.37410 1.39107 1.93750 2.77678 \n\n\n\n\nCode\n# Summary of all spatial features created\nspatial_summary &lt;- boston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %&gt;%\n  summary()\n\nspatial_summary\n\n\n crimes.Buffer    crimes_500ft     crime_nn3       dist_downtown_mi \n Min.   : 0.00   Min.   : 0.00   Min.   :  16.58   Min.   :0.05467  \n 1st Qu.: 0.00   1st Qu.: 0.00   1st Qu.: 331.19   1st Qu.:0.80476  \n Median : 3.00   Median : 0.00   Median : 611.03   Median :1.37410  \n Mean   : 7.78   Mean   : 4.49   Mean   : 791.84   Mean   :1.39107  \n 3rd Qu.: 9.00   3rd Qu.: 6.00   3rd Qu.:1079.86   3rd Qu.:1.93750  \n Max.   :75.00   Max.   :63.00   Max.   :3558.73   Max.   :2.77678  \n\n\n\ncrimes.Buffer (660ft) is a buffer count measuring number of crimes near a house.\ncrimes_500ft is a buffer county measuring crimes within 500ft.\ncrime_nn3 is a kNN distance measuring average distance to 3 nearest crimes in feet.\ndist_downtown_mi is a point distance measuring miles from downtown boston.\n\n\n\nCode\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2015 - YR_BUILT)  \n\n# Model 1: Structural only\nmodel_structural &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age, \n                       data = boston.sf)\n\n# Model 2: Add spatial features\nmodel_spatial &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age +\n                    crimes_500ft + crime_nn3 + dist_downtown_mi,\n                    data = boston.sf)\n\n# Compare\ncat(\"Structural R²:\", round(summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nStructural R²: 0.2374 \n\n\n\n\nCode\ncat(\"With spatial R²:\", round(summary(model_spatial)$r.squared, 4), \"\\n\")\n\n\nWith spatial R²: 0.3644 \n\n\n\n\nCode\ncat(\"Improvement:\", round(summary(model_spatial)$r.squared - \n                          summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.127 \n\n\n\n\n\n\n\nCategorical variables that capture all unmeasured characteristics of a group.\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable.\nCaptures everything unique about that neighborhood we didn’t explicitly measure.\n\nTechnically done when went over categorical data.\nWhat’s captured:\n\nSchool quality.\nPrestige or reputation.\nWalkability.\nAccess to jobs.\nCultural amenities.\nThings we can’t easily measure.\n\n\n\n\n\n\nCode\n# Behind the scenes, R creates dummies:\n# is_BackBay = 1 if Back Bay, 0 otherwise\n# is_Beacon = 1 if Beacon Hill, 0 otherwise\n# is_Allston = 1 if Allston, 0 otherwise\n# ... (R drops one as reference category)\n\n\nEach coefficient is equal to the price premium/discount for that neighborhood (holding all else constant).\n\nWhy use fixed effects?\n\nDramatically improves prediction.\n\nIn current data it’s because neighborhoods bundle many unmeasured factors like school districts, job access, amenities, and “cool factor”.\n\nCoefficients change.\n\nCrime coefficient without FE: -$125/crime\nCrime coefficient with FE: -$85/crime\n\nWithout FE: Captured confounders too.\nWith FE: Neighborhoods “absorb” other differences.\nNow just the crime effect.\n\nTrade-Off: FEs are powerful, but they’re a black box, we don’t know why Back Bay commands a premium.\n\n\n\nCompare All Models\n\n\n\n\n\n\nThree common validation approaches:\n\nTrain/Test Split: 80/20 training/testing split, simple but unstable.\nk-Fold Cross-Validation: Split into k-folds, train on k-1, test on 1, repeat.\nLOOCV: Leave one observation out at a time (special case of k-fold).\n\n\nUse k-fold CV to compare hedonic models.\n\nCV tells us how well model predicts new data.\nMore honest than in-sample R^2.\nHelps detect overfitting.\n\nRule of Thumb: Categories with n &lt; 10 will likely cause CV problems.\n\nSolution: Group small neighborhoods.\n\nTrade-Off: Lose granularity for small neighborhoods, but avoid CV crashes.\n\nAlternative: Drop sparse categories.\n\nWarning: Consider carefully, which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why.\n\n\nRECOMMMENDED WORKFLOW\n\nNote: These values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact.\nKey Insight: Each layer improves out-of-sample prediction, with fixed effects providing the biggest boost.\nWhy? Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can’t easily quantify individually.\nLook for: - Prices over $2-3 million (could be luxury condos or errors) - Prices near 0 (data errors) - Long right tail in histogram\n\nLog Transform the skewed dependent variable.\n\nInterpreting log models:\n\nRMSE is now in log-dollars (hard to interpret).\nTo convert back: exp(predictions) gives actual dollars.\nCoefficients now represent percentage changes, not dollar changes.\nThis is standard practice in hedonic modeling!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-06/week-06-notes.html#key-concepts-learned",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "",
    "text": "Code\n# Load packages and data\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# Load Boston housing data\nboston &lt;- read_csv(here(\"data/boston.csv\"))\n\n# Quick look at the data\nglimpse(boston)\n\n\nRows: 1,485\nColumns: 24\n$ ...1       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ Parcel_No  &lt;dbl&gt; 100032000, 100058000, 100073000, 100112000, 100137000, 1001…\n$ SalePrice  &lt;dbl&gt; 450000, 600000, 450000, 670000, 260000, 355000, 665000, 355…\n$ PricePerSq &lt;dbl&gt; 228.89, 164.34, 105.98, 291.94, 217.21, 190.96, 227.35, 120…\n$ LivingArea &lt;dbl&gt; 1966, 3840, 4246, 2295, 1197, 1859, 2925, 2904, 892, 1916, …\n$ Style      &lt;chr&gt; \"Conventional\", \"Semi?Det\", \"Decker\", \"Row\\xa0End\", \"Coloni…\n$ GROSS_AREA &lt;dbl&gt; 3111, 5603, 6010, 3482, 1785, 2198, 4341, 3892, 1658, 3318,…\n$ NUM_FLOORS &lt;dbl&gt; 2.0, 3.0, 3.0, 3.0, 2.0, 1.5, 3.0, 3.0, 2.0, 2.0, 1.5, 2.0,…\n$ R_BDRMS    &lt;dbl&gt; 4, 8, 9, 6, 2, 3, 8, 6, 2, 2, 4, 3, 3, 3, 6, 8, 4, 4, 3, 2,…\n$ R_FULL_BTH &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 1, 2,…\n$ R_HALF_BTH &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ R_KITCH    &lt;dbl&gt; 2, 3, 3, 3, 1, 2, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 2, 2,…\n$ R_AC       &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\",…\n$ R_FPLACE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ LU         &lt;chr&gt; \"R2\", \"R3\", \"R3\", \"R3\", \"R1\", \"R2\", \"R3\", \"E\", \"R1\", \"R2\", …\n$ OWN_OCC    &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\",…\n$ R_BLDG_STY &lt;chr&gt; \"CV\", \"SD\", \"DK\", \"RE\", \"CL\", \"CV\", \"DK\", \"DK\", \"RE\", \"TF\",…\n$ R_ROOF_TYP &lt;chr&gt; \"H\", \"F\", \"F\", \"F\", \"F\", \"G\", \"F\", \"F\", \"G\", \"F\", \"G\", \"F\",…\n$ R_EXT_FIN  &lt;chr&gt; \"M\", \"B\", \"M\", \"M\", \"P\", \"M\", \"M\", \"A\", \"A\", \"M\", \"W\", \"W\",…\n$ R_TOTAL_RM &lt;dbl&gt; 10, 17, 20, 14, 5, 8, 14, 14, 4, 9, 7, 7, 5, 5, 15, 14, 11,…\n$ R_HEAT_TYP &lt;chr&gt; \"W\", \"W\", \"W\", \"W\", \"E\", \"E\", \"W\", \"W\", \"W\", \"W\", \"W\", \"W\",…\n$ YR_BUILT   &lt;dbl&gt; 1900, 1910, 1910, 1905, 1860, 1905, 1900, 1890, 1900, 1900,…\n$ Latitude   &lt;dbl&gt; 42.37963, 42.37877, 42.37940, 42.38014, 42.37967, 42.37953,…\n$ Longitude  &lt;dbl&gt; -71.03076, -71.02943, -71.02846, -71.02859, -71.02903, -71.…\n\n\n\n\nCode\n# Simple model: Predict price from living area\n# SalePrice is y (response) and LivingArea is x (predictor).\nbaseline_model &lt;- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,    Adjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpretation\n\nIntercept’s Estimate: Base Sale Price at 0 sq. ft. is ~$157,968.32. Not useful in practice.\nLiving Area’s Estimate: Each additional square foot of Living Area adds ~$216 to Sale Price.\nRelationship is statistically significant (p &lt; 0.001). If p is small, reject the null hypothesis.\nR^2 is 0.13, so 13% of variation is explained by regression model output.\n\nMost of the 87% variation is unexplained, so what’s the problem?\n\nLimitations\n\nIgnores location (North End vs. Roxbury vs. Back Bay); proximity to downtown, waterfront, parks, etc.; nearby crime levels; school quality; neighborhood characteristics.\nMight fail because 1,000 sq ft in Back Bay is not equivalent to 1,000 sq ft in Roxbury; same house, different locations, means very different prices; location!\nCould improve by adding spatial features like crime and distance to amenities; control for neighborhood (fixed effects); include interactions (does size matter more in wealthy areas)?\nJust doing a linear regression is a limit, so that’s where spatial features come in.\n\n\n\n\n\n\n\nCode\n# Add number of bathrooms\nbetter_model &lt;- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\nsummary(better_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-783019 -230756  -65737   75367 9193133 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 107683.96   37340.88   2.884  0.00399 ** \nLivingArea     145.68      21.33   6.828 1.25e-11 ***\nR_FULL_BTH  106978.13   23800.30   4.495 7.50e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 560200 on 1482 degrees of freedom\nMultiple R-squared:  0.1429,    Adjusted R-squared:  0.1418 \nF-statistic: 123.6 on 2 and 1482 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# Compare models\ncat(\"Baseline R²:\", summary(baseline_model)$r.squared, \"\\n\")\n\n\nBaseline R²: 0.1312636 \n\n\n\n\nCode\ncat(\"With bathrooms R²:\", summary(better_model)$r.squared, \"\\n\")\n\n\nWith bathrooms R²: 0.1429474 \n\n\n\n\nCode\n# Convert boston data to sf object\nboston.sf &lt;- boston %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n# Check it worked\nhead(boston.sf)\n\n\nSimple feature collection with 6 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 238643.6 ymin: 903246 xmax: 238833.2 ymax: 903399.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 23\n   ...1 Parcel_No SalePrice PricePerSq LivingArea Style    GROSS_AREA NUM_FLOORS\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1     1 100032000    450000       229.       1966 \"Conven…       3111        2  \n2     2 100058000    600000       164.       3840 \"Semi?D…       5603        3  \n3     3 100073000    450000       106.       4246 \"Decker\"       6010        3  \n4     4 100112000    670000       292.       2295 \"Row\\xa…       3482        3  \n5     5 100137000    260000       217.       1197 \"Coloni…       1785        2  \n6     6 100138000    355000       191.       1859 \"Conven…       2198        1.5\n# ℹ 15 more variables: R_BDRMS &lt;dbl&gt;, R_FULL_BTH &lt;dbl&gt;, R_HALF_BTH &lt;dbl&gt;,\n#   R_KITCH &lt;dbl&gt;, R_AC &lt;chr&gt;, R_FPLACE &lt;dbl&gt;, LU &lt;chr&gt;, OWN_OCC &lt;chr&gt;,\n#   R_BLDG_STY &lt;chr&gt;, R_ROOF_TYP &lt;chr&gt;, R_EXT_FIN &lt;chr&gt;, R_TOTAL_RM &lt;dbl&gt;,\n#   R_HEAT_TYP &lt;chr&gt;, YR_BUILT &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\n\n\nCode\nclass(boston.sf)  # Should show \"sf\" and \"data.frame\"\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nCode\n# Load neighborhood boundaries\nnhoods &lt;- read_sf(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %&gt;%\n  st_transform('ESRI:102286')  # Match CRS!\n\n# Check the neighborhoods\nhead(nhoods)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229049.1 ymin: 890856 xmax: 236590.9 ymax: 900302.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 8\n  sqmiles name         neighborhood_id  acres SHAPE__Length objectid SHAPE__Area\n    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\n1    2.51 Roslindale   15              1606.         53564.       53   69938273.\n2    3.94 Jamaica Pla… 11              2519.         56350.       54  109737890.\n3    0.55 Mission Hill 13               351.         17919.       55   15283120.\n4    0.29 Longwood     28               189.         11909.       56    8215904.\n5    0.04 Bay Village  33                26.5         4651.       57    1156071.\n6    0.02 Leather Dis… 27                15.6         3237.       58     681272.\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [m]&gt;\n\n\n\n\nCode\nnrow(nhoods)  # How many neighborhoods?\n\n\n[1] 26\n\n\n\n\nCode\n# Spatial join: Assign each house to its neighborhood\nboston.sf &lt;- boston.sf %&gt;%\n  st_join(nhoods, join = st_intersects)\n\n# Check results\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(`name`) %&gt;%\n  arrange(desc(n))\n\n\n# A tibble: 19 × 2\n   name              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 Dorchester      344\n 2 West Roxbury    242\n 3 Hyde Park       152\n 4 East Boston     149\n 5 Roslindale      142\n 6 Jamaica Plain   113\n 7 South Boston     80\n 8 Charlestown      65\n 9 Mattapan         65\n10 Roxbury          63\n11 South End        20\n12 Beacon Hill      17\n13 Mission Hill     14\n14 Allston           6\n15 Brighton          6\n16 Back Bay          3\n17 Fenway            2\n18 Bay Village       1\n19 Downtown          1\n\n\n\nWhy do you think certain neighborhoods command higher prices? Proximity to downtown? Historical character? School quality? Safety? All of the above? This is why we need spatial features and neighborhood controls!\n\n\n\n\n\n\n\nContinuous Variables\n\nSquare footage\nAge of house\nIncome levels\nDistance to downtown\n\nCategorical Variables\n\nNeighborhood\n\nn-1 Rule: One neighborhood is chosen as the reference category (omitted). R picks the first alphabetically unless specified otherwise.\n\nSchool district\nBuilding type\nHas garage\n\n\n\n\nCode\n# Ensure name is a factor\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(name = as.factor(name))\n\n# Check which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\n\n\n[1] \"Allston\"\n\n\n\n\nCode\n# Fit model with neighborhood fixed effects\nmodel_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, \n                          data = boston.sf)\n\n# Show just first 10 coefficients\nsummary(model_neighborhoods)$coef[1:10, ]\n\n\n                    Estimate   Std. Error    t value      Pr(&gt;|t|)\n(Intercept)      704306.0751 9.022210e+04  7.8063584  1.112892e-14\nLivingArea          138.3206 6.468186e+00 21.3847638  1.605175e-88\nnameBack Bay    7525585.0560 1.533714e+05 49.0677253 1.522921e-311\nnameBay Village 1284057.5326 2.320256e+05  5.5341206  3.700442e-08\nnameBeacon Hill 1767805.2297 1.020211e+05 17.3278366  2.465874e-61\nnameBrighton    -168941.2113 1.239972e+05 -1.3624597  1.732623e-01\nnameCharlestown   -2556.8667 9.208989e+04 -0.0277649  9.778534e-01\nnameDorchester  -576819.4858 8.846990e+04 -6.5199517  9.653393e-11\nnameDowntown      58553.1237 2.322150e+05  0.2521505  8.009601e-01\nnameEast Boston -534060.4110 8.962950e+04 -5.9585340  3.182615e-09\n\n\n\nInterperetation\n\nReference Category: Allston was automatically chosen as the intercept because it’s the first alphabetically.\nStructural Variables\n\nLiving Area: Each additional sq. ft. adds this amount (same for all neighborhoods).\nBedrooms: Effect of one or more full bathroom (same for all neighborhoods).\n\nNeighborhood Dummies\n\nPositive Coefficient: This neighborhood is more expensive than Allston.\nNegative Coefficient: This neighborhood is less expensive than Allston.\nAll other variables constant and held equal, same size and same bathrooms.\n\nHouse A in Back Bay\n\nLiving Area: 1,500 sq. ft.\nBaths: 2\nNeighborhood: Back Bay\nPredicted Price: $8,458,873\n\nHouse B in Roxbury\n\nLiving Area: 1,500 sq. ft.\nBaths: 2\nNeighborhood: Roxbury\nPredicted Price: $311,066\n\nNeighborhood Effect Price Difference: $8,127,807\n\nSame house, different location, but huge price difference. This is what neighborhood dummies capture.\n\nR automatically handles dummy variables as booleans, so 1 is true for Back Bay and 0 is false for Back Bay when creating neighborhood variables.\n\n\n\n\n\n\n\n\n\n\nThe Question: Doess the effect of one variable depend on the level of another variable?\n\nExample Scenarios\n\nHousing: Does square footage matter more in wealthy neighborhoods?\nEducation: Do tutoring effects vary by initial skill level?\nPublic Health: Do pollution effects differ by age?\n\nExample for this study: Is the value of square footage the same across all Boston neighborhoods?\n\nSalePrice = β₀ + β₁(LivingArea) + β₂(WealthyNeighborhood) + β₃(LivingArea × WealthyNeighborhood) + ε\n\n\nTheory: Luxury Premium Hypothesis\n\nIn Wealthy Neighborhoods (Back Bay, Beacon Hill, South End)\n\nHigh-end buyers pay premium for space.\nLuxury finishes, location prestige.\nEach sq. ft. adds substantial value.\nSteep slope.\nHypothesis: $300+ per sq. ft.\n\nIn Working-Class Neighborhoods (Dorchester, Mattapan, East Boston)\n\nBuyers value function over luxury.\nMore price-sensitive market.\nSpace matters, but less premium.\nFlatter slope.\nHypothesis: $100-150 per sq. ft.\n\nKey Question: If we assume one slope for all neighborhoods, are we misunderstanding the market?\n\n\n\n\nCode\n# Define wealthy neighborhoods based on median prices\nwealthy_hoods &lt;- c(\"Back Bay\", \"Beacon Hill\", \"South End\", \"Bay Village\")\n\n# Create binary indicator\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n# Check the split\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n# A tibble: 2 × 2\n  wealthy_neighborhood     n\n  &lt;fct&gt;                &lt;int&gt;\n1 Not Wealthy           1444\n2 Wealthy                 41\n\n\nModel 1: No Interaction (Parallel Slopes)\n\n\nCode\n# Model assumes same slope everywhere\nmodel_no_interact &lt;- lm(SalePrice ~ LivingArea + wealthy_neighborhood, \n                        data = boston.sf)\n\nsummary(model_no_interact)$coef\n\n\n                                Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)                  213842.7642 23893.619394  8.949785  1.039392e-18\nLivingArea                      160.2357     9.713346 16.496442  2.936087e-56\nwealthy_neighborhoodWealthy 2591012.0471 59958.794614 43.213211 1.103579e-264\n\n\n\nAssumes living area has the same effect in all neighborhoods. Only the intercept differs (wealthy areas start higher). Parallel lines on a plot.\n\nModel 2: With Interaction (Different Slopes)\n\n\nCode\n# Model allows different slopes\nmodel_interact &lt;- lm(SalePrice ~ LivingArea * wealthy_neighborhood, \n                     data = boston.sf)\n\nsummary(model_interact)$coef\n\n\n                                            Estimate   Std. Error   t value\n(Intercept)                             358542.41696 1.863997e+04 19.235144\nLivingArea                                  95.63947 7.620382e+00 12.550483\nwealthy_neighborhoodWealthy            -377937.38372 1.005554e+05 -3.758498\nLivingArea:wealthy_neighborhoodWealthy     985.12500 2.975904e+01 33.103383\n                                            Pr(&gt;|t|)\n(Intercept)                             8.875710e-74\nLivingArea                              2.079700e-34\nwealthy_neighborhoodWealthy             1.775774e-04\nLivingArea:wealthy_neighborhoodWealthy 2.445570e-180\n\n\n\nAllows living are to have different effects in different neighborhoods. Both intercept and slope differ. Non-parallel lines on a plot.\nWe get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive at what sq. ft. = 384 (358,542 / 985).\n\nNot Wealthy Areas Equation: \\(Price = 358,542 + 96 × LivingArea\\)\n\nInterpretation is at base price $358,542, each sq. ft. adds $96.\n\nWealthy Areas Equation: \\(Price = -19,395 + 1,081 × LivingArea\\)\n\nInterpretation is at base price -$19,395, each sq. ft. adds $1,081.\n\nThe Interaction Effect. Wealthy areas value each sq ft $985 more than non-wealthy areas!\n\nKey Observation: The lines are NOT parallel when plotted - that’s the interaction!\n\nComparing Model Performance\n\n\nCode\n# Compare R-squared\ncat(\"Model WITHOUT interaction R²:\", round(summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITHOUT interaction R²: 0.6156 \n\n\n\n\nCode\ncat(\"Model WITH interaction R²:\", round(summary(model_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITH interaction R²: 0.7791 \n\n\n\n\nCode\ncat(\"Improvement:\", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.1635 \n\n\n\nModel Improvement: Adding the interaction improves R² by 0.1635 (a 26.6% relative improvement)\n\nInterpretation: We explain 16.35% more variation in prices by allowing different slopes!\n\n\n\n\n\n\nSigns of Non-Linearity:\n\nCurved residual plots.\nDiminishing returns.\nAccelerating effects.\nU-shaped or inverted-U patterns.\nTheoretical reasons.\n\nExamples:\n\nHouse Age: Depreciation, then vintage premium.\nTest Scores: Plateau after studying.\nAdvertising: Diminishing returns.\nCrime Prevention: Early gains, then plateaus.\n\nPolynomial Regression\n\nSalePrice = β₀ + β₁(Age) + β₂(Age²) + ε\n\nAllows for curved relationship.\n\n\nAge’s Non-Linear Effect\n\nNew Houses (0-20 Years)\n\nModern amenities.\nMove-in ready.\nNo repairs needed.\nHigh value.\nSteep depreciation initially.\n\nMiddle-Aged (20-80 Years)\n\nNeeds updates.\nWear and tear.\nNot yet “historic”.\nLowest value.\nTrough of the curve.\n\nHistoric/Vintage (80+ Years)\n\nArchitectural character.\nHistoric districts.\nPrestige value.\nRising value.\n“Vintage premium”.\n\n\nBoston’s Context: The city has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?\n\n\n\nCode\n# Calculate age from year built\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2025 - YR_BUILT)%&gt;% filter(Age &lt;2000)\n\n\n# Check the distribution of age\nsummary(boston.sf$Age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    95.0   115.0   108.1   125.0   223.0 \n\n\n\n\nCode\n# Visualize age distribution\nggplot(boston.sf, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Distribution of House Age in Boston\",\n       x = \"Age (years)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLinear Model (Baseline)\n\n\nCode\n# Simple linear relationship\nmodel_age_linear &lt;- lm(SalePrice ~ Age + LivingArea, data = boston.sf)\n\nsummary(model_age_linear)$coef\n\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -120808.8738 57836.98918 -2.088782 3.689911e-02\nAge            2834.0198   497.72013  5.694003 1.496246e-08\nLivingArea      202.8312    15.10373 13.429214 7.228187e-39\n\n\n\nEach additional year of age changes price by $2834.01 (assumed constant rate).\n\nAdd Polynomial Term: Age Squared\n\n\nCode\n# Quadratic model (Age²)\nmodel_age_quad &lt;- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)\n\nsummary(model_age_quad)$coef\n\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 570397.14406 97894.237962  5.826667 6.938345e-09\nAge         -13007.51918  1896.446156 -6.858892 1.019524e-11\nI(Age^2)        80.88988     9.360652  8.641479 1.425208e-17\nLivingArea     203.75007    14.739041 13.823835 5.975020e-41\n\n\n\nImportant: The I() Function Why I(Age^2) instead of just Age^2? In R formulas, ^ has special meaning. I() tells R: “interpret this literally, compute Age²” Without I(): R would interpret it differently in the formula.\nModel Equation: \\(Price = 570,397 + - 13,008 × Age + 80 × Age² + 204 × LivingArea\\)\n\nWarning: Can’t interpret coefficients directly. With Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of \\(Age = β₁ + 2 × β₂ × Age\\) This means the effect changes at every age!\n\n\nCompare Models\n\n\nCode\n# R-squared comparison\nr2_linear &lt;- summary(model_age_linear)$r.squared\nr2_quad &lt;- summary(model_age_quad)$r.squared\n\ncat(\"Linear model R²:\", round(r2_linear, 4), \"\\n\")\n\n\nLinear model R²: 0.1549 \n\n\n\n\nCode\ncat(\"Quadratic model R²:\", round(r2_quad, 4), \"\\n\")\n\n\nQuadratic model R²: 0.1959 \n\n\n\n\nCode\ncat(\"Improvement:\", round(r2_quad - r2_linear, 4), \"\\n\\n\")\n\n\nImprovement: 0.0409 \n\n\n\n\nCode\n# F-test: Is the Age² term significant?\nanova(model_age_linear, model_age_quad)\n\n\nAnalysis of Variance Table\n\nModel 1: SalePrice ~ Age + LivingArea\nModel 2: SalePrice ~ Age + I(Age^2) + LivingArea\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1469 4.5786e+14                                   \n2   1468 4.3569e+14  1 2.2163e+13 74.675 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck Residual Plot\n\n\nCode\n# Compare residual plots\npar(mfrow = c(1, 2))\n\n# Linear model residuals\nplot(fitted(model_age_linear), residuals(model_age_linear),\n     main = \"Linear Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Quadratic model residuals  \nplot(fitted(model_age_quad), residuals(model_age_quad),\n     main = \"Quadratic Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Everything is related to everything else, but near things are more related than distant things.”\n- Waldo Tobler, 1970\n\nFor house prices, crime nearby matters more than crime across the city.\nParks within walking distance affect value.\nImmediate neighborhood defines market.\n\n\n\n\n\n\n\n\n\n\n\nCount or sum events within a defined distance.\n\nExample: Number of crimes within 500’.\n\n\n\n\nCode\nneighborhood_boundaries &lt;- st_read(\"data/BPDA_Neighborhood_Boundaries.geojson\")\n\n\nReading layer `BPDA_Neighborhood_Boundaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-06\\data\\BPDA_Neighborhood_Boundaries.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 26 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -71.19125 ymin: 42.22792 xmax: -70.92278 ymax: 42.39699\nGeodetic CRS:  WGS 84\n\n\nCode\ncrimes.sf &lt;- read_csv(\"data/bostonCrimes.csv\")\ncrimes.sf &lt;- crimes.sf %&gt;%\n  na.omit()\n\nneighborhood_boundaries &lt;- st_transform(neighborhood_boundaries, 102286)\nboston.sf &lt;- st_transform(boston.sf, 102286)\ncrimes.sf &lt;- st_as_sf(crimes.sf, coords = c(\"Long\", \"Lat\"), crs = 4326)\ncrimes.sf &lt;- st_transform(crimes.sf, 102286)\n\n\n\n\nCode\n# Create buffer features - these will work now that CRS is correct\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crimes.Buffer = lengths(st_intersects(\n      st_buffer(geometry, 660),\n      crimes.sf\n    )),\n    crimes_500ft = lengths(st_intersects(\n      st_buffer(geometry, 500),\n      crimes.sf\n    ))\n  )\n\n\n\n\n\n\nAverage distance to k closest events.\n\nExample: Average distance to 3 nearest violent crimes.\n\n\n\n\nCode\n# Calculate distance matrix (houses to crimes)\ndist_matrix &lt;- st_distance(boston.sf, crimes.sf)\n\n# Function to get mean distance to k nearest neighbors\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    # Sort and take first k, then average\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create multiple kNN features\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crime_nn1 = get_knn_distance(dist_matrix, k = 1),\n    crime_nn3 = get_knn_distance(dist_matrix, k = 3),\n    crime_nn5 = get_knn_distance(dist_matrix, k = 5)\n  )\n\n# Check results\nsummary(boston.sf %&gt;% st_drop_geometry() %&gt;% select(starts_with(\"crime_nn\")))\n\n\n   crime_nn1         crime_nn3         crime_nn5      \n Min.   :  16.58   Min.   :  16.58   Min.   :  16.58  \n 1st Qu.: 299.19   1st Qu.: 331.19   1st Qu.: 382.47  \n Median : 585.10   Median : 611.03   Median : 679.62  \n Mean   : 757.88   Mean   : 791.84   Mean   : 857.06  \n 3rd Qu.:1043.77   3rd Qu.:1079.86   3rd Qu.:1174.81  \n Max.   :3558.73   Max.   :3558.73   Max.   :3630.10  \n\n\n\ncrime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet.\n\n\n\nCode\n# Which k value correlates most with price?\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(SalePrice)\n\n\n           SalePrice\nSalePrice 1.00000000\ncrime_nn1 0.03267382\ncrime_nn3 0.03117884\ncrime_nn5 0.06963100\n\n\n\nFinding the kNN feature with the strongest correlation tells us the relevant “zone of influence” for crime perception!\n\n\n\n\n\nStraight-line (Euclidian) distance to important locations.\n\nExample: Distance to downtown, nearest T-Station.\n\n\n\n\nCode\n# Define downtown Boston (Boston Common: 42.3551° N, 71.0656° W)\ndowntown &lt;- st_sfc(st_point(c(-71.0656, 42.3551)), crs = \"EPSG:4326\") %&gt;%\n  st_transform('ESRI:102286')\n\n# Calculate distance from each house to downtown\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),\n    dist_downtown_mi = dist_downtown_ft / 5280\n  )\n\n# Summary\nsummary(boston.sf$dist_downtown_mi)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.05467 0.80476 1.37410 1.39107 1.93750 2.77678 \n\n\n\n\nCode\n# Summary of all spatial features created\nspatial_summary &lt;- boston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %&gt;%\n  summary()\n\nspatial_summary\n\n\n crimes.Buffer    crimes_500ft     crime_nn3       dist_downtown_mi \n Min.   : 0.00   Min.   : 0.00   Min.   :  16.58   Min.   :0.05467  \n 1st Qu.: 0.00   1st Qu.: 0.00   1st Qu.: 331.19   1st Qu.:0.80476  \n Median : 3.00   Median : 0.00   Median : 611.03   Median :1.37410  \n Mean   : 7.78   Mean   : 4.49   Mean   : 791.84   Mean   :1.39107  \n 3rd Qu.: 9.00   3rd Qu.: 6.00   3rd Qu.:1079.86   3rd Qu.:1.93750  \n Max.   :75.00   Max.   :63.00   Max.   :3558.73   Max.   :2.77678  \n\n\n\ncrimes.Buffer (660ft) is a buffer count measuring number of crimes near a house.\ncrimes_500ft is a buffer county measuring crimes within 500ft.\ncrime_nn3 is a kNN distance measuring average distance to 3 nearest crimes in feet.\ndist_downtown_mi is a point distance measuring miles from downtown boston.\n\n\n\nCode\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2015 - YR_BUILT)  \n\n# Model 1: Structural only\nmodel_structural &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age, \n                       data = boston.sf)\n\n# Model 2: Add spatial features\nmodel_spatial &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age +\n                    crimes_500ft + crime_nn3 + dist_downtown_mi,\n                    data = boston.sf)\n\n# Compare\ncat(\"Structural R²:\", round(summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nStructural R²: 0.2374 \n\n\n\n\nCode\ncat(\"With spatial R²:\", round(summary(model_spatial)$r.squared, 4), \"\\n\")\n\n\nWith spatial R²: 0.3644 \n\n\n\n\nCode\ncat(\"Improvement:\", round(summary(model_spatial)$r.squared - \n                          summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.127 \n\n\n\n\n\n\n\nCategorical variables that capture all unmeasured characteristics of a group.\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable.\nCaptures everything unique about that neighborhood we didn’t explicitly measure.\n\nTechnically done when went over categorical data.\nWhat’s captured:\n\nSchool quality.\nPrestige or reputation.\nWalkability.\nAccess to jobs.\nCultural amenities.\nThings we can’t easily measure.\n\n\n\n\n\n\nCode\n# Behind the scenes, R creates dummies:\n# is_BackBay = 1 if Back Bay, 0 otherwise\n# is_Beacon = 1 if Beacon Hill, 0 otherwise\n# is_Allston = 1 if Allston, 0 otherwise\n# ... (R drops one as reference category)\n\n\nEach coefficient is equal to the price premium/discount for that neighborhood (holding all else constant).\n\nWhy use fixed effects?\n\nDramatically improves prediction.\n\nIn current data it’s because neighborhoods bundle many unmeasured factors like school districts, job access, amenities, and “cool factor”.\n\nCoefficients change.\n\nCrime coefficient without FE: -$125/crime\nCrime coefficient with FE: -$85/crime\n\nWithout FE: Captured confounders too.\nWith FE: Neighborhoods “absorb” other differences.\nNow just the crime effect.\n\nTrade-Off: FEs are powerful, but they’re a black box, we don’t know why Back Bay commands a premium.\n\n\n\nCompare All Models\n\n\n\n\n\n\nThree common validation approaches:\n\nTrain/Test Split: 80/20 training/testing split, simple but unstable.\nk-Fold Cross-Validation: Split into k-folds, train on k-1, test on 1, repeat.\nLOOCV: Leave one observation out at a time (special case of k-fold).\n\n\nUse k-fold CV to compare hedonic models.\n\nCV tells us how well model predicts new data.\nMore honest than in-sample R^2.\nHelps detect overfitting.\n\nRule of Thumb: Categories with n &lt; 10 will likely cause CV problems.\n\nSolution: Group small neighborhoods.\n\nTrade-Off: Lose granularity for small neighborhoods, but avoid CV crashes.\n\nAlternative: Drop sparse categories.\n\nWarning: Consider carefully, which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why.\n\n\nRECOMMMENDED WORKFLOW\n\nNote: These values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact.\nKey Insight: Each layer improves out-of-sample prediction, with fixed effects providing the biggest boost.\nWhy? Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can’t easily quantify individually.\nLook for: - Prices over $2-3 million (could be luxury condos or errors) - Prices near 0 (data errors) - Long right tail in histogram\n\nLog Transform the skewed dependent variable.\n\nInterpreting log models:\n\nRMSE is now in log-dollars (hard to interpret).\nTo convert back: exp(predictions) gives actual dollars.\nCoefficients now represent percentage changes, not dollar changes.\nThis is standard practice in hedonic modeling!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06/week-06-notes.html#coding-techniques",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nsum(is.na())\nglimpse()\nsummary()\nst_drop_geometry()"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06/week-06-notes.html#questions-challenges",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nDo NOT use interactions when:\n\nThere are small samples, sufficient data is needed in each group.\nThere are already too many interactions, it makes models unstable and causes overfitting.\n\nHow do we quantify “nearness” in a way a regression model can use?\n\nMust creaate spaatial features that measure proximity to amenities/disamenities.\n\nWhen CV fails with categorical variables, the problem tends to be sparse categories.\n\nIn example:\n\nRandom split created 10 folds.\nAll “West End” sales ended up in one fold, the test fold.\nTraining folds never saw “West End”.\nModel can’t predict for a category it never learned.\n\nIssue: When neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the saame fold, breaking the model."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06/week-06-notes.html#connections-to-policy",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBoston’s Housing Market\n\nMarket Segmentation: Boston operates as two distinct housing markets.\n\nLuxury Market: Every sq. ft. is premium at $1,081/sq. ft.\nStandard Market: Space is valued, but lower premium at $96 / sq. ft.\n\nAffordability Crisis: The interaction amplifies inequality.\n\nLarge homes in wealthy areas become exponentially more expensive.\nCreates barriers to mobility between neighborhoods.\n\nPolicy Design: One-size-fits-all policies likely will fail.\n\nProperty tax assessments should account for neighborhood-specific valuation.\nHousing assistance needs vary dramatically by area."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06/week-06-notes.html#reflection",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Reflection",
    "text": "Reflection\nTips for Success:\n\nDo:\n\nStart simple, add complexity.\nCheck for NAs: sum(is.na()).\nTest on small subset first.\nCommment code.\nCheck coefficient signs.\nUse glimpse() and summary().\n\nDon’t:\n\nAdd 50 variables at once.\nIgnore errors.\nForget st_drop_geometry() for non-spatial operations.\nSkip sparse category check.\n\nCommon Errors:\n\n“Factor has new levels” -&gt; Group sparse categories.\n“Computationally singular” -&gt; Remove collinear variables.\n“Very high RMSE” -&gt; Check outliers, scale.\n“CV takes forever” -&gt; Simplify model or reduce folds.\n“Negative R^2” -&gt; Model worse than mean, rethink variables."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html",
    "href": "weekly-notes/week-05/week-05-notes.html",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "",
    "text": "STATISTICAL LEARNING FRAMEWORK\n\nStatistical Learning: Set of approaches for estimating relationships.\nFormalizing relationships, where:\n\nY = f(X) + ε\nf: The systematic information X provides about Y.\nε: Random error (irreducible).\n\nf represents the TRUE relationship between predictors and outcome.\n\nIt’s FIXED but UNKNOWN.\nIt’s what people try to estimate.\nDifferent X values produce different Y values through f.\n\nTwo reasons to estimate f:\n\nPrediction\n\nEstimate Y for new observations.\nDon’t necessarily care about the exact form of f.\nFocus is on accuracy of predictions.\n\nInference\n\nUnderstand how X affects Y.\nWhich predictions matter?\nWhat is the nature of the relationship?\nFocus is on interpreting the model.\n\n\nHow to estimate f? With two broad approaches:\n\nParametric Methods\n\nMake an assumption about functional form (e.g. linear).\nReduces problem to estimating a few parameters.\nEasier to interpret.\nMore common.\n\nNon-Parametric Methods\n\nDon’t assume a specific form.\nMore flexible.\nRequires more data.\nHarder to interpret.\n\nKEY DIFFERENCE: In parametric we assume f is linear, then estimate β₀ and β₁, etc. In non-parametric we let the data determine the shape of f.\n\nDeep Learning: Neural networks are technically parametric (millions of parameters), but achieve flexibility through parameter quantity rather than assuming a rigid form.\n\nLinear Regression: Parametric\n\nAssumption: Relationship between X and Y is linear.\nTask: Estimate the β coefficients using sample data.\nMethod: Ordinary Least Squares (OLS).\nAdvantages:\n\nSimple and interpretable.\nWell-understood properties.\nWorks remarkably well for many problems.\nFoundation for more complex methods.\n\nDisadvantages:\n\nAssumes linearity.\nSensitive to outliers.\nMakes several assumptions.\n\n\n\n\nTWO GOALS\n\n\nUnderstanding Relationships vs. Making Predictions: Same model serves different purposes.\n\nInference\n\n“Does education affect income?”\nFocus on coefficients.\nStatistical significance matters.\nUnderstand mechanisms and what the coefficients, or predictors, tell us.\n\nPrediction\n\n“What’s county Y’s income?”\nFocus on accuracy.\nPrediction intervals matter.\nDon’t need to understand why certain relationships exist.\n\n\n\n\nMODEL BUILDING\n\n\nConsiderable scatter can generally mean the relationships aren’t deterministic, but rather more probablistic or stochastic.\nInterpreting coefficients example:\n\nIntercept (β₀) = $62,855\n\nExpected income when population = 0.\nNot usually meaningful in practice.\n\nSlope (β₁) = $0.02\n\nFor each additional person, income increases by $0.02.\nThis is more useful, because for every 1,000 people, income increases by $20.\n\np-value &lt;0.001 means it’s very unlikely to see this if true, when β₁ = 0.\nCan reject the null hypothesis.\n\nHoly Grail Concept: Estimates are just estimates of the true unknown parameters.\nDifferent samples produce different linear regression lines, but no matter how many samples and statistical tests, the true relationship is unknowable.\nStandard errors quantify the above uncertainty.\nStatistical significance:\n\nNull Hypothesis (H₀): β₁ = 0 (no relationship).\nOur Estimate: β₁ = 0.02.\nQuestion: Could we get $0.02 just by chance if H₀ is true?\n\n\nt-statistic: How many standard errors away from 0?\n\nBigger |t| means more confidence that the relationship is real.\n\np-value: Probability of seeing our estimate if H₀ is true.\n\nSmall p, reject H₀, conclude relationship exists.\nLarge p, fail to reject H₀, conclude that there’s a possibility no relationship exists.\n\n\n\n\nMODEL EVALUATION\n\n\nTwo key questions:\n\nHow well does the data used fit? (in-sample fit)\nHow well would it predict new data? (out-of-sample performance)\n\n\nNOT THE SAME.\nIn-Simple Fit: R^2\n\nR^2 = 0.208 means “21% of variation in income is explained by population.”\nIs this good? It depends on the goal. For prediction, it’s moderately good. For inference, it shows population matters, but other factors exist.\nR^2 alone doesn’t tell if the model is trustworthy.\n\nOverfitting Problem\n\nUnderfitting: Model is too simple (high bias).\nGood Fit: Captures pattern without noise.\nOverfitting: Memorizes training data (high variance).\n\n\nDANGER: High R^2 doesn’t mean good predictions!\nOverfit regression means it can’t be applied to other samples and is too aligned with the data samples it was tested and derived from.\n70% training, so fit on training data only. 30% testing, so predict on test data after getting the fit on the training data.\n\nRMSE: A number of 9,536 means that on new data (test set), predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\nCross-Validation: A better approach doing multiple training and testing splits that gives more stable estimate of true prediction performance. Average the multiple RMSEs.\n\n\n\nCHECKING ASSUMPTIONS\n\n\nLinear regression makes assumptions, if violated:\n\nCoefficients may be biased.\nStandard errors are wrong.\nPredictions unreliable.\nCheck diagnostics before trusting any model.\n\nAssumption 1: Linearity.\n\nAssume relationship is linear.\nCheck with residual plot.\n\nGood plot will have random scatter, points around 0, and constant spread.\nBad plot will have curved pattern (parabolic relationship), model is missing something, predictions are biased.\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and a straight line is fitted, there will be a systematic underprediction in some regions and overprediction in otherss.\nBiased predictions in predictable ways (not random errors).\nResidual plots should show random scatter, any pattern means the model is missing something systematic.\n\n\n\nAssumption 2: Constant Variance.\n\nHeteroskedasticity: Variance changes across X.\nImpact: Standard errors are wrong, so p-values are misleading.\nHeteroskedasticity is often a sympton of model misspecification.\n\nModel fits well for some values, but poorly for other values (aggregation matters).\nMay indicate missing variables that matter more at certain X values.\nAsk what’s different about observations with large residuals.\nExample: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\nKey Insight: Large counties vary widely in income because some have high education and others have low education. Adding education as a predictor accounts for variation.\nKey Insight: Adding the right predictor can fix heteroskedasticity.\n\nFormal Test: Breusch-Pagan\n\np &gt; 0.05 means constant variance assumption is okay.\np &lt; 0.05 means there’s evidence of heteroskedasticity.\nSolutions for heteroskedasticity:\n\nTransform Y (like log(income)).\nRobust standard errors.\nAdd missing variables.\nAccept it (point predictions are still okay for prediction goals).\n\n\nAssumption 3: Normality of Residuals.\n\nAssume residuals are normally distributed.\nMatters because it’s less critical for point predictions (unbiased regardless). It’s important for confidence intervals and prediction intervals. It’s needed for valid hypothesis tests (t-tests, F-tests).\nQ-Q (Quantile-Quantile) plot of residuals to check.\n\nTheoretical Quantiles on x and Sample Quantiles on y.\n\n\nAssumption 4: No Multicollinearity.\n\nFor multiple regression, predictors shouldn’t be too correlated.\nIt matters because coefficients become unstable, and difficult to interpret.\n\nAssumption 5: No Influential Outliers.\n\nNot all outliers are problems, only those with high leverage AND large residuals.\n\nPulls regression line.\n\nDealing with influential points:\n\nInvestigate why this observation is unusual. Is it a data error or truly unique?\nReport the influential observations in analysis.\nSensitivity check. Refit the model without the outlier(s), do the conclusions change?\nDon’t automatically remove outliers, they might represent real, important cases.\n\nFor policy, an influential county might need special attention, NOT exclusion.\n\n\n\nIMPROVING PREDICTIONS\n\n\nIf a relationship is curved, try log transformations.\n\nLog models show percentage relationships.\n\nCategorical variables, R creates dummy variables automatically.\n\nSUMMARY OF REGRESSION WORKFLOW\n1. **Understand the framework.** What's f? What's the goal?\n\n2. **Visualize first.** Does a linear model make sense?\n\n3. **Fit the model.** Estimate coefficients.\n\n4. **Evaluate performance.** Train/test split, cross-validation.\n\n5. **Check assumptions.** Residual plots, VIF, outliers.\n\n6. **Improve if needed.** Transformations, more variables.\n\n7. **Consider ethics.** Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05/week-05-notes.html#key-concepts-learned",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "",
    "text": "STATISTICAL LEARNING FRAMEWORK\n\nStatistical Learning: Set of approaches for estimating relationships.\nFormalizing relationships, where:\n\nY = f(X) + ε\nf: The systematic information X provides about Y.\nε: Random error (irreducible).\n\nf represents the TRUE relationship between predictors and outcome.\n\nIt’s FIXED but UNKNOWN.\nIt’s what people try to estimate.\nDifferent X values produce different Y values through f.\n\nTwo reasons to estimate f:\n\nPrediction\n\nEstimate Y for new observations.\nDon’t necessarily care about the exact form of f.\nFocus is on accuracy of predictions.\n\nInference\n\nUnderstand how X affects Y.\nWhich predictions matter?\nWhat is the nature of the relationship?\nFocus is on interpreting the model.\n\n\nHow to estimate f? With two broad approaches:\n\nParametric Methods\n\nMake an assumption about functional form (e.g. linear).\nReduces problem to estimating a few parameters.\nEasier to interpret.\nMore common.\n\nNon-Parametric Methods\n\nDon’t assume a specific form.\nMore flexible.\nRequires more data.\nHarder to interpret.\n\nKEY DIFFERENCE: In parametric we assume f is linear, then estimate β₀ and β₁, etc. In non-parametric we let the data determine the shape of f.\n\nDeep Learning: Neural networks are technically parametric (millions of parameters), but achieve flexibility through parameter quantity rather than assuming a rigid form.\n\nLinear Regression: Parametric\n\nAssumption: Relationship between X and Y is linear.\nTask: Estimate the β coefficients using sample data.\nMethod: Ordinary Least Squares (OLS).\nAdvantages:\n\nSimple and interpretable.\nWell-understood properties.\nWorks remarkably well for many problems.\nFoundation for more complex methods.\n\nDisadvantages:\n\nAssumes linearity.\nSensitive to outliers.\nMakes several assumptions.\n\n\n\n\nTWO GOALS\n\n\nUnderstanding Relationships vs. Making Predictions: Same model serves different purposes.\n\nInference\n\n“Does education affect income?”\nFocus on coefficients.\nStatistical significance matters.\nUnderstand mechanisms and what the coefficients, or predictors, tell us.\n\nPrediction\n\n“What’s county Y’s income?”\nFocus on accuracy.\nPrediction intervals matter.\nDon’t need to understand why certain relationships exist.\n\n\n\n\nMODEL BUILDING\n\n\nConsiderable scatter can generally mean the relationships aren’t deterministic, but rather more probablistic or stochastic.\nInterpreting coefficients example:\n\nIntercept (β₀) = $62,855\n\nExpected income when population = 0.\nNot usually meaningful in practice.\n\nSlope (β₁) = $0.02\n\nFor each additional person, income increases by $0.02.\nThis is more useful, because for every 1,000 people, income increases by $20.\n\np-value &lt;0.001 means it’s very unlikely to see this if true, when β₁ = 0.\nCan reject the null hypothesis.\n\nHoly Grail Concept: Estimates are just estimates of the true unknown parameters.\nDifferent samples produce different linear regression lines, but no matter how many samples and statistical tests, the true relationship is unknowable.\nStandard errors quantify the above uncertainty.\nStatistical significance:\n\nNull Hypothesis (H₀): β₁ = 0 (no relationship).\nOur Estimate: β₁ = 0.02.\nQuestion: Could we get $0.02 just by chance if H₀ is true?\n\n\nt-statistic: How many standard errors away from 0?\n\nBigger |t| means more confidence that the relationship is real.\n\np-value: Probability of seeing our estimate if H₀ is true.\n\nSmall p, reject H₀, conclude relationship exists.\nLarge p, fail to reject H₀, conclude that there’s a possibility no relationship exists.\n\n\n\n\nMODEL EVALUATION\n\n\nTwo key questions:\n\nHow well does the data used fit? (in-sample fit)\nHow well would it predict new data? (out-of-sample performance)\n\n\nNOT THE SAME.\nIn-Simple Fit: R^2\n\nR^2 = 0.208 means “21% of variation in income is explained by population.”\nIs this good? It depends on the goal. For prediction, it’s moderately good. For inference, it shows population matters, but other factors exist.\nR^2 alone doesn’t tell if the model is trustworthy.\n\nOverfitting Problem\n\nUnderfitting: Model is too simple (high bias).\nGood Fit: Captures pattern without noise.\nOverfitting: Memorizes training data (high variance).\n\n\nDANGER: High R^2 doesn’t mean good predictions!\nOverfit regression means it can’t be applied to other samples and is too aligned with the data samples it was tested and derived from.\n70% training, so fit on training data only. 30% testing, so predict on test data after getting the fit on the training data.\n\nRMSE: A number of 9,536 means that on new data (test set), predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\nCross-Validation: A better approach doing multiple training and testing splits that gives more stable estimate of true prediction performance. Average the multiple RMSEs.\n\n\n\nCHECKING ASSUMPTIONS\n\n\nLinear regression makes assumptions, if violated:\n\nCoefficients may be biased.\nStandard errors are wrong.\nPredictions unreliable.\nCheck diagnostics before trusting any model.\n\nAssumption 1: Linearity.\n\nAssume relationship is linear.\nCheck with residual plot.\n\nGood plot will have random scatter, points around 0, and constant spread.\nBad plot will have curved pattern (parabolic relationship), model is missing something, predictions are biased.\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and a straight line is fitted, there will be a systematic underprediction in some regions and overprediction in otherss.\nBiased predictions in predictable ways (not random errors).\nResidual plots should show random scatter, any pattern means the model is missing something systematic.\n\n\n\nAssumption 2: Constant Variance.\n\nHeteroskedasticity: Variance changes across X.\nImpact: Standard errors are wrong, so p-values are misleading.\nHeteroskedasticity is often a sympton of model misspecification.\n\nModel fits well for some values, but poorly for other values (aggregation matters).\nMay indicate missing variables that matter more at certain X values.\nAsk what’s different about observations with large residuals.\nExample: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\nKey Insight: Large counties vary widely in income because some have high education and others have low education. Adding education as a predictor accounts for variation.\nKey Insight: Adding the right predictor can fix heteroskedasticity.\n\nFormal Test: Breusch-Pagan\n\np &gt; 0.05 means constant variance assumption is okay.\np &lt; 0.05 means there’s evidence of heteroskedasticity.\nSolutions for heteroskedasticity:\n\nTransform Y (like log(income)).\nRobust standard errors.\nAdd missing variables.\nAccept it (point predictions are still okay for prediction goals).\n\n\nAssumption 3: Normality of Residuals.\n\nAssume residuals are normally distributed.\nMatters because it’s less critical for point predictions (unbiased regardless). It’s important for confidence intervals and prediction intervals. It’s needed for valid hypothesis tests (t-tests, F-tests).\nQ-Q (Quantile-Quantile) plot of residuals to check.\n\nTheoretical Quantiles on x and Sample Quantiles on y.\n\n\nAssumption 4: No Multicollinearity.\n\nFor multiple regression, predictors shouldn’t be too correlated.\nIt matters because coefficients become unstable, and difficult to interpret.\n\nAssumption 5: No Influential Outliers.\n\nNot all outliers are problems, only those with high leverage AND large residuals.\n\nPulls regression line.\n\nDealing with influential points:\n\nInvestigate why this observation is unusual. Is it a data error or truly unique?\nReport the influential observations in analysis.\nSensitivity check. Refit the model without the outlier(s), do the conclusions change?\nDon’t automatically remove outliers, they might represent real, important cases.\n\nFor policy, an influential county might need special attention, NOT exclusion.\n\n\n\nIMPROVING PREDICTIONS\n\n\nIf a relationship is curved, try log transformations.\n\nLog models show percentage relationships.\n\nCategorical variables, R creates dummy variables automatically.\n\nSUMMARY OF REGRESSION WORKFLOW\n1. **Understand the framework.** What's f? What's the goal?\n\n2. **Visualize first.** Does a linear model make sense?\n\n3. **Fit the model.** Estimate coefficients.\n\n4. **Evaluate performance.** Train/test split, cross-validation.\n\n5. **Check assumptions.** Residual plots, VIF, outliers.\n\n6. **Improve if needed.** Transformations, more variables.\n\n7. **Consider ethics.** Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05/week-05-notes.html#coding-techniques",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nTrain/Test Split\nEvaluate Predictions\nCross-Validation\nResidual Plot\nBreusch-Pagan Formal Test\nMulticollinearity\nLog Transformations\nCategorical Variables"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05/week-05-notes.html#questions-challenges",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nPublic policy is very much a balancing act between technically and statistically good models vs. actually applying them to real-world policy without the regression causing any ethical consequences."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05/week-05-notes.html#connections-to-policy",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nReal-life healthcare algorithm model discriminated despite being technically good with good R^2 and low prediction error with a good fit. Ethically this ended up amplifying existing discrimination. A model can be statistically “good” while being ethically terrible for decision-making.\nInfluential points, or outliers, have connections to algorithmic bias. High-influence observations could represent marginalized communities or unique populations. Removing them can erase important populations from analysis and lead to biased policy decisions.\nALWAYS INVESTIGATE OUTLIERS BEFORE REMOVING."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05/week-05-notes.html#reflection",
    "title": "PREDICTIVE MODELING WITH LINEAR REGRESSION",
    "section": "Reflection",
    "text": "Reflection\nKEY TAKEAWAYS\n\nStatistical Learning:\n\nEstimating f(X), the systematic relationship.\nParametric methods assume a form (most choose linear).\n\nTwo Purposes:\n\nInference: Understand relationships.\nPrediction: Forecast new values.\n\nModel Evaluation:\n\nIn-sample fit is NOT equivalent to out-of-sample performance.\nBeware of overfitting!\n\nDiagnostics Matter:\n\nAlways check assumptions.\nPlots reveal what R^2 hides."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html",
    "href": "weekly-notes/week-04/week-04-notes.html",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Each feature has geometry, which has shape and location, and attributes, which are data about that feature like population or income.\nst_intersects(): Any overlap at all.\nst_touches(): Share boundary, no interior overlap.\nst_within(): Completely inside.\nst_contains(): Completely contains.\nst_overlaps(): Partial overlap.\nst_disjoint(): No spatial relationship.\nArea calculations depend on coordinate reference system.\n(.): The dot is a placeholder that represents data being passed through pipe.\n\npa_counties &lt;- pa_counties %&gt;% mutate(area_sqkm = as.numeric(st_area(.)) / 1000000)\n\n(.) refers to pa_counties, the data frame being passed through the pipe.\nEquivalent to pa_counties &lt;- pa_counties %&gt;% mutate(area_sqkm = as.numeric(st_area(pa_counties)) / 1000000)\n\n\nUnion operations combine multiple features into one.\nSpatial aggregation summarizes data across spatial boundaries.\nProjections matter because we can’t preserve area, distance, and angles simultaneously, one has to give.\nDifferent projections optimize different properties.\nWrong projects can lead to wrong analysis results.\nGeographic Coordinate Systems (GCS):\n\nLatitude/longitude coordinates.\nUnits in decimal degrees.\nGood for global datasets and web mapping.\nBad for area/distance calculations.\n\nProjected Coordinate Systems (PCS):\n\nX/Y coordinates on a flat plane.\nUnits in meters, feet, etc.\nGood for local analysis and accurate measurements.\nBad for large areas and global datasets.\n\nCommon CRS:\n\nWGS84 (EPSG:4326): GPS standard, geographic.\nWeb Mercator (EPSG:3857): Web mapping standard, projected, heavily distorts near poles.\nState Plane / UTM Zones: Local accuracy, different zones for different regions, optimized for specific geographic areas.\nAlbers Equal Area: Preserves area and good for demographic/statistical analysis.\n\nTransform CRS when:\n\nCalculating areas or distances.\nCreating buffers.\nDoing geometric operations.\nWorking with local/regional data."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04/week-04-notes.html#key-concepts-learned",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Each feature has geometry, which has shape and location, and attributes, which are data about that feature like population or income.\nst_intersects(): Any overlap at all.\nst_touches(): Share boundary, no interior overlap.\nst_within(): Completely inside.\nst_contains(): Completely contains.\nst_overlaps(): Partial overlap.\nst_disjoint(): No spatial relationship.\nArea calculations depend on coordinate reference system.\n(.): The dot is a placeholder that represents data being passed through pipe.\n\npa_counties &lt;- pa_counties %&gt;% mutate(area_sqkm = as.numeric(st_area(.)) / 1000000)\n\n(.) refers to pa_counties, the data frame being passed through the pipe.\nEquivalent to pa_counties &lt;- pa_counties %&gt;% mutate(area_sqkm = as.numeric(st_area(pa_counties)) / 1000000)\n\n\nUnion operations combine multiple features into one.\nSpatial aggregation summarizes data across spatial boundaries.\nProjections matter because we can’t preserve area, distance, and angles simultaneously, one has to give.\nDifferent projections optimize different properties.\nWrong projects can lead to wrong analysis results.\nGeographic Coordinate Systems (GCS):\n\nLatitude/longitude coordinates.\nUnits in decimal degrees.\nGood for global datasets and web mapping.\nBad for area/distance calculations.\n\nProjected Coordinate Systems (PCS):\n\nX/Y coordinates on a flat plane.\nUnits in meters, feet, etc.\nGood for local analysis and accurate measurements.\nBad for large areas and global datasets.\n\nCommon CRS:\n\nWGS84 (EPSG:4326): GPS standard, geographic.\nWeb Mercator (EPSG:3857): Web mapping standard, projected, heavily distorts near poles.\nState Plane / UTM Zones: Local accuracy, different zones for different regions, optimized for specific geographic areas.\nAlbers Equal Area: Preserves area and good for demographic/statistical analysis.\n\nTransform CRS when:\n\nCalculating areas or distances.\nCreating buffers.\nDoing geometric operations.\nWorking with local/regional data."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04/week-04-notes.html#coding-techniques",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nVector data:\n\nPoints: Locations like schools, hospitals, or crime incidents.\nLines: Linear features like roads, rivers, or transit routes.\nPolygons: Areas like census tracts, neighborhoods, or service areas.\n\nsf package is a spatial package that integrates with tidyverse workflows, follows international standards, and is fast and reliable.\n\nSpatial data is data.frame + geometry column.\n\nSpatial data formats:\n\nShapefiles (.shp)\nGeoJSON (.geojson)\nKML/KMZ (Google Earth)\nDatabase Connections (PostGIS)\n\nSpatial Subsetting: Extract features based on spatial relationships.\n\nst_filter(), st_intersects(), st_touches, st_within()\n\n“Which counties border Allegheny?” Use st_touches.\n“Which tracts are in Allegheny?” Use st_within.\n“Which tracts overlap a metro area?” Use st_intersects.\n\n.predicate parameter tells st_filter() what spatial relationship to look for.\n\nst_filter(data_to_filter, reference_geometry, .predicate = relationship)\nIf no .predicate is specified, it uses st_intersects.\n\n\nst_filter with predicates selects complete features (keeps or removes entire rows).\n\nst_intersection() and st_union() modifies geometries (creates new shapes).\nst_filter() when we want to select/identify features based on location, need complete features with original boundaries, or counting.\nst_intersection() when we need to calculate areas, populations, or other measures within specific boundaries, doing spatial overlay analysis, or need to clip data to a study area.\n\nst_crs(data) checks current CRS.\ndata &lt;- st_set_crs(data, 4326) sets the CRS ONLY if missing.\nAlbers Equal Area CRS is good for area calculations."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04/week-04-notes.html#questions-challenges",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nTraining data may under-represent certain areas.\nSpatial autocorrelation violates independence assumptions.\nService delivery algorithms may reinforce geographic inequities."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04/week-04-notes.html#connections-to-policy",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nReal policy questions need spatial answers.\n\nWhich communities have the lowest income? Are they clustered? Isolated? Near resources?\nWhere should we locate a new health clinic? How do we optimize access for underserved populations?\nHow do school districts compare? How do we account for geographic boundaries and spillovers?\nIs there an environmental justice concern? Do pollution sources cluster near vulnerable communities?"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04/week-04-notes.html#reflection",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "Reflection",
    "text": "Reflection\n\nPolicy analysis workflow for spatial analysis:\n\nLoad data, get spatial boundaries and attribute data.\nCheck projections, transform to appropriate CRS.\nJoin datasets, combine spatial and non-spatial data.\nSpatial operations, include buffers, intersections, distance calculations, etc.\nAggregation, summarize across sptial units.\nVisualization, maps and charts.\nInterpretation, policy recommendations."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html",
    "href": "weekly-notes/week-03/week-03-notes.html",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "",
    "text": "Different data with the exact same summary statistics can visualize differently and have very different spatial patterns.\nVisualization in a policy context is especially valuable due to working with real lives and geographical areas, so taking data at face-value in tabular form is different than seeing it aggregated via census tracts.\nIt’s important to report margins-of-error, many planners and data scientists forget this key aspect and implement policy changes with low-confidence data.\nNot just bad data, but bad visualization can have consequences as well, especially hidden or ignored uncertainty being the main issue.\nEDA mindset is detective work with five questions:\n\nWhat does the data look like?\nWhat patterns exist?\nWhat’s unusual?\nWhat questions does this raise?\nHow reliable is this data?\n\nEDA workflow with data quality focus:\n\nLoad and inspect.\nAssess reliability.\nVisualize distributions.\nExplore relationships.\nIdentify patterns.\nQuestion anomolies.\nDocument limitations."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03/week-03-notes.html#key-concepts-learned",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "",
    "text": "Different data with the exact same summary statistics can visualize differently and have very different spatial patterns.\nVisualization in a policy context is especially valuable due to working with real lives and geographical areas, so taking data at face-value in tabular form is different than seeing it aggregated via census tracts.\nIt’s important to report margins-of-error, many planners and data scientists forget this key aspect and implement policy changes with low-confidence data.\nNot just bad data, but bad visualization can have consequences as well, especially hidden or ignored uncertainty being the main issue.\nEDA mindset is detective work with five questions:\n\nWhat does the data look like?\nWhat patterns exist?\nWhat’s unusual?\nWhat questions does this raise?\nHow reliable is this data?\n\nEDA workflow with data quality focus:\n\nLoad and inspect.\nAssess reliability.\nVisualize distributions.\nExplore relationships.\nIdentify patterns.\nQuestion anomolies.\nDocument limitations."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03/week-03-notes.html#coding-techniques",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2 library’s graphics grammar starts with Data -&gt; Aesthetics -&gt; Geometris -&gt; Visuals\nBasic structure: ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\nThe plus sign adds layers.\nTabular data joins:\n\nleft_join() keeps all rows from left dataset. Most common.\nright_join() keeps all rows from right dataset.\ninner_join() keeps only rows that match in both.\nfull_join() keeps all rows from both datasets."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03/week-03-notes.html#questions-challenges",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nEDA is an important concept, but sometimes working in public policy, or especially private companies, don’t allow the space for that initial experimentation."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03/week-03-notes.html#connections-to-policy",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nSummary statistics can hide critical patterns.\nOutliers my represent important communities.\nRelationships aren’t always linear.\nVisual inspection reveals data quality issues.\nGood practices are ethical requirements under AICP Code of Ethics."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03/week-03-notes.html#reflection",
    "title": "EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION",
    "section": "Reflection",
    "text": "Reflection\n\nThe goal is to understand the data being worked on before making decisions or building models.\nResearch-based recommendations for public policy and planning:\n\nReport corresponding MOEs of ACS estimates.\nInclude footnote when not reporting MOEs.\nProvide context for (un)reliability.\nReduce statistical uncertainty.\nAlways conduct statistical significance tests."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html",
    "href": "weekly-notes/week-02/week-02-notes.html",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "",
    "text": "Main Concepts\n\nAlgorithms are a set of instructions or rules for solving a problem or completing a task.\nAlgorithms may provide efficiency, consistency, objectivity, and cost-savings in certain scenarios, but they aren’t infallible, especially since the data that is fed to them could be collected in a biased manner.\nAlgorithms can help with decision-making, but at the end of the day, it’s humans that make the decisions. The many choices behind creating algorithms and making predictions embed human values and biases.\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification & prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural networks, etc.)\nAlgorithmic Decision Making: Understanding why your analysis matters for real policy decisions.\nData Subjectivity: Why we emphasize transparent, reproducible methods in this class.\nCensus Data: The foundation for most urban planning and policy analysis.\nR Skills: The tools to do this work professionally and ethically.\n\nTechnical Skills\n\nCensus data is publicly accessible.\nCounty level data is used for state and regional planning. Census tract level data is used for neighborhood analysis. Block group level is used for very local analysis, but there’s large margins of error.\nData de-indentification by adding mathematical “noise” in the data to preserve patterns, but maintain privacy. Created impossible results like people living underwater, so be wary.\nCan use R packages to access data like tidycensus package, so there’s always the latest data. This makes it easier to create reproducible workflows, provide automatic geographic boundaries, and there’s built-in error handling.\nLarge MOE relative to estimate = less reliable. Small MOE relative to estimate = more reliable. Always report MOE alongside estimates. Be cautious comparing estimates with overlapping error margins. Consider using 5-year estimates for greater reliability.\nCensus Summary Tables: Pre-calculated statistics by geography. Median income, percent college-educated, etc. Good for mapping and comparing places.\nPUMS, or Individual Records: Anonymous individual/household responses. Good for custom analysis and regression models. More complex, but more flexible."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02/week-02-notes.html#key-concepts-learned",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "",
    "text": "Main Concepts\n\nAlgorithms are a set of instructions or rules for solving a problem or completing a task.\nAlgorithms may provide efficiency, consistency, objectivity, and cost-savings in certain scenarios, but they aren’t infallible, especially since the data that is fed to them could be collected in a biased manner.\nAlgorithms can help with decision-making, but at the end of the day, it’s humans that make the decisions. The many choices behind creating algorithms and making predictions embed human values and biases.\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification & prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural networks, etc.)\nAlgorithmic Decision Making: Understanding why your analysis matters for real policy decisions.\nData Subjectivity: Why we emphasize transparent, reproducible methods in this class.\nCensus Data: The foundation for most urban planning and policy analysis.\nR Skills: The tools to do this work professionally and ethically.\n\nTechnical Skills\n\nCensus data is publicly accessible.\nCounty level data is used for state and regional planning. Census tract level data is used for neighborhood analysis. Block group level is used for very local analysis, but there’s large margins of error.\nData de-indentification by adding mathematical “noise” in the data to preserve patterns, but maintain privacy. Created impossible results like people living underwater, so be wary.\nCan use R packages to access data like tidycensus package, so there’s always the latest data. This makes it easier to create reproducible workflows, provide automatic geographic boundaries, and there’s built-in error handling.\nLarge MOE relative to estimate = less reliable. Small MOE relative to estimate = more reliable. Always report MOE alongside estimates. Be cautious comparing estimates with overlapping error margins. Consider using 5-year estimates for greater reliability.\nCensus Summary Tables: Pre-calculated statistics by geography. Median income, percent college-educated, etc. Good for mapping and comparing places.\nPUMS, or Individual Records: Anonymous individual/household responses. Good for custom analysis and regression models. More complex, but more flexible."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02/week-02-notes.html#coding-techniques",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "CODING TECHNIQUES",
    "text": "CODING TECHNIQUES\n\nPiping %&gt;% is used to stack different functions, think of it as a replacement for “and then” or as a replacement equivalent to nesting functions.\nget_acs() used to grab ACS survey data.\nstr_remove() used to remove a string.\nstr_extract() used to extract a string.\nstr_replace() used to replace a string.\ncase_when() for categories, MOE calculations.\nkable() for professional formatting."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02/week-02-notes.html#questions-challenges",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "QUESTIONS & CHALLENGES",
    "text": "QUESTIONS & CHALLENGES\n\nUsing algorithms to completely replace human decision-makers is problematic, because computers and algorithms, no matter how complex they can be programmed, can’t perfectly mimic the messiness of everyday human life.\nUsing historical data to inform policy is a challenge, because very biased policies against different races, genders, incomes, and more created socio-economic disadvantages that persist today. Like trying to geographically predict crime would clearly target disinvested and oppressed communities that haven’t had a chance or will likely not have a chance to recover within a lifetime or more.\nAlgorithms can fail because of people, because humans are behind data cleaning decisions, data coding or classification, data collection like use of imperfect proxies, how results are interpreted, what variables are put in the model.\nMany real life examples of biased algorithms harming historically minoritized communities."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02/week-02-notes.html#connections-to-policy",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "CONNECTIONS TO POLICY",
    "text": "CONNECTIONS TO POLICY\n\n“Algorithmic decision-making is used in government to assist or replace human decision-makers”.\nUsing historical data to inform policy and decision-makers by predicting the future as precisely as possible. There are applications in housing, finance, healthcare, crime, etc.\nGovernments have limited budgets and need to serve everyone to optimizing and being cost-effective means using algorithmic decision making. Proponents argue in favor that it provides efficiency, consistency, objectivity, and cost-savings, but this isn’t always the case in real life.\nDecennial census is 9 questions for the entire US.\nAmerican Community Survey (ACS) is a survey of 3% households annually with detailed questions.\nAlgorithmic Fairness: Unreliable data can bias automated decisions.\nResource Allocation: Know which areas need extra attention.\nEquity Analysis: Some communities may be systematically under-counted.\nProfessional Credibility: Always assess your data quality."
  },
  {
    "objectID": "weekly-notes/week-02/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02/week-02-notes.html#reflection",
    "title": "DYPLR BASICS AND CENSUS DATA",
    "section": "REFLECTION",
    "text": "REFLECTION\n\nCensus data is used for understanding community demographics, allocating government resources, and tracking neighborhood change, but the census itself has had a biased and racist history.\nACS 1-Year Estimates (areas &gt; 65,000 people) has the most current data and is the smallest sample. ACS 5-Year Estimates include all areas including census tracts and is the most reliable data and largest sample of the ACS. It does have a margin of error, but is pretty robust for data analysis.\nEven “objective” data involves subjective choices about privacy vs. accuracy (e.g. 202 Decennial Census).\nWhat assumptions am I making in my data choices?\nWho might be excluded from my analysis?\nHow could my findings be misused if taken out of context?\nWhat would I want policymakers to understand about my methods?"
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "",
    "text": "Code\n# Import relevant libraries.\nlibrary(car)\nlibrary(dplyr)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(sf)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidycensus)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tmap)\nlibrary(units)\n\noptions(scipen = 999)\n\n\n\n\nCode\n# Property data.\nproperties_path &lt;- \"data/philly_properties.csv\"\nproperties &lt;- read.csv(properties_path)\n\n# Capture dimensions.\nog_property_dimension &lt;- dim(properties)\n\n# Set Census API key.\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n\n\n\nCode\n# All variables are character strings, remove white space then convert numeric character variables to numeric classes for chosen variables.\n\nproperties &lt;- properties %&gt;%\n  mutate(across(where(is.character), str_trim),\n         across(c(fireplaces, garage_spaces, number_of_bathrooms, number_stories,\n                  sale_price, total_livable_area, year_built), as.numeric)) %&gt;%\n  rename(fireplace_num = fireplaces,\n         garage_num = garage_spaces,\n         bath_num = number_of_bathrooms,\n         story_num = number_stories,\n         square_feet = total_livable_area,\n         basement_type = basements,\n         ac_binary = central_air,\n         fuel_type = fuel,\n         )\n\n\n\n\nCode\n# Filter to residential properties and 2023-2024 sales.\n# Note: Category code #1 is for residential.\nresidential_prop &lt;- properties %&gt;%\n  filter(.,\n         category_code == 1,\n         startsWith(sale_date, \"2023\") | startsWith(sale_date, \"2024\"))\n\n# Drop empty variables or variables not needed for model.\nresidential_prop &lt;- residential_prop %&gt;%\n  select(c(basement_type, ac_binary, fireplace_num, fuel_type, garage_num,\n           bath_num, story_num, sale_date, sale_price,\n           square_feet, year_built, shape)\n         )\n\n# Make empty character column values NA.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(across(where(is.character), ~na_if(., \"\")))\n\n\n\n\nCode\n# Drop prices that are less than $10,000 as a catch-all (might not be as reflective for rural areas). Avoiding dropping prices based on percent of assessed value since property assessments can be biased against minoritized communities. Ideal drop would add deed type to drop any family or forced transfers.\nresidential_prop &lt;- residential_prop %&gt;%\n  filter(sale_price &gt; 10000,\n         square_feet &gt; 0) %&gt;%\n  mutate(.,\n         price_per_sqft = sale_price / square_feet) %&gt;%\n  filter(.,\n         price_per_sqft &gt; quantile(price_per_sqft, 0.05, na.rm = TRUE))\n\n\n\n\nCode\n# Create building age column, change central air to binary, and adjust basement and fuel types.\n# Create log value for the sale price.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(ln_sale_price = log(sale_price),\n         age = 2025 - year_built,\n         ln_square_feet = log(square_feet),\n         ac_binary = case_when(\n           ac_binary == \"Y\" ~ 1,\n           ac_binary == \"N\" ~ 0),\n         basement_type = case_when(\n           basement_type == \"0\" ~ \"None\",\n           basement_type == \"A\" ~ \"Full Finished\",\n           basement_type == \"B\" ~ \"Full Semi-Finished\",\n           basement_type == \"C\" ~ \"Full Unfinished\",\n           basement_type == \"D\" ~ \"Full Unknown Finish\",\n           basement_type == \"E\" ~ \"Partial Finished\",\n           basement_type == \"F\" ~ \"Partial Semi-Finished\",\n           basement_type == \"G\" ~ \"Partial Unfinished\",\n           basement_type == \"H\" ~ \"Partial Unknown Finish\",\n           basement_type == \"I\" ~ \"Unknown Size Finished\",\n           basement_type == \"J\" ~ \"Unknown Size Unfinished\"),\n         fuel_type = case_when(\n           fuel_type == \"A\" ~ \"Natural Gas\",\n           fuel_type == \"B\" ~ \"Oil Heat\",\n           fuel_type == \"C\" ~ \"Electric\",\n           fuel_type == \"D\" ~ \"Coal\",\n           fuel_type == \"E\" ~ \"Solar\",\n           fuel_type == \"F\" ~ \"Wood\",\n           fuel_type == \"G\" ~ \"Other\",\n           fuel_type == \"H\" ~ \"None\")\n         )\n\n\n\n\nCode\n# Turn categorical data into factors so OLS regression doesn't handle the data as a list of strings.\nresidential_prop$basement_type &lt;- as.factor(residential_prop$basement_type)\nresidential_prop$fuel_type &lt;- as.factor(residential_prop$fuel_type)\n\n# Change the reference categories for baseline comparison.\nresidential_prop$basement_type &lt;- relevel(residential_prop$basement_type, ref = \"None\")\nresidential_prop$fuel_type &lt;- relevel(residential_prop$fuel_type, ref = \"Natural Gas\")\n\n# Place fuel type with 10 or less counts into other category.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(fuel_type = fct_lump_min(fuel_type, min = 11, other_level = \"Other\"))\n\n\n\n\nCode\n# Fixed effect temporal market fluctuations. Based on sale date, splitting the years into quarters (Q1, Q2, Q3, Q4). Potential fixed effect.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(\n    quarters_fe = quarter(as_datetime(sale_date))\n    )\n\n# Make it a factor.\nresidential_prop$quarters_fe &lt;- factor(residential_prop$quarters_fe)\n\n\n\n\nCode\n# Capture dimensions.\nafter_property_dimension &lt;- dim(residential_prop)\n\n# Convert residential property to geodataframe. Use EPSG 2272 for South Pennsylvania in feet.\n# Drop shape when finished creating geometry.\nresidential_prop_gdf &lt;- residential_prop %&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%\n  st_as_sf(crs = 2272) %&gt;%\n  rename(geometry_point = geometry) %&gt;%\n  select(-c(shape))"
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#primary-philadelphia-sales-data-opendataphilly",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#primary-philadelphia-sales-data-opendataphilly",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "",
    "text": "Code\n# Import relevant libraries.\nlibrary(car)\nlibrary(dplyr)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(sf)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(tidycensus)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tmap)\nlibrary(units)\n\noptions(scipen = 999)\n\n\n\n\nCode\n# Property data.\nproperties_path &lt;- \"data/philly_properties.csv\"\nproperties &lt;- read.csv(properties_path)\n\n# Capture dimensions.\nog_property_dimension &lt;- dim(properties)\n\n# Set Census API key.\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n\n\n\nCode\n# All variables are character strings, remove white space then convert numeric character variables to numeric classes for chosen variables.\n\nproperties &lt;- properties %&gt;%\n  mutate(across(where(is.character), str_trim),\n         across(c(fireplaces, garage_spaces, number_of_bathrooms, number_stories,\n                  sale_price, total_livable_area, year_built), as.numeric)) %&gt;%\n  rename(fireplace_num = fireplaces,\n         garage_num = garage_spaces,\n         bath_num = number_of_bathrooms,\n         story_num = number_stories,\n         square_feet = total_livable_area,\n         basement_type = basements,\n         ac_binary = central_air,\n         fuel_type = fuel,\n         )\n\n\n\n\nCode\n# Filter to residential properties and 2023-2024 sales.\n# Note: Category code #1 is for residential.\nresidential_prop &lt;- properties %&gt;%\n  filter(.,\n         category_code == 1,\n         startsWith(sale_date, \"2023\") | startsWith(sale_date, \"2024\"))\n\n# Drop empty variables or variables not needed for model.\nresidential_prop &lt;- residential_prop %&gt;%\n  select(c(basement_type, ac_binary, fireplace_num, fuel_type, garage_num,\n           bath_num, story_num, sale_date, sale_price,\n           square_feet, year_built, shape)\n         )\n\n# Make empty character column values NA.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(across(where(is.character), ~na_if(., \"\")))\n\n\n\n\nCode\n# Drop prices that are less than $10,000 as a catch-all (might not be as reflective for rural areas). Avoiding dropping prices based on percent of assessed value since property assessments can be biased against minoritized communities. Ideal drop would add deed type to drop any family or forced transfers.\nresidential_prop &lt;- residential_prop %&gt;%\n  filter(sale_price &gt; 10000,\n         square_feet &gt; 0) %&gt;%\n  mutate(.,\n         price_per_sqft = sale_price / square_feet) %&gt;%\n  filter(.,\n         price_per_sqft &gt; quantile(price_per_sqft, 0.05, na.rm = TRUE))\n\n\n\n\nCode\n# Create building age column, change central air to binary, and adjust basement and fuel types.\n# Create log value for the sale price.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(ln_sale_price = log(sale_price),\n         age = 2025 - year_built,\n         ln_square_feet = log(square_feet),\n         ac_binary = case_when(\n           ac_binary == \"Y\" ~ 1,\n           ac_binary == \"N\" ~ 0),\n         basement_type = case_when(\n           basement_type == \"0\" ~ \"None\",\n           basement_type == \"A\" ~ \"Full Finished\",\n           basement_type == \"B\" ~ \"Full Semi-Finished\",\n           basement_type == \"C\" ~ \"Full Unfinished\",\n           basement_type == \"D\" ~ \"Full Unknown Finish\",\n           basement_type == \"E\" ~ \"Partial Finished\",\n           basement_type == \"F\" ~ \"Partial Semi-Finished\",\n           basement_type == \"G\" ~ \"Partial Unfinished\",\n           basement_type == \"H\" ~ \"Partial Unknown Finish\",\n           basement_type == \"I\" ~ \"Unknown Size Finished\",\n           basement_type == \"J\" ~ \"Unknown Size Unfinished\"),\n         fuel_type = case_when(\n           fuel_type == \"A\" ~ \"Natural Gas\",\n           fuel_type == \"B\" ~ \"Oil Heat\",\n           fuel_type == \"C\" ~ \"Electric\",\n           fuel_type == \"D\" ~ \"Coal\",\n           fuel_type == \"E\" ~ \"Solar\",\n           fuel_type == \"F\" ~ \"Wood\",\n           fuel_type == \"G\" ~ \"Other\",\n           fuel_type == \"H\" ~ \"None\")\n         )\n\n\n\n\nCode\n# Turn categorical data into factors so OLS regression doesn't handle the data as a list of strings.\nresidential_prop$basement_type &lt;- as.factor(residential_prop$basement_type)\nresidential_prop$fuel_type &lt;- as.factor(residential_prop$fuel_type)\n\n# Change the reference categories for baseline comparison.\nresidential_prop$basement_type &lt;- relevel(residential_prop$basement_type, ref = \"None\")\nresidential_prop$fuel_type &lt;- relevel(residential_prop$fuel_type, ref = \"Natural Gas\")\n\n# Place fuel type with 10 or less counts into other category.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(fuel_type = fct_lump_min(fuel_type, min = 11, other_level = \"Other\"))\n\n\n\n\nCode\n# Fixed effect temporal market fluctuations. Based on sale date, splitting the years into quarters (Q1, Q2, Q3, Q4). Potential fixed effect.\nresidential_prop &lt;- residential_prop %&gt;%\n  mutate(\n    quarters_fe = quarter(as_datetime(sale_date))\n    )\n\n# Make it a factor.\nresidential_prop$quarters_fe &lt;- factor(residential_prop$quarters_fe)\n\n\n\n\nCode\n# Capture dimensions.\nafter_property_dimension &lt;- dim(residential_prop)\n\n# Convert residential property to geodataframe. Use EPSG 2272 for South Pennsylvania in feet.\n# Drop shape when finished creating geometry.\nresidential_prop_gdf &lt;- residential_prop %&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%\n  st_as_sf(crs = 2272) %&gt;%\n  rename(geometry_point = geometry) %&gt;%\n  select(-c(shape))"
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#spatial-data-opendataphilly",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#spatial-data-opendataphilly",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Spatial Data (OpenDataPhilly)",
    "text": "Spatial Data (OpenDataPhilly)\n\n\nCode\n# Read in Philadelpha census tracts.\nphilly_tracts_path &lt;- \"data/philly_tracts/philly_tracts.shp\"\nphilly_tracts &lt;- st_read(philly_tracts_path)\n\n\nReading layer `philly_tracts' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\midterm-project\\data\\philly_tracts\\philly_tracts.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3446 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.51985 ymin: 39.7198 xmax: -74.68956 ymax: 42.51607\nGeodetic CRS:  NAD83\n\n\nCode\n# Match CRS.\nphilly_tracts &lt;- st_transform(philly_tracts, crs = 2272)\n\n# Left spatial join.\nresidential_points &lt;- st_join(residential_prop_gdf, philly_tracts)\n\n# Drop unnecessary columns and remove incomplete observations (rows) for upcoming spatial feature computations.\nresidential_points &lt;- residential_points %&gt;%\n  select(-c(FUNCSTAT, MTFCC, NAMELSAD, NAME,\n            STATEFP, COUNTYFP, TRACTCE)) %&gt;%\n  na.omit(.)\n\n# Remove unneeded datasets for housekeeping and call garbage collector to reduce memory.\nrm(properties, residential_prop, residential_prop_gdf)\ngc()\n\n\n           used  (Mb) gc trigger   (Mb)  max used   (Mb)\nNcells  8660780 462.6   12563275  671.0  11778460  629.1\nVcells 73531134 561.0  138446873 1056.3 138446681 1056.3\n\n\n\n\nCode\n# Proximity to downtown.\n\n# Decided on Euclidean distance because network proximity computation is demanding on thousands of points, even with parallel programming.\n\n# Create single Center City point feature based on City Hall coordinates.\ncenter_city &lt;- st_sfc(st_point(c(-75.163500, 39.952800)), crs = 4326) %&gt;%\n  st_transform(crs = 2272)\n\n# Need to add mile units for operations. Then remove units object for easier plotting.\nresidential_points$city_dist_mi &lt;- (st_distance(residential_points, st_union(center_city))) %&gt;%\n  set_units(\"mi\") %&gt;%\n  drop_units() %&gt;%\n  as.numeric()\n\n# Log transform because distance benefit diminishes, for potential use.\nresidential_points$ln_city_dist &lt;- log(residential_points$city_dist_mi + 0.1)\n\n\n\n\nCode\n# Transit proximity.\n# Major cities could be distance to nearest transit like metro/rail stations, but suburban and rural areas might be better served by distance to nearest major highway.\n# Read in SEPTA stops.\nsepta_stops_path &lt;- \"data/septa_stops.csv\"\n\nsepta_stops_df &lt;- read.csv(septa_stops_path)\n\n# Make csv a geodataframe.\nsepta_stops &lt;- septa_stops_df %&gt;%\n  st_as_sf(., coords = c(\"Lon\", \"Lat\"), crs = 4326)\n\n# Match CRS.\nsepta_stops &lt;- septa_stops %&gt;%\n  st_transform(., crs = 2272)\n\n# Stops are duplicated for the same station because the data includes directions for all cardinal directions as well as bus, rail, and trolley for the same location. This means a single station could have more than one point representing a single location residents go to commute.\n# Create new column with stop name without the cardinal suffixes and keep only the unique station values.\nsepta_stops &lt;- septa_stops %&gt;%\n  mutate(stations = if_else(\n    str_detect(StopAbbr, \"NO$|SO$|EA$|WE$|NE$|NW$|SE$|SW$\"),\n    str_sub(StopAbbr, end = -3),\n    str_sub(StopAbbr)\n    )\n  ) %&gt;%\n  distinct(stations, .keep_all = TRUE)\n\n# Create buffer zone for stops within a half mile. This is ~10 minute walk, depending on topography.\n# Note: EPSG 2272 is measured in feet, not miles.\nsepta_distance &lt;- st_buffer(residential_points, 2640)\n\n# Create number of stops in the buffer zone.\nsepta_stations &lt;- st_intersects(septa_distance, septa_stops)\n\n# Append buffer zone counts and put into main tract data. Create a logged version for potential use as well because distance benefit tapers off.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(\n    septa_half_mi = lengths(septa_stations),\n    ln_septa_half_mi = log(septa_half_mi + 0.1)\n  )\n\n\n\n\nCode\n# Park proximity / size. Measuring distance is important for accessibility, but the size of the park often matters because a property near a block-sized pocket of green space is not equivalent to being near a large one like Wissahickon Valley Park.\n\n# Read in geojson data.\nparks_path &lt;- \"data/parks.geojson\"\n\nparks &lt;- st_read(parks_path)\n\n\nReading layer `parks' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\midterm-project\\data\\parks.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 63 features and 18 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.2837 ymin: 39.87048 xmax: -74.95865 ymax: 40.13191\nGeodetic CRS:  WGS 84\n\n\nCode\n# Match CRS and filter by parks.\nparks &lt;- parks %&gt;%\n  st_transform(., crs = 2272) %&gt;%\n  filter(str_detect(USE_, \"PARK\"))\n\n# Get distance to the edge of the nearest park.\n# Note: Don't try to do spatial operations in apply() and mutate().\n# Distance matrix of residential properties to parks.\nparks_matrix &lt;- st_distance(residential_points, parks)\n\n# Get the nearest distance for each point.\nresidential_points$parks_mile &lt;- apply(parks_matrix, 1, min)\n\n# Convert to miles.\nresidential_points$parks_mile &lt;- as.numeric(residential_points$parks_mile) / 5280\n\n# Log parks data for potential use because of diminishing distance benefits.\nresidential_points$ln_park_dist &lt;- as.numeric(log(residential_points$parks_mile + 0.1))\n\n\n\n\nCode\n# Convenience/Food points of interest. Using kNN to measure the density of these amenities rather than nearest amenity point.\namenities_path &lt;- \"data/osm_pois/osm_pois.shp\"\namenities &lt;- st_read(amenities_path)\n\n\nReading layer `osm_pois' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\midterm-project\\data\\osm_pois\\osm_pois.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 65127 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.52111 ymin: 39.71816 xmax: -74.69473 ymax: 42.25797\nGeodetic CRS:  WGS 84\n\n\nCode\n# Filter amenities by convenience and food.\namenities &lt;- amenities %&gt;%\n  filter(fclass %in% c(\n    \"atm\", \"bakery\", \"bank\", \"bar\", \"beauty_shop\", \"biergarten\", \"bookshop\",\n    \"butcher\", \"cafe\", \"convenience\", \"department_store\", \"fast_food\", \"food_court\",\n    \"greengrocer\", \"hairdresser\", \"kiosk\", \"laundry\", \"market_place\", \"pharmacy\",\n    \"mall\", \"pub\", \"restaurant\", \"supermarket\"\n  )\n         ) %&gt;%\n  st_transform(., crs = 2272)\n\n# Distance matrix of residential properties to amenities.\namenities_matrix &lt;- st_distance(residential_points, amenities)\n\n\n\n\nCode\n# k-Nearest Neighbors (kNN) function.\nknn_distance &lt;- function(distance_matrix, k) {\n  apply(distance_matrix, 1, function(distances){\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create kNN feature for amenities. k = 4 to balance for urban and suburban areas, probably not as representative of rural areas.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(\n    knn_amenity_mi = as.numeric(knn_distance(amenities_matrix, k = 4))\n  )\n\n# Convert to miles.\nresidential_points$knn_amenity_mi &lt;- as.numeric(residential_points$knn_amenity_mi / 5280)\n\n\n\n\nCode\n# Fixed effect neighborhoods.\nneighborhoods_path &lt;- \"data/philadelphia_neighborhoods/philadelphia_neighborhoods.shp\"\nphilly_neighborhoods &lt;- st_read(neighborhoods_path)\n\n\nReading layer `philadelphia_neighborhoods' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\midterm-project\\data\\philadelphia_neighborhoods\\philadelphia_neighborhoods.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 159 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.28026 ymin: 39.86701 xmax: -74.95576 ymax: 40.13799\nGeodetic CRS:  WGS 84\n\n\nCode\n# Match CRS.\nphilly_neighborhoods &lt;- philly_neighborhoods %&gt;%\n  st_transform(., crs = 2272)\n\n# Join to residential points and rename to neighborhoods.\nresidential_points &lt;- residential_points %&gt;%\n  st_join(., philly_neighborhoods) %&gt;%\n  rename(neighborhood_fe = MAPNAME)\n\n# Make the neighborhoods a factor.\nresidential_points$neighborhood_fe &lt;- relevel(factor(residential_points$neighborhood_fe), ref = \"East Falls\")\n\n# Place neighborhoods with 10 or less sales into a small neighborhoods category.\nresidential_points &lt;- residential_points %&gt;%\n  mutate(neighborhood_fe = fct_lump_min(neighborhood_fe, min = 11, other_level = \"Small Neighborhoods\"))\n\n\n\n\nCode\n# Capture spatial feature dimensions.\nafter_feat_eng_dimension &lt;- dim(residential_points)\n\n\n\n\nCode\n# Spatial feature creation table, select spatial features into a separate data frame and drop geometry.\nspatial_feature_df &lt;- residential_points %&gt;%\n  select(c(city_dist_mi, ln_city_dist, septa_half_mi, ln_septa_half_mi,\n           parks_mile, ln_park_dist, knn_amenity_mi)) %&gt;%\n  na.omit(.) %&gt;%\n  st_drop_geometry()\n\n# Create a tibble from the selected spatial features.\nspatial_summary &lt;- tibble(\n  \"Spatial Features\" = names(spatial_feature_df),\n  \"Description\" = c(\"Distance from city (mi).\", \"Log of distance from city.\", \"Within 0.5mi of SEPTA station.\",\n                    \"Log of 0.5 SEPTA station.\", \"Distance from nearest park (mi).\",\n                    \"Log of distance from nearest park.\", \"k-Nearest Neighbors convenience and food amenities.\")\n)\n\n# Make Kable of spatial features.\nspatial_kable &lt;- kable(spatial_summary,\n                       caption = \"Feature Engineered Variables\",\n                       format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\",\n                full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"5cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\", bold = TRUE)\n\nspatial_kable\n\n\n\nFeature Engineered Variables\n\n\nSpatial Features\nDescription\n\n\n\n\ncity_dist_mi\nDistance from city (mi).\n\n\nln_city_dist\nLog of distance from city.\n\n\nsepta_half_mi\nWithin 0.5mi of SEPTA station.\n\n\nln_septa_half_mi\nLog of 0.5 SEPTA station.\n\n\nparks_mile\nDistance from nearest park (mi).\n\n\nln_park_dist\nLog of distance from nearest park.\n\n\nknn_amenity_mi\nk-Nearest Neighbors convenience and food amenities.\n\n\n\n\n\nPrimary:\nFrom the CSV, we are analyzing the conditions of basements, number of fireplaces, garage spaces, number of bathrooms, number of stories, the total livable area in square feet, the existence of central air, and type of fuel used on the property. We filtered residential category code with the sale dates between 2023 and 2024. We eliminated property prices &lt;10k. Rather than adhering to the percentage of assessed value as a guide for this filter, for it could incorporate marginalized bias, filtering the property prices removes non-market transactions but still incorporates a wide diversity of communities.\nThe forced the central air characteristic to become binary rather than “Y” and “N” and made sure to turn the categorical data to factors. The reference categories for types of basements is “None” and for fuel type, it’s “Natural Gas”. We including a building age category computed from the year built data. We logged the square footage and the sale price to correct for right-skewedness.\nSpatial:\nWe inserted the Philadelphia census tracts, changing the CRS to 2272, the ideal projection for SE Pennsylvania analysis. We decided to perform a log transformation on the following variables - city_dist (distance from City Hall in Center City), septa_half_mi (half mile buffer zone from all septa stops within the city geometry), and parks_dist (distance to edge of nearest park in miles) - because their effects on housing prices were non-linear. This transformation ensures that changes in proximity are measured more consistently across the range of distances, rather than being dominated by properties very close to these features.\nRegarding amenities, we used k-NN (k nearest neighbors) to measure the density of amenity accessibility rather than individual point data. The amenities are as follows: ATM, bakery, bank, bar, beauty shop, biergarten, bookshop, butcher, café, convenience, department store, fast food, food court, greengrocer, hairdresser, kiosk, laundry, marketplace, pharmacy, mall, pub, restaurant, supermarket. We filtered by convenience and food, transformed the CRS to 2272 for consistency, and then developed a matrix. The distance was inverted, log-transformed to account for diminishing returns, and scaled it to produce a single numeric value in which higher, positive values indicate greater accessibility to amenities.\nWe included neighborhoods as fixed effects to help explain unknown, unquantifiable factors like cultural atmosphere and other neighborhood-specific factors we cannot statistically account for that may influence housing prices. It was converted into a factor so each neighborhood can receive its own baseline model. Fiscal quarters were also introduced as fixed effects; splitting a year into 4 quarters for unknown factors when it comes to purchasing property (e.g. people are more likely to buy real estate in spring and summer).\n\nCensus Data (TidyCensus)\n\n\nCode\n# Open tidycensus data. Using 2023 data, because we are looking at sales 2023-2024\nacs_vars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n# Get acs dimensions.\nog_acs_dimension &lt;- dim(acs_vars)\n\n# The variables that we want from tidycensus\nvariables &lt;- c(\n  median_household_income = \"B19013_001\",\n  total_pop = \"B01003_001\",\n  poverty_white = \"B17001A_001\", # To get poverty percentage\n  poverty_black = \"B17001B_001\",\n  poverty_native = \"B17001C_001\",\n  poverty_asian = \"B17001D_001\",\n  poverty_islander = \"B17001E_001\",\n  poverty_other = \"B17001F_001\",\n  poverty_multiracial = \"B17001G_001\",\n  male_18_24_bach = \"B15001_009\", # Tracts only show bachelor's degrees, unless we want to look at only people 25+\n  male_25_34_bach = \"B15001_017\",\n  male_35_44_bach = \"B15001_025\",\n  male_45_64_bach = \"B15001_033\",\n  male_65plus_bach = \"B15001_041\",\n  female_18_24_bach = \"B15001_050\",\n  female_25_34_bach = \"B15001_058\",\n  female_35_44_bach = \"B15001_066\",\n  female_45_64_bach = \"B15001_074\",\n  female_65plus_bach = \"B15001_082\",\n  total_vacant = \"B25005_001\", # To get vacancy percentage\n  white_total_units = \"B25032A_001\", # Need total units to get percentage of single, detached units and vacant units.\n  white_single_family = \"B25032A_002\",\n  black_total_units = \"B25032B_001\",\n  black_single_family = \"B25032B_002\",\n  native_total_units = \"B25032C_001\",\n  native_single_family = \"B25032C_002\",\n  asian_total_units = \"B25032D_001\",\n  asian_single_family = \"B25032D_002\",\n  islander_total_units = \"B25032E_001\",\n  islander_single_family = \"B25032E_002\",\n  other_total_units = \"B25032F_001\",\n  other_single_family = \"B25032F_002\",\n  multiracial_total_units = \"B25032G_001\",\n  multiracial_single_family = \"B25032G_002\",\n  medhhinc_white = \"B19013A_001\", # Median Household Income\n  medhhinc_black = \"B19013B_001\",\n  medhhinc_native = \"B19013C_001\", \n  medhhinc_asian = \"B19013D_001\", \n  medhhinc_other = \"B19013F_001\",  # There is no tract data for native hawiian/pacific islander, I'm including it with other\n  medhhinc_multiracial = \"B19013G_001\", \n  white_pop = \"B01001A_001\",\n  black_pop = \"B01001B_001\",\n  native_pop = \"B01001C_001\",\n  asian_pop = \"B01001D_001\",\n  islander_pop = \"B01001E_001\",\n  other_pop = \"B01001F_001\",\n  multiracial_pop = \"B01001G_001\"\n)\n\n# We are grouping our data by tracts\nphilly_tract_acs &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = variables,\n  year = 2022,\n  survey = \"acs5\",\n  cache_table = TRUE, \n  output = \"wide\"\n)\n\n\n\n\nCode\n# Summing up the variables that we need to create our percentage variables\nphilly_tract_acs &lt;- philly_tract_acs %&gt;%\n  mutate(\n    total_poverty = poverty_whiteE + poverty_blackE + poverty_nativeE + poverty_asianE + poverty_islanderE + poverty_otherE + poverty_multiracialE, # Adding all poverty populations together \n    \n    total_bach = male_18_24_bachE + male_25_34_bachE + male_35_44_bachE + male_45_64_bachE + male_65plus_bachE + female_18_24_bachE + female_25_34_bachE + female_35_44_bachE + female_45_64_bachE + female_65plus_bachE, #Adding all bachelors degrees together\n    \n    total_units = white_total_unitsE + black_total_unitsE + native_total_unitsE + asian_total_unitsE + islander_total_unitsE + other_total_unitsE + multiracial_total_unitsE, # Total housing units\n    \n    total_single_family = white_single_familyE + black_single_familyE + native_single_familyE + asian_single_familyE + islander_single_familyE + other_single_familyE + multiracial_single_familyE # Total single family homes\n  )\n\n\n\n\nCode\n# Creating our variables that we are going to analyze\nphilly_tract_acs &lt;- philly_tract_acs %&gt;%\n  mutate(\n    pct_poverty = (total_poverty/total_popE)*100, # Divide total poverty population by total population\n\n    pct_bach = (total_bach/total_popE)*100, # Divide bachelor degree holders by total population\n    \n    pct_vacant = (total_vacantE/total_units)*100, # Divide vacant units by total housing units\n    pct_vacant = ifelse(is.infinite(pct_vacant) | total_vacantE &gt; total_units, 100, pct_vacant), # Fixing errors when units equal zero or high MOE\n    \n    pct_single_family = (total_single_family/total_units)*100, # Divide single family homes by total housing units\n    \n    medhhinc = \n  (ifelse(is.na(medhhinc_whiteE), 0, medhhinc_whiteE) * white_popE +\n   ifelse(is.na(medhhinc_blackE), 0, medhhinc_blackE) * black_popE +\n   ifelse(is.na(medhhinc_nativeE), 0, medhhinc_nativeE) * native_popE +\n   ifelse(is.na(medhhinc_asianE), 0, medhhinc_asianE) * asian_popE +\n   ifelse(is.na(medhhinc_otherE), 0, medhhinc_otherE) * (islander_popE + other_popE) +\n   ifelse(is.na(medhhinc_multiracialE), 0, medhhinc_multiracialE) * multiracial_popE) / total_popE)\n# For median household income, I had to turn all median household incomes that were NA to 0, so that it would not mess up the formula. \n# Multiplying median household income times population by race. There was no islander median household income, so I included it in other. All divided by the total population, to get the total median household income. \n\n\n\n\nCode\n# Creating a summary table \nphilly_acs_summary &lt;- philly_tract_acs %&gt;%\n  select(\n    GEOID, \n    NAME,\n    pct_poverty,\n    pct_bach,\n    pct_vacant,\n    pct_single_family,\n    medhhinc\n  )\n\n# Get after acs dimension.\nafter_acs_dimension &lt;- dim(philly_acs_summary)\n\n\n\n\nCode\n# Join primary and census data.\nfinal_data &lt;- residential_points %&gt;%\n  left_join(philly_acs_summary, by = \"GEOID\") %&gt;%\n  select(-c(sale_date, year_built, ALAND, AWATER,\n            INTPTLAT, INTPTLON, NAME.x, LISTNAME, NAME.y,\n            Shape_Leng, Shape_Area)\n         )\n\n\n\n\nCode\n# Create key variables list.\nkey_columns &lt;- c(\"sale_price\", \"ln_sale_price\", \"square_feet\", \"ln_square_feet\",\n                 \"bath_num\", \"fireplace_num\", \"garage_num\", \"ac_binary\",\n                 \"story_num\", \"age\", \"city_dist_mi\", \"ln_city_dist\",\n                 \"septa_half_mi\", \"ln_septa_half_mi\", \"parks_mile\", \"ln_park_dist\",\n                 \"knn_amenity_mi\", \"pct_poverty\", \"pct_bach\",\n                 \"pct_vacant\", \"pct_single_family\", \"medhhinc\",\n                 \"basement_type\", \"fuel_type\", \"neighborhood_fe\", \"quarters_fe\")\n\n# Reorder for key columns first and drop all rows with NA because OLS needs complete observations.\nfinal_data &lt;- final_data %&gt;%\n  select(any_of(key_columns), everything()) %&gt;%\n  na.omit(.)\n\n# Get final dimension.\nfinal_dimension &lt;- dim(final_data)\n\n\n\n\nCode\n# Separate before/after dimensions for data.\ndimensions &lt;- data.frame(\n  rows_columns = c(\"Rows\", \"Columns\"),\n  \"Property Data Before\" = og_property_dimension,\n  \"Property Data After\" = after_property_dimension,\n  \"Property Data After Feature Engineering\" = after_feat_eng_dimension,\n  \"ACS Data Before\" = og_acs_dimension,\n  \"ACS Data After\" = after_acs_dimension,\n  \"Final Data\" = final_dimension\n)\n\n# Make Kable of dimensions.\ndimensions_kable &lt;- kable(dimensions,\n                          col.names = c(\"Dimensions\", \"Property Data Before\", \"Property Data After\",\n                                        \"Property Data After Feature Engineering\",\n                                        \"ACS Data Before\", \"ACS Data After\", \"Final Data\"),\n                          digits = 2,\n                          caption = \"Before and After Data Dimensions\",\n                          format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\",\n                full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\", bold = TRUE)\n\ndimensions_kable\n\n\n\nBefore and After Data Dimensions\n\n\nDimensions\nProperty Data Before\nProperty Data After\nProperty Data After Feature Engineering\nACS Data Before\nACS Data After\nFinal Data\n\n\n\n\nRows\n559,322\n25,026\n13,743\n28,261\n3,446\n13,742\n\n\nColumns\n79\n17\n34\n4\n7\n29\n\n\n\n\n\nCensus:\nUsing tidycensus, we imported all variables that aligned with our structural data from 2023 ACS data by tracts: median household income, total population, poverty by ethnicity (White, Black, Native American, Asian, Pacific Islander, “Other,” Multiracial), males and females aged 18–65+ with bachelor’s degrees or higher, total vacancy, and total housing units by ethnicity, as well as single-family households and median household income per ethnic group. We compiled the individual poverty, bachelor’s degree, unit, and single-family household counts by ethnicity to form the following percentage variables: total_poverty, total_bach, total_units, and total_single_family.\nFrom this, we calculated pct_poverty, pct_bach, pct_vacant (accounting for ACS errors), pct_single_family, and medhhinc, transforming NAs to 0 for regression analysis.\nUsing our residential property vector data (which includes structural, spatial, and feature-engineered variables), we performed a left join on the cleaned ACS summary data by GEOID.\nAfter organizing the final dataset so key variables appear first, we generated a kable summarizing the workflow. The final dataset contains 26,344 observations and 29 columns."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#model-building-progression",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#model-building-progression",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Model Building Progression",
    "text": "Model Building Progression\nCheck for multicollinearity:\n\n\nCode\n# VIF check for multicollinearity.\nvif_check &lt;- lm(ln_sale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + story_num  + garage_num + ln_septa_half_mi + ln_park_dist + ln_city_dist + basement_type + fuel_type, data = residential_points)\n\nvif(vif_check)\n\n\n                     GVIF Df GVIF^(1/(2*Df))\nln_square_feet   2.141293  1        1.463316\nbath_num         1.898869  1        1.377995\nac_binary        1.321625  1        1.149619\nfireplace_num    1.255247  1        1.120378\nstory_num        1.484250  1        1.218298\ngarage_num       2.278772  1        1.509560\nln_septa_half_mi 3.027921  1        1.740092\nln_park_dist     1.042800  1        1.021176\nln_city_dist     3.375708  1        1.837310\nbasement_type    3.090736 10        1.058042\nfuel_type        1.049685  3        1.008114\n\n\n\n\nCode\n# Build Model step by step\n# First Model (structural features only)\n\nfirst_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data)\n\nsummary(first_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2), data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-946748  -77732  -12797   55389 4964714 \n\nCoefficients:\n                                          Estimate    Std. Error t value\n(Intercept)                          -1600341.5644    47253.3429 -33.867\nln_square_feet                         269653.6615     6788.7652  39.721\nbath_num                                50882.9698     2849.4691  17.857\nac_binary                               92503.4518     3521.1431  26.271\nfireplace_num                          139112.9209     4562.8499  30.488\nstory_num                               35580.0358     3368.1737  10.564\ngarage_num                              73100.2236     4143.2172  17.643\nbasement_typeFull Finished             -56656.4572    10210.3683  -5.549\nbasement_typeFull Semi-Finished        -68695.6797    11788.3042  -5.827\nbasement_typeFull Unfinished           -71544.1253    10619.4100  -6.737\nbasement_typeFull Unknown Finish       -85234.6422    10988.6228  -7.757\nbasement_typePartial Finished         -123696.2917    10824.4574 -11.427\nbasement_typePartial Semi-Finished    -134365.3131    11283.6000 -11.908\nbasement_typePartial Unfinished       -128970.6719    13686.9805  -9.423\nbasement_typePartial Unknown Finish   -135902.6994    13315.8037 -10.206\nbasement_typeUnknown Size Finished      58578.8098    49757.4371   1.177\nbasement_typeUnknown Size Unfinished   -25344.9839    38787.2223  -0.653\nfuel_typeElectric                       -9301.9858    10307.7869  -0.902\nfuel_typeOil Heat                        -540.2228    20885.3920  -0.026\nfuel_typeOther                         257792.1255    62355.3517   4.134\nage                                     -4478.8377      145.9713 -30.683\nI(age^2)                                   26.3993        0.8056  32.769\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                            &lt; 0.0000000000000002 ***\ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished            0.00000002927742725 ***\nbasement_typeFull Semi-Finished       0.00000000575462653 ***\nbasement_typeFull Unfinished          0.00000000001680098 ***\nbasement_typeFull Unknown Finish      0.00000000000000933 ***\nbasement_typePartial Finished        &lt; 0.0000000000000002 ***\nbasement_typePartial Semi-Finished   &lt; 0.0000000000000002 ***\nbasement_typePartial Unfinished      &lt; 0.0000000000000002 ***\nbasement_typePartial Unknown Finish  &lt; 0.0000000000000002 ***\nbasement_typeUnknown Size Finished                  0.239    \nbasement_typeUnknown Size Unfinished                0.513    \nfuel_typeElectric                                   0.367    \nfuel_typeOil Heat                                   0.979    \nfuel_typeOther                        0.00003582455204890 ***\nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 175600 on 13720 degrees of freedom\nMultiple R-squared:  0.5926,    Adjusted R-squared:  0.592 \nF-statistic: 950.5 on 21 and 13720 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Second Model (structural features + census features)\n\nsecond_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # Census feature.\n                    data = final_data)\n\nsummary(second_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family, \n    data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-986519  -66233   -4791   50723 5083386 \n\nCoefficients:\n                                           Estimate     Std. Error t value\n(Intercept)                          -1657315.06881    43605.53969 -38.007\nln_square_feet                         253118.55357     6307.84302  40.128\nbath_num                                57006.19468     2628.26335  21.690\nac_binary                               51835.01737     3358.99340  15.432\nfireplace_num                          123565.08573     4242.29279  29.127\nstory_num                                7968.41354     3192.54360   2.496\ngarage_num                              78214.57757     3858.22989  20.272\nbasement_typeFull Finished             -14732.49113     9430.76150  -1.562\nbasement_typeFull Semi-Finished        -32016.37282    10875.01516  -2.944\nbasement_typeFull Unfinished           -29578.12515     9803.78380  -3.017\nbasement_typeFull Unknown Finish       -37097.04187    10153.08951  -3.654\nbasement_typePartial Finished          -85338.66669    10006.30270  -8.528\nbasement_typePartial Semi-Finished     -84103.05213    10454.72829  -8.044\nbasement_typePartial Unfinished        -77849.36109    12644.21848  -6.157\nbasement_typePartial Unknown Finish    -89193.51530    12299.71498  -7.252\nbasement_typeUnknown Size Finished      94023.39096    45782.49895   2.054\nbasement_typeUnknown Size Unfinished    17742.27294    35702.54782   0.497\nfuel_typeElectric                        4024.94651     9509.01594   0.423\nfuel_typeOil Heat                       -8357.59723    19218.43367  -0.435\nfuel_typeOther                         146353.99212    57422.32275   2.549\nage                                     -3216.68050      136.72500 -23.527\nI(age^2)                                   19.96630        0.75560  26.424\nmedhhinc                                    2.62135        0.05512  47.556\npct_vacant                                506.82584      221.43981   2.289\npct_single_family                       -1820.98270      167.65022 -10.862\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                                        0.012574 *  \ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                       0.118270    \nbasement_typeFull Semi-Finished                  0.003245 ** \nbasement_typeFull Unfinished                     0.002557 ** \nbasement_typeFull Unknown Finish                 0.000259 ***\nbasement_typePartial Finished        &lt; 0.0000000000000002 ***\nbasement_typePartial Semi-Finished   0.000000000000000937 ***\nbasement_typePartial Unfinished      0.000000000762449291 ***\nbasement_typePartial Unknown Finish  0.000000000000433699 ***\nbasement_typeUnknown Size Finished               0.040024 *  \nbasement_typeUnknown Size Unfinished             0.619234    \nfuel_typeElectric                                0.672100    \nfuel_typeOil Heat                                0.663661    \nfuel_typeOther                                   0.010822 *  \nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\nmedhhinc                             &lt; 0.0000000000000002 ***\npct_vacant                                       0.022108 *  \npct_single_family                    &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 161500 on 13717 degrees of freedom\nMultiple R-squared:  0.6555,    Adjusted R-squared:  0.6549 \nF-statistic:  1087 on 24 and 13717 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Third Model (structural features + census features + spatial features)\n\nthird_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family + # Census feature.\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial \n                    data = final_data)\n\nsummary(third_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family + \n    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, \n    data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-816942  -62918   -2847   50974 5091643 \n\nCoefficients:\n                                           Estimate     Std. Error t value\n(Intercept)                          -1781195.80963    42334.56160 -42.074\nln_square_feet                         263309.01771     6049.05829  43.529\nbath_num                                50713.57098     2526.41632  20.073\nac_binary                               46036.50868     3229.25216  14.256\nfireplace_num                          127678.22421     4059.52596  31.452\nstory_num                               -3447.03355     3085.21586  -1.117\ngarage_num                              87656.05910     3701.77217  23.679\nbasement_typeFull Finished               5895.68799     9057.10484   0.651\nbasement_typeFull Semi-Finished         -4392.18676    10439.57843  -0.421\nbasement_typeFull Unfinished            -9094.58940     9422.68066  -0.965\nbasement_typeFull Unknown Finish       -16102.64276     9754.15342  -1.651\nbasement_typePartial Finished          -41565.01250     9663.05277  -4.301\nbasement_typePartial Semi-Finished     -38554.36301    10136.29140  -3.804\nbasement_typePartial Unfinished        -47551.21985    12124.43903  -3.922\nbasement_typePartial Unknown Finish    -56260.00839    11798.54030  -4.768\nbasement_typeUnknown Size Finished     119709.84911    43779.33316   2.734\nbasement_typeUnknown Size Unfinished    36743.10664    34139.64024   1.076\nfuel_typeElectric                        3252.26875     9092.12397   0.358\nfuel_typeOil Heat                        1068.57062    18378.08170   0.058\nfuel_typeOther                         101957.26849    54918.42690   1.857\nage                                     -2566.18573      132.11722 -19.424\nI(age^2)                                   14.65461        0.73820  19.852\nmedhhinc                                    2.08972        0.05605  37.282\npct_vacant                              -1064.56769      225.95522  -4.711\npct_single_family                         721.85615      182.79866   3.949\ncity_dist_mi                            -2525.63513      765.79341  -3.298\nsepta_half_mi                            1838.63667       70.22731  26.181\nln_park_dist                           -16813.69954     2091.60942  -8.039\nknn_amenity_mi                         -11750.98383     8566.09770  -1.372\n                                                 Pr(&gt;|t|)    \n(Intercept)                          &lt; 0.0000000000000002 ***\nln_square_feet                       &lt; 0.0000000000000002 ***\nbath_num                             &lt; 0.0000000000000002 ***\nac_binary                            &lt; 0.0000000000000002 ***\nfireplace_num                        &lt; 0.0000000000000002 ***\nstory_num                                        0.263896    \ngarage_num                           &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                       0.515092    \nbasement_typeFull Semi-Finished                  0.673963    \nbasement_typeFull Unfinished                     0.334471    \nbasement_typeFull Unknown Finish                 0.098792 .  \nbasement_typePartial Finished        0.000017086847334551 ***\nbasement_typePartial Semi-Finished               0.000143 ***\nbasement_typePartial Unfinished      0.000088269793253619 ***\nbasement_typePartial Unknown Finish  0.000001876161074540 ***\nbasement_typeUnknown Size Finished               0.006258 ** \nbasement_typeUnknown Size Unfinished             0.281830    \nfuel_typeElectric                                0.720572    \nfuel_typeOil Heat                                0.953635    \nfuel_typeOther                                   0.063401 .  \nage                                  &lt; 0.0000000000000002 ***\nI(age^2)                             &lt; 0.0000000000000002 ***\nmedhhinc                             &lt; 0.0000000000000002 ***\npct_vacant                           0.000002484237299472 ***\npct_single_family                    0.000078899018066142 ***\ncity_dist_mi                                     0.000976 ***\nsepta_half_mi                        &lt; 0.0000000000000002 ***\nln_park_dist                         0.000000000000000982 ***\nknn_amenity_mi                                   0.170148    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 154400 on 13713 degrees of freedom\nMultiple R-squared:  0.6853,    Adjusted R-squared:  0.6846 \nF-statistic:  1066 on 28 and 13713 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Build Model step by step\n# Final Model \n## (structural features + census features + spatial features + interactions and fixed effects)\n\nfinal_model &lt;- lm(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family + # Census feature.\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial \n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe + quarters_fe, # Fixed effect \n                    data = final_data)\n\nsummary(final_model)\n\n\n\nCall:\nlm(formula = sale_price ~ ln_square_feet + bath_num + ac_binary + \n    fireplace_num + story_num + garage_num + basement_type + \n    fuel_type + age + I(age^2) + medhhinc + pct_vacant + pct_single_family + \n    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + \n    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + \n    neighborhood_fe + quarters_fe, data = final_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-797069  -47661    1774   44023 5122998 \n\nCoefficients:\n                                                 Estimate     Std. Error\n(Intercept)                                -1680423.17837    45558.65141\nln_square_feet                               265937.03690     5681.37118\nbath_num                                      43663.93044     2310.00232\nac_binary                                     38464.17499     3023.18590\nfireplace_num                                 88608.64257     3853.51007\nstory_num                                     -4144.06972     2940.58826\ngarage_num                                    77189.41047     3409.24428\nbasement_typeFull Finished                    98525.27985     8828.49735\nbasement_typeFull Semi-Finished               88799.65176     9987.28877\nbasement_typeFull Unfinished                  78498.57361     9132.28816\nbasement_typeFull Unknown Finish              70571.65644     9388.12707\nbasement_typePartial Finished                 45806.43683     9334.39091\nbasement_typePartial Semi-Finished            42102.78827     9834.77606\nbasement_typePartial Unfinished               34619.35768    11416.85091\nbasement_typePartial Unknown Finish           28615.58029    11146.88289\nbasement_typeUnknown Size Finished           164012.01649    39570.27927\nbasement_typeUnknown Size Unfinished         129088.32440    30905.12976\nfuel_typeElectric                            -10424.55766     8245.32392\nfuel_typeOil Heat                              4650.15056    16495.40912\nfuel_typeOther                                 5774.27149    49418.05374\nage                                           -1460.87062      129.89087\nI(age^2)                                          5.81812        0.74348\nmedhhinc                                          0.60322        0.08449\npct_vacant                                     -965.50624      293.85463\npct_single_family                               212.09860      214.62153\ncity_dist_mi                                  -4553.57975     3216.84114\nsepta_half_mi                                  1424.35076      193.72573\nln_park_dist                                  -6814.18834     4839.55248\nknn_amenity_mi                               -18870.06231    24171.57110\nneighborhood_feAcademy Gardens                30669.64847    29014.63400\nneighborhood_feAllegheny West                -84326.20368    19908.73956\nneighborhood_feAndorra                       -43018.51128    30565.19587\nneighborhood_feAston-Woodbridge               29590.48833    33122.43202\nneighborhood_feBella Vista                    85541.46383    24484.74222\nneighborhood_feBelmont                       -81778.48637    41082.96239\nneighborhood_feBrewerytown                   -77421.65693    18824.34153\nneighborhood_feBridesburg                      1710.91055    22154.76396\nneighborhood_feBurholme                      -18606.61082    36054.50784\nneighborhood_feBustleton                      22257.51027    24641.24298\nneighborhood_feCarroll Park                  -84704.42396    20556.02393\nneighborhood_feCedar Park                     42867.20752    21553.82620\nneighborhood_feCedarbrook                    -25188.42562    23491.31873\nneighborhood_feChestnut Hill                 397454.21064    20963.48914\nneighborhood_feClearview                     -91779.28146    31929.38435\nneighborhood_feCobbs Creek                   -84903.74356    16233.31855\nneighborhood_feDickinson Narrows             -51551.39712    20637.84635\nneighborhood_feDunlap                       -156952.79384    34756.05799\nneighborhood_feEast Germantown               -60417.36691    22418.31503\nneighborhood_feEast Kensington               -30367.71927    18619.09940\nneighborhood_feEast Mount Airy               -47517.28192    18313.99912\nneighborhood_feEast Oak Lane                -119250.18458    29126.52029\nneighborhood_feEast Parkside                -105116.02099    42890.08173\nneighborhood_feEast Passyunk                  17261.48248    19775.56206\nneighborhood_feEastwick                      -72554.39138    34336.10964\nneighborhood_feElmwood                       -69789.44000    18216.99419\nneighborhood_feFairhill                      -77286.29111    39526.22758\nneighborhood_feFairmount                      74788.51989    18110.38937\nneighborhood_feFeltonville                   -68221.00791    19970.91688\nneighborhood_feFern Rock                     -88213.88974    34310.11295\nneighborhood_feFishtown - Lower Kensington     6642.75225    15468.02532\nneighborhood_feFitler Square                 428945.20103    30583.16380\nneighborhood_feFox Chase                      -8172.98510    21418.40111\nneighborhood_feFrancisville                  -60922.30928    23396.55254\nneighborhood_feFrankford                     -68727.49076    18721.31012\nneighborhood_feFranklinville                -135336.58871    30074.41651\nneighborhood_feGermantown - Morton           -75339.08463    26885.42269\nneighborhood_feGermantown - Penn Knox       -147182.95037    44224.89002\nneighborhood_feGermantown - Westside        -106892.87655    39218.96860\nneighborhood_feGermany Hill                    3641.09771    26058.81120\nneighborhood_feGirard Estates                -36298.68952    18908.17503\nneighborhood_feGlenwood                     -124379.12793    29176.82389\nneighborhood_feGraduate Hospital              92492.11696    19943.72065\nneighborhood_feGrays Ferry                   -81372.93890    17428.02925\nneighborhood_feGreenwich                     -69478.31525    25173.77597\nneighborhood_feHaddington                    -92703.28897    18515.45588\nneighborhood_feHarrowgate                    -61400.43416    19376.40807\nneighborhood_feHartranft                    -136237.57343    23155.29187\nneighborhood_feHawthorne                       8354.36976    28773.97721\nneighborhood_feHolmesburg                    -24162.62772    20540.88797\nneighborhood_feHunting Park                  -60762.49980    22033.99309\nneighborhood_feJuniata Park                  -54461.67540    17926.20525\nneighborhood_feKingsessing                  -105736.34718    17253.09686\nneighborhood_feLawndale                      -45643.13310    17679.63001\nneighborhood_feLexington Park                 16746.99936    28470.63535\nneighborhood_feLogan                        -104336.51091    19993.43638\nneighborhood_feLogan Square                  371677.31683    30110.85397\nneighborhood_feLower Moyamensing             -70518.79528    17469.14127\nneighborhood_feManayunk                       20785.71956    18185.41191\nneighborhood_feMantua                        -97200.81354    27624.82311\nneighborhood_feMayfair                       -18012.86446    18324.60681\nneighborhood_feMelrose Park Gardens          -74349.82030    31937.39166\nneighborhood_feMill Creek                   -113008.50944    26822.82923\nneighborhood_feMillbrook                      13044.88706    30535.35647\nneighborhood_feModena                         49978.90381    28499.29876\nneighborhood_feMorrell Park                   18289.30618    27895.75298\nneighborhood_feNewbold                       -60272.32918    21772.58513\nneighborhood_feNicetown                      -69814.36099    37978.26098\nneighborhood_feNormandy Village              145245.77189    48998.78581\nneighborhood_feNorth Central                -109392.15757    22147.45990\nneighborhood_feNorthern Liberties             -1915.64772    19218.85306\nneighborhood_feNorthwood                    -104643.74889    23314.18573\nneighborhood_feOgontz                        -44603.99265    20151.94816\nneighborhood_feOld City                      437057.82858    45068.46756\nneighborhood_feOld Kensington                -51347.96127    20313.31020\nneighborhood_feOlney                         -75360.46915    17148.17482\nneighborhood_feOverbrook                     -97468.82258    17000.26405\nneighborhood_feOxford Circle                 -29216.20323    17815.62113\nneighborhood_fePacker Park                     1168.47197    25642.00121\nneighborhood_feParkwood Manor                 45962.04911    29491.33784\nneighborhood_fePaschall                      -76314.27498    20125.58805\nneighborhood_fePassyunk Square                 2240.73919    20602.82391\nneighborhood_fePennsport                     -37485.73218    19633.51732\nneighborhood_fePennypack                      -5582.98462    23874.93182\nneighborhood_fePennypack Woods                 4576.00314    46592.25036\nneighborhood_fePenrose                       -64836.46208    30724.87105\nneighborhood_fePoint Breeze                  -68957.41738    17701.03102\nneighborhood_feQueen Village                 105488.47678    21255.03813\nneighborhood_feRhawnhurst                      6640.73399    21732.57021\nneighborhood_feRichmond                      -32075.32706    15378.83550\nneighborhood_feRittenhouse                   421722.17051    26352.16981\nneighborhood_feRoxborough                      3864.67594    16120.16569\nneighborhood_feRoxborough Park               -41303.84099    32727.00509\nneighborhood_feSharswood                    -102909.47871    41154.89517\nneighborhood_feSociety Hill                  345350.83459    26079.97038\nneighborhood_feSomerton                       51084.57008    29047.05728\nneighborhood_feSouthwest Germantown         -107776.18251    24831.28612\nneighborhood_feSouthwest Schuylkill          -95017.78116    21103.31136\nneighborhood_feSpring Garden                 171517.01973    25804.63563\nneighborhood_feSpruce Hill                   126748.28677    29987.02426\nneighborhood_feStadium District              -28456.95502    21938.05783\nneighborhood_feStanton                      -140258.86393    21511.75897\nneighborhood_feStrawberry Mansion           -106030.57131    20726.83784\nneighborhood_feSummerdale                    -46187.87741    21709.89629\nneighborhood_feTacony                        -45600.58731    19886.66084\nneighborhood_feTioga                        -119468.81468    23729.82915\nneighborhood_feTorresdale                    -31364.19737    27023.70486\nneighborhood_feUpper Kensington              -93034.31125    18906.66383\nneighborhood_feUpper Roxborough              -37494.05482    20256.00247\nneighborhood_feWalnut Hill                   -67122.45328    30574.74421\nneighborhood_feWashington Square West         82800.69308    29877.55848\nneighborhood_feWest Central Germantown         6813.01484    27821.74276\nneighborhood_feWest Kensington               -74065.89375    20281.89566\nneighborhood_feWest Mount Airy                29317.91211    18493.81852\nneighborhood_feWest Oak Lane                 -52354.37842    18262.99934\nneighborhood_feWest Passyunk                 -83846.34957    19776.57784\nneighborhood_feWest Poplar                  -117546.61586    43427.12409\nneighborhood_feWest Powelton                -110672.71114    34305.38079\nneighborhood_feWhitman                       -48110.26150    18243.17836\nneighborhood_feWinchester Park                32508.21786    34632.32742\nneighborhood_feWissahickon                    -3310.11898    20232.93473\nneighborhood_feWissahickon Hills               6748.29913    32288.28238\nneighborhood_feWissinoming                   -29898.14143    18379.86906\nneighborhood_feWister                        -77071.61727    31383.32050\nneighborhood_feWynnefield                   -106313.16098    20099.54235\nneighborhood_feSmall Neighborhoods           -33519.28313    19019.55028\nquarters_fe2                                   3266.17711     3382.86756\nquarters_fe3                                   4915.70677     3441.82750\nquarters_fe4                                   5563.97881     3572.92555\ncity_dist_mi:knn_amenity_mi                    1442.35933     2651.46269\ncity_dist_mi:septa_half_mi                     -332.87956       41.37260\n                                           t value             Pr(&gt;|t|)    \n(Intercept)                                -36.885 &lt; 0.0000000000000002 ***\nln_square_feet                              46.809 &lt; 0.0000000000000002 ***\nbath_num                                    18.902 &lt; 0.0000000000000002 ***\nac_binary                                   12.723 &lt; 0.0000000000000002 ***\nfireplace_num                               22.994 &lt; 0.0000000000000002 ***\nstory_num                                   -1.409             0.158780    \ngarage_num                                  22.641 &lt; 0.0000000000000002 ***\nbasement_typeFull Finished                  11.160 &lt; 0.0000000000000002 ***\nbasement_typeFull Semi-Finished              8.891 &lt; 0.0000000000000002 ***\nbasement_typeFull Unfinished                 8.596 &lt; 0.0000000000000002 ***\nbasement_typeFull Unknown Finish             7.517 0.000000000000059500 ***\nbasement_typePartial Finished                4.907 0.000000934211114953 ***\nbasement_typePartial Semi-Finished           4.281 0.000018732246351384 ***\nbasement_typePartial Unfinished              3.032             0.002432 ** \nbasement_typePartial Unknown Finish          2.567             0.010265 *  \nbasement_typeUnknown Size Finished           4.145 0.000034213112365352 ***\nbasement_typeUnknown Size Unfinished         4.177 0.000029732574826675 ***\nfuel_typeElectric                           -1.264             0.206144    \nfuel_typeOil Heat                            0.282             0.778020    \nfuel_typeOther                               0.117             0.906984    \nage                                        -11.247 &lt; 0.0000000000000002 ***\nI(age^2)                                     7.825 0.000000000000005429 ***\nmedhhinc                                     7.139 0.000000000000985048 ***\npct_vacant                                  -3.286             0.001020 ** \npct_single_family                            0.988             0.323050    \ncity_dist_mi                                -1.416             0.156932    \nsepta_half_mi                                7.352 0.000000000000205800 ***\nln_park_dist                                -1.408             0.159148    \nknn_amenity_mi                              -0.781             0.435009    \nneighborhood_feAcademy Gardens               1.057             0.290512    \nneighborhood_feAllegheny West               -4.236 0.000022940570064266 ***\nneighborhood_feAndorra                      -1.407             0.159321    \nneighborhood_feAston-Woodbridge              0.893             0.371676    \nneighborhood_feBella Vista                   3.494             0.000478 ***\nneighborhood_feBelmont                      -1.991             0.046548 *  \nneighborhood_feBrewerytown                  -4.113 0.000039310930860351 ***\nneighborhood_feBridesburg                    0.077             0.938445    \nneighborhood_feBurholme                     -0.516             0.605815    \nneighborhood_feBustleton                     0.903             0.366403    \nneighborhood_feCarroll Park                 -4.121 0.000038002669715311 ***\nneighborhood_feCedar Park                    1.989             0.046738 *  \nneighborhood_feCedarbrook                   -1.072             0.283630    \nneighborhood_feChestnut Hill                18.959 &lt; 0.0000000000000002 ***\nneighborhood_feClearview                    -2.874             0.004054 ** \nneighborhood_feCobbs Creek                  -5.230 0.000000171828553487 ***\nneighborhood_feDickinson Narrows            -2.498             0.012505 *  \nneighborhood_feDunlap                       -4.516 0.000006359727437564 ***\nneighborhood_feEast Germantown              -2.695             0.007048 ** \nneighborhood_feEast Kensington              -1.631             0.102914    \nneighborhood_feEast Mount Airy              -2.595             0.009481 ** \nneighborhood_feEast Oak Lane                -4.094 0.000042605651448194 ***\nneighborhood_feEast Parkside                -2.451             0.014265 *  \nneighborhood_feEast Passyunk                 0.873             0.382750    \nneighborhood_feEastwick                     -2.113             0.034613 *  \nneighborhood_feElmwood                      -3.831             0.000128 ***\nneighborhood_feFairhill                     -1.955             0.050566 .  \nneighborhood_feFairmount                     4.130 0.000036558118918651 ***\nneighborhood_feFeltonville                  -3.416             0.000637 ***\nneighborhood_feFern Rock                    -2.571             0.010149 *  \nneighborhood_feFishtown - Lower Kensington   0.429             0.667602    \nneighborhood_feFitler Square                14.026 &lt; 0.0000000000000002 ***\nneighborhood_feFox Chase                    -0.382             0.702774    \nneighborhood_feFrancisville                 -2.604             0.009227 ** \nneighborhood_feFrankford                    -3.671             0.000242 ***\nneighborhood_feFranklinville                -4.500 0.000006849957506002 ***\nneighborhood_feGermantown - Morton          -2.802             0.005082 ** \nneighborhood_feGermantown - Penn Knox       -3.328             0.000877 ***\nneighborhood_feGermantown - Westside        -2.726             0.006428 ** \nneighborhood_feGermany Hill                  0.140             0.888878    \nneighborhood_feGirard Estates               -1.920             0.054912 .  \nneighborhood_feGlenwood                     -4.263 0.000020311449479471 ***\nneighborhood_feGraduate Hospital             4.638 0.000003556691191596 ***\nneighborhood_feGrays Ferry                  -4.669 0.000003054412626027 ***\nneighborhood_feGreenwich                    -2.760             0.005789 ** \nneighborhood_feHaddington                   -5.007 0.000000560346626112 ***\nneighborhood_feHarrowgate                   -3.169             0.001534 ** \nneighborhood_feHartranft                    -5.884 0.000000004107739554 ***\nneighborhood_feHawthorne                     0.290             0.771557    \nneighborhood_feHolmesburg                   -1.176             0.239488    \nneighborhood_feHunting Park                 -2.758             0.005829 ** \nneighborhood_feJuniata Park                 -3.038             0.002385 ** \nneighborhood_feKingsessing                  -6.129 0.000000000911404708 ***\nneighborhood_feLawndale                     -2.582             0.009842 ** \nneighborhood_feLexington Park                0.588             0.556394    \nneighborhood_feLogan                        -5.219 0.000000182996833996 ***\nneighborhood_feLogan Square                 12.344 &lt; 0.0000000000000002 ***\nneighborhood_feLower Moyamensing            -4.037 0.000054490920980685 ***\nneighborhood_feManayunk                      1.143             0.253063    \nneighborhood_feMantua                       -3.519             0.000435 ***\nneighborhood_feMayfair                      -0.983             0.325631    \nneighborhood_feMelrose Park Gardens         -2.328             0.019927 *  \nneighborhood_feMill Creek                   -4.213 0.000025346340673632 ***\nneighborhood_feMillbrook                     0.427             0.669236    \nneighborhood_feModena                        1.754             0.079506 .  \nneighborhood_feMorrell Park                  0.656             0.512073    \nneighborhood_feNewbold                      -2.768             0.005643 ** \nneighborhood_feNicetown                     -1.838             0.066044 .  \nneighborhood_feNormandy Village              2.964             0.003039 ** \nneighborhood_feNorth Central                -4.939 0.000000793507275168 ***\nneighborhood_feNorthern Liberties           -0.100             0.920603    \nneighborhood_feNorthwood                    -4.488 0.000007234509435205 ***\nneighborhood_feOgontz                       -2.213             0.026888 *  \nneighborhood_feOld City                      9.698 &lt; 0.0000000000000002 ***\nneighborhood_feOld Kensington               -2.528             0.011489 *  \nneighborhood_feOlney                        -4.395 0.000011178568619381 ***\nneighborhood_feOverbrook                    -5.733 0.000000010054841725 ***\nneighborhood_feOxford Circle                -1.640             0.101045    \nneighborhood_fePacker Park                   0.046             0.963655    \nneighborhood_feParkwood Manor                1.558             0.119140    \nneighborhood_fePaschall                     -3.792             0.000150 ***\nneighborhood_fePassyunk Square               0.109             0.913395    \nneighborhood_fePennsport                    -1.909             0.056248 .  \nneighborhood_fePennypack                    -0.234             0.815110    \nneighborhood_fePennypack Woods               0.098             0.921764    \nneighborhood_fePenrose                      -2.110             0.034857 *  \nneighborhood_fePoint Breeze                 -3.896 0.000098396171956079 ***\nneighborhood_feQueen Village                 4.963 0.000000702585015746 ***\nneighborhood_feRhawnhurst                    0.306             0.759940    \nneighborhood_feRichmond                     -2.086             0.037026 *  \nneighborhood_feRittenhouse                  16.003 &lt; 0.0000000000000002 ***\nneighborhood_feRoxborough                    0.240             0.810534    \nneighborhood_feRoxborough Park              -1.262             0.206945    \nneighborhood_feSharswood                    -2.501             0.012412 *  \nneighborhood_feSociety Hill                 13.242 &lt; 0.0000000000000002 ***\nneighborhood_feSomerton                      1.759             0.078654 .  \nneighborhood_feSouthwest Germantown         -4.340 0.000014329244783269 ***\nneighborhood_feSouthwest Schuylkill         -4.503 0.000006771596548580 ***\nneighborhood_feSpring Garden                 6.647 0.000000000031106684 ***\nneighborhood_feSpruce Hill                   4.227 0.000023861807553049 ***\nneighborhood_feStadium District             -1.297             0.194602    \nneighborhood_feStanton                      -6.520 0.000000000072743521 ***\nneighborhood_feStrawberry Mansion           -5.116 0.000000316978788212 ***\nneighborhood_feSummerdale                   -2.128             0.033396 *  \nneighborhood_feTacony                       -2.293             0.021862 *  \nneighborhood_feTioga                        -5.035 0.000000485129454051 ***\nneighborhood_feTorresdale                   -1.161             0.245818    \nneighborhood_feUpper Kensington             -4.921 0.000000872393876726 ***\nneighborhood_feUpper Roxborough             -1.851             0.064190 .  \nneighborhood_feWalnut Hill                  -2.195             0.028155 *  \nneighborhood_feWashington Square West        2.771             0.005590 ** \nneighborhood_feWest Central Germantown       0.245             0.806552    \nneighborhood_feWest Kensington              -3.652             0.000261 ***\nneighborhood_feWest Mount Airy               1.585             0.112926    \nneighborhood_feWest Oak Lane                -2.867             0.004154 ** \nneighborhood_feWest Passyunk                -4.240 0.000022531934561275 ***\nneighborhood_feWest Poplar                  -2.707             0.006803 ** \nneighborhood_feWest Powelton                -3.226             0.001258 ** \nneighborhood_feWhitman                      -2.637             0.008370 ** \nneighborhood_feWinchester Park               0.939             0.347919    \nneighborhood_feWissahickon                  -0.164             0.870048    \nneighborhood_feWissahickon Hills             0.209             0.834450    \nneighborhood_feWissinoming                  -1.627             0.103829    \nneighborhood_feWister                       -2.456             0.014069 *  \nneighborhood_feWynnefield                   -5.289 0.000000124669047179 ***\nneighborhood_feSmall Neighborhoods          -1.762             0.078031 .  \nquarters_fe2                                 0.966             0.334309    \nquarters_fe3                                 1.428             0.153250    \nquarters_fe4                                 1.557             0.119432    \ncity_dist_mi:knn_amenity_mi                  0.544             0.586460    \ncity_dist_mi:septa_half_mi                  -8.046 0.000000000000000927 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 137800 on 13582 degrees of freedom\nMultiple R-squared:  0.7515,    Adjusted R-squared:  0.7486 \nF-statistic: 258.3 on 159 and 13582 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\nCode\n# Step model to check for most influential predictors.\nstep_model &lt;- step(final_model, direction = \"both\")\n\n\nStart:  AIC=325399.2\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + fuel_type + age + \n    I(age^2) + medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + knn_amenity_mi * \n    city_dist_mi + septa_half_mi * city_dist_mi + neighborhood_fe + \n    quarters_fe\n\n                               Df      Sum of Sq             RSS    AIC\n- fuel_type                     3    32389297317 258062730389632 325395\n- quarters_fe                   3    55701981498 258086043073814 325396\n- city_dist_mi:knn_amenity_mi   1     5621897811 258035962990126 325397\n- pct_single_family             1    18553936107 258048895028423 325398\n&lt;none&gt;                                           258030341092316 325399\n- ln_park_dist                  1    37663867116 258068004959431 325399\n- story_num                     1    37730513689 258068071606004 325399\n- pct_vacant                    1   205093618832 258235434711147 325408\n- medhhinc                      1   968344699087 258998685791402 325449\n- I(age^2)                      1  1163402365725 259193743458040 325459\n- city_dist_mi:septa_half_mi    1  1229859942648 259260201034963 325463\n- age                           1  2403108130952 260433449223268 325525\n- ac_binary                     1  3075319104067 261105660196382 325560\n- basement_type                10  4972830458159 263003171550474 325642\n- bath_num                      1  6787781798786 264818122891102 325754\n- garage_num                    1  9738818198436 267769159290752 325906\n- fireplace_num                 1 10044913943096 268075255035411 325922\n- ln_square_feet                1 41625395516378 299655736608694 327452\n- neighborhood_fe             126 53653286818295 311683627910610 327743\n\nStep:  AIC=325394.9\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    quarters_fe + city_dist_mi:knn_amenity_mi + city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- quarters_fe                   3    54387890031 258117118279664 325392\n- city_dist_mi:knn_amenity_mi   1     5819486123 258068549875755 325393\n- pct_single_family             1    18231861940 258080962251573 325394\n- story_num                     1    37234777160 258099965166792 325395\n&lt;none&gt;                                           258062730389632 325395\n- ln_park_dist                  1    38176077779 258100906467411 325395\n+ fuel_type                     3    32389297317 258030341092316 325399\n- pct_vacant                    1   209675335538 258272405725170 325404\n- medhhinc                      1   973197176055 259035927565687 325445\n- I(age^2)                      1  1162088500866 259224818890498 325455\n- city_dist_mi:septa_half_mi    1  1233833768960 259296564158592 325458\n- age                           1  2396993729867 260459724119499 325520\n- ac_binary                     1  3139018476709 261201748866342 325559\n- basement_type                10  4999114710757 263061845100389 325639\n- bath_num                      1  6766976732963 264829707122595 325749\n- garage_num                    1  9763652361538 267826382751170 325903\n- fireplace_num                 1 10088592404525 268151322794158 325920\n- ln_square_feet                1 41849100885222 299911831274855 327458\n- neighborhood_fe             126 53686799108094 311749529497726 327740\n\nStep:  AIC=325391.8\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    city_dist_mi:knn_amenity_mi + city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- city_dist_mi:knn_amenity_mi   1     5756830841 258122875110505 325390\n- pct_single_family             1    18658730034 258135777009698 325391\n- story_num                     1    37488545765 258154606825428 325392\n&lt;none&gt;                                           258117118279664 325392\n- ln_park_dist                  1    37878814136 258154997093800 325392\n+ quarters_fe                   3    54387890031 258062730389632 325395\n+ fuel_type                     3    31075205850 258086043073814 325396\n- pct_vacant                    1   210823644869 258327941924533 325401\n- medhhinc                      1   976133373250 259093251652914 325442\n- I(age^2)                      1  1162551659566 259279669939229 325452\n- city_dist_mi:septa_half_mi    1  1238796793796 259355915073460 325456\n- age                           1  2400177602981 260517295882645 325517\n- ac_binary                     1  3141691617156 261258809896819 325556\n- basement_type                10  4983295308254 263100413587918 325635\n- bath_num                      1  6765462169116 264882580448780 325745\n- garage_num                    1  9748721063405 267865839343068 325899\n- fireplace_num                 1 10088185393881 268205303673545 325917\n- ln_square_feet                1 41841680242931 299958798522595 327454\n- neighborhood_fe             126 53699881267112 311816999546776 327737\n\nStep:  AIC=325390.1\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + knn_amenity_mi + neighborhood_fe + \n    city_dist_mi:septa_half_mi\n\n                               Df      Sum of Sq             RSS    AIC\n- knn_amenity_mi                1     9463957017 258132339067522 325389\n- pct_single_family             1    18204884533 258141079995038 325389\n- story_num                     1    36181431648 258159056542153 325390\n&lt;none&gt;                                           258122875110505 325390\n- ln_park_dist                  1    37697065101 258160572175606 325390\n+ city_dist_mi:knn_amenity_mi   1     5756830841 258117118279664 325392\n+ quarters_fe                   3    54325234750 258068549875755 325393\n+ fuel_type                     3    31271022938 258091604087567 325394\n- pct_vacant                    1   207829810082 258330704920587 325399\n- medhhinc                      1  1022223082224 259145098192729 325442\n- I(age^2)                      1  1163915248969 259286790359474 325450\n- city_dist_mi:septa_half_mi    1  1421034201138 259543909311643 325464\n- age                           1  2407286229613 260530161340118 325516\n- ac_binary                     1  3144162873201 261267037983706 325554\n- basement_type                10  4982909989261 263105785099766 325633\n- bath_num                      1  6759835661550 264882710772054 325743\n- garage_num                    1  9743273533387 267866148643892 325897\n- fireplace_num                 1 10082458826585 268205333937090 325915\n- ln_square_feet                1 41852960908266 299975836018771 327453\n- neighborhood_fe             126 53791205940811 311914081051316 327739\n\nStep:  AIC=325388.6\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + pct_single_family + city_dist_mi + \n    septa_half_mi + ln_park_dist + neighborhood_fe + city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n- pct_single_family            1    16233695937 258148572763459 325387\n- story_num                    1    35745374109 258168084441630 325389\n&lt;none&gt;                                          258132339067522 325389\n- ln_park_dist                 1    39838604645 258172177672167 325389\n+ knn_amenity_mi               1     9463957017 258122875110505 325390\n+ quarters_fe                  3    54712085454 258077626982068 325392\n+ fuel_type                    3    31263319772 258101075747749 325393\n- pct_vacant                   1   207377729293 258339716796815 325398\n- medhhinc                     1  1050183340159 259182522407681 325442\n- I(age^2)                     1  1162132948696 259294472016217 325448\n- city_dist_mi:septa_half_mi   1  1419954455139 259552293522660 325462\n- age                          1  2405621060553 260537960128075 325514\n- ac_binary                    1  3141565383491 261273904451013 325553\n- basement_type               10  4975328891408 263107667958929 325631\n- bath_num                     1  6760798186702 264893137254224 325742\n- garage_num                   1  9734395002015 267866734069537 325895\n- fireplace_num                1 10073002006327 268205341073849 325913\n- ln_square_feet               1 41865500824052 299997839891573 327452\n- neighborhood_fe            126 54107035645834 312239374713356 327752\n\nStep:  AIC=325387.5\nsale_price ~ ln_square_feet + bath_num + ac_binary + fireplace_num + \n    story_num + garage_num + basement_type + age + I(age^2) + \n    medhhinc + pct_vacant + city_dist_mi + septa_half_mi + ln_park_dist + \n    neighborhood_fe + city_dist_mi:septa_half_mi\n\n                              Df      Sum of Sq             RSS    AIC\n&lt;none&gt;                                          258148572763459 325387\n- story_num                    1    37761781537 258186334544996 325387\n- ln_park_dist                 1    41799094387 258190371857846 325388\n+ pct_single_family            1    16233695937 258132339067522 325389\n+ knn_amenity_mi               1     7492768421 258141079995038 325389\n+ quarters_fe                  3    55072283882 258093500479577 325391\n+ fuel_type                    3    30929437123 258117643326336 325392\n- pct_vacant                   1   205244039646 258353816803105 325396\n- medhhinc                     1  1103136857618 259251709621077 325444\n- I(age^2)                     1  1163894619328 259312467382787 325447\n- city_dist_mi:septa_half_mi   1  1455527337792 259604100101251 325463\n- age                          1  2405567874956 260554140638415 325513\n- ac_binary                    1  3151173792145 261299746555604 325552\n- basement_type               10  5036635022541 263185207786000 325633\n- bath_num                     1  6773444710630 264922017474089 325741\n- garage_num                   1  9848790936982 267997363700441 325900\n- fireplace_num                1 10157075517463 268305648280922 325916\n- ln_square_feet               1 42052380950189 300200953713648 327459\n- neighborhood_fe            126 54090902485293 312239475248752 327750\n\n\n\n\nCode\n# Compute TSS\ny &lt;- model.response(model.frame(step_model))\ntss &lt;- sum((y - mean(y))^2)\n\n# Sequential (Type I) Sum Sq → ΔR²\nanova_model &lt;- anova(step_model)\nanova_model &lt;- anova_model[!is.na(anova_model$\"Sum Sq\"), , drop = FALSE]\nseq_imp &lt;- transform(anova_model,\n                     term = rownames(anova_model),\n                     delta_R2 = `Sum Sq` / tss)\n\n# Get top 5 predictors\nseq_top4 &lt;- seq_imp[order(-seq_imp$delta_R2), c(\"term\", \"Df\", \"delta_R2\")]\nhead(seq_top4, 5)\n\n\n                           term    Df   delta_R2\nln_square_feet   ln_square_feet     1 0.41253629\nResiduals             Residuals 13591 0.24860274\nneighborhood_fe neighborhood_fe   126 0.06520037\nmedhhinc               medhhinc     1 0.05979506\nbath_num               bath_num     1 0.04972568\n\n\n\n\nCode\n# Residual map preparation\n\n# Match CRS\nphilly_neighborhoods &lt;- st_transform(philly_neighborhoods, st_crs(final_data))\n\n# Spatial join: assign each point to a neighborhood\npoints_with_neighborhood &lt;- st_join(final_data, philly_neighborhoods)\n\n# Add residual column\npoints_with_neighborhood$residuals &lt;- residuals(final_model)\n\n# Calculate average residual by neighborhood\nneighborhood_residuals &lt;- points_with_neighborhood %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(MAPNAME) %&gt;%\n  summarise(\n    mean_residual = mean(residuals, na.rm = TRUE),\n    median_residual = median(residuals, na.rm = TRUE),\n    n_sales = n()\n  )\n\n# Join to neighborhoods\nneighborhoods_with_residuals &lt;- philly_neighborhoods %&gt;%\n  left_join(neighborhood_residuals, by = \"MAPNAME\")\n\n\n\n\nCode\n# Map the averaged residuals\n\nneighborhood_map &lt;- ggplot(neighborhoods_with_residuals) +\n  geom_sf(aes(fill = mean_residual), color = \"black\", size = 0.3) +\n  scale_fill_gradient2(\n    low = \"blue2\", \n    mid = \"#f5f4f0\", \n    high = \"red2\",\n    midpoint = 0,\n    labels = scales::dollar,\n    name = \"Mean Residual ($)\"\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Average Model Residuals by Neighborhood\",\n    subtitle = \"Red = Under-Predicted | Blue = Over-Predicted\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  )\n\nneighborhood_map\n\n\n\n\n\n\n\n\n\nCode\n#ggsave(\"slide_images/residual-neighborhood.png\", plot = neighborhood_map, width = 8, height = 6, units = \"in\", dpi = 300)\n\n\nUniversity City is the hardest to predict, Penn owns a lot of property that doesn’t get taxed. And some less wealthy and disinvested neighborhoods are overvalued, like Parkside and Wynnefield in West Philadelphia, but the overall model predicts pretty accurately for most neighborhoods in Philadelphia."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#fold-cross-validation",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#fold-cross-validation",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "10-Fold Cross-Validation",
    "text": "10-Fold Cross-Validation\n\nLogged Price Response Variable\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# first model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_1 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_1$results\n\n\n  intercept      RMSE  Rsquared       MAE      RMSESD RsquaredSD       MAESD\n1      TRUE 0.3816682 0.6094278 0.2897425 0.009088709 0.01740783 0.004395093\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# second model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_2 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # census\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_2$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.3102648 0.7419939 0.2299054 0.01217619 0.01899135 0.005887415\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_3 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_3$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.2952488 0.7662942 0.2176894 0.01067494 0.01599652 0.005499245\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_log_4 &lt;- train(ln_sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial\n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe, # Fixed effect  \n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_log_4$results\n\n\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD       MAESD\n1      TRUE 0.2551879 0.8253801 0.1815212 0.01267003 0.01652301 0.004692818\n\n\n\n\nCode\n# Compare all 4 models\n\n## Combine four models：cv_model_1, cv_model_2, cv_model_3, cv_model_4\nlog_compare &lt;- bind_rows(\n  cv_log_1$results %&gt;% \n    mutate(Model = \"Model 1: Structural\"),\n  cv_log_2$results %&gt;% \n    mutate(Model = \"Model 2: Structural + Census\"),\n  cv_log_3$results %&gt;% \n    mutate(Model = \"Model 3: Structural + Census + Spatial\"),\n  cv_log_4$results %&gt;% \n    mutate(Model = \"Model 4: Final Model\")\n) %&gt;%\n  select(Model, RMSE, Rsquared, MAE) %&gt;% \n  mutate(across(c(RMSE, Rsquared, MAE), round, 3))\n\n## Plot\nlog_kable &lt;- kable(log_compare,\n                     col.names = c(\"Model\", \"RMSE\", \"R²\", \"MAE\"),\n                     caption = \"Log Model Performance Comparison (10-Fold Cross-Validation)\",\n                     digits = 4,\n                     format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"10cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\")\n\nlog_kable\n\n\n\nLog Model Performance Comparison (10-Fold Cross-Validation)\n\n\nModel\nRMSE\nR²\nMAE\n\n\n\n\nModel 1: Structural\n0.382\n0.609\n0.290\n\n\nModel 2: Structural + Census\n0.310\n0.742\n0.230\n\n\nModel 3: Structural + Census + Spatial\n0.295\n0.766\n0.218\n\n\nModel 4: Final Model\n0.255\n0.825\n0.182\n\n\n\n\n\n\n\nUnlogged Price Response Variable\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# first model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_1 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2), # Age polynomial.\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_1$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 174881.3 0.5946383 102296.6 25555.15 0.06642462 3870.938\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# second model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_2 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family, # census\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_2$results\n\n\n  intercept     RMSE  Rsquared     MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 160396.3 0.6588349 88635.4 28556.53 0.07587715 3803.434\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_3 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi, # Spatial\n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_3$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 153234.2 0.6884576 84701.65 28578.51 0.07398629 4072.517\n\n\n\n\nCode\nlibrary(caret)\n# 10-fold cross-validation\n# third model\nset.seed(0)\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 10, savePredictions=\"final\")\n\ncv_model_4 &lt;- train(sale_price ~ ln_square_feet + bath_num + # Structural.\n                    ac_binary + fireplace_num + story_num + garage_num + # Structural.\n                    basement_type + fuel_type + # Categorical structural.\n                    age + I(age^2) + # Age polynomial.\n                    medhhinc + pct_vacant + pct_single_family +\n                    city_dist_mi + septa_half_mi + ln_park_dist + knn_amenity_mi + # Spatial\n                    knn_amenity_mi * city_dist_mi + septa_half_mi * city_dist_mi + # Interaction between amenities and downtown distance.\n                    neighborhood_fe, # Fixed effect  \n                    data = final_data,\n                    method = \"lm\",\n                    trControl = train_control)\n\ncv_model_4$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 137218.3 0.7496165 72327.67 29554.89 0.07463082 3804.496\n\n\n\n\nCode\n# Compare all 4 models\n\n## Combine four models：cv_model_1, cv_model_2, cv_model_3, cv_model_4\nmodel_compare &lt;- bind_rows(\n  cv_model_1$results %&gt;% \n    mutate(Model = \"Model 1: Structural\"),\n  cv_model_2$results %&gt;% \n    mutate(Model = \"Model 2: Structural + Census\"),\n  cv_model_3$results %&gt;% \n    mutate(Model = \"Model 3: Structural + Census + Spatial\"),\n  cv_model_4$results %&gt;% \n    mutate(Model = \"Model 4: Final Model\")\n) %&gt;%\n  select(Model, RMSE, Rsquared, MAE) %&gt;% \n  mutate(across(c(RMSE, Rsquared, MAE), round, 3))\n\n## Plot\nmodel_kable &lt;- kable(model_compare,\n                     col.names = c(\"Model\", \"RMSE ($)\", \"R²\", \"MAE ($)\"),\n                     caption = \"Model Performance Comparison (10-Fold Cross-Validation)\",\n                     digits = 4,\n                     format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE, width = \"10cm\") %&gt;%\n  row_spec(0, color = \"#f5f4f0\", background = \"#ff4100\")\n\nmodel_kable\n\n\n\nModel Performance Comparison (10-Fold Cross-Validation)\n\n\nModel\nRMSE ($)\nR²\nMAE ($)\n\n\n\n\nModel 1: Structural\n174,881.3\n0.595\n102,296.64\n\n\nModel 2: Structural + Census\n160,396.3\n0.659\n88,635.40\n\n\nModel 3: Structural + Census + Spatial\n153,234.2\n0.688\n84,701.65\n\n\nModel 4: Final Model\n137,218.3\n0.750\n72,327.67\n\n\n\n\n\n\n\nCode\n# Create predicted vs. actual plot\n\npred_df &lt;- cv_model_4$pred\n\n# Plot Predicted vs Actual \npred_v_act &lt;- ggplot(pred_df, aes(x = obs, y = pred)) +\n  geom_point(alpha = 0.5, color = \"#2C7BB6\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Predicted vs. Actual Sale Price\",\n    x = \"Actual Sale Price ($)\",\n    y = \"Predicted Sale Price ($)\"\n  ) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\npred_v_act"
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#check-assumptions",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Appendix.html#check-assumptions",
    "title": "Philadelphia Housing Model—Technical Appendix",
    "section": "Check Assumptions",
    "text": "Check Assumptions\n\n\nCode\nresid_df &lt;- data.frame(\n  fitted = fitted(final_model),\n  residuals = resid(final_model)\n)\n\nresid_v_fit &lt;- ggplot(resid_df, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5, color = \"#2C7BB6\") +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Residuals vs. Fitted Values\",\n    x = \"Fitted Values (Predicted Sale Price)\",\n    y = \"Residuals\"\n  ) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\nresid_v_fit\n\n\n\n\n\n\n\n\n\nResiduals vs Fitted Values:\nThe residual plot shows that most residuals are centered around zero, but the spread of residuals increases as the fitted values grow. This “funnel-shaped” pattern suggests potential heteroskedasticity, meaning the variance of errors may increase for higher-priced properties. While the overall linearity assumption appears reasonable, the increasing dispersion indicates that the model’s prediction error is not constant across the price range.\n\n\nCode\nresid_df &lt;- data.frame(residuals = residuals(final_model))\n\n# Q-Q Plot\nqq_plot &lt;- ggplot(resid_df, aes(sample = residuals)) +\n  stat_qq(color = \"#2C7BB6\", alpha = 0.6, size = 2) +      \n  stat_qq_line(color = \"red\", linetype = \"dashed\") +        \n  labs(\n    title = \"Normal Q-Q Plot of Residuals\",\n    subtitle = \"Check for normality assumption.\",\n    x = \"Theoretical Quantiles (Normal Distribution)\",\n    y = \"Sample Quantiles (Residuals)\"\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::dollar) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\nqq_plot\n\n\n\n\n\n\n\n\n\nNormal Q–Q Plot:\nThe Q–Q plot reveals that the residuals deviate from the reference line in both tails, especially in the upper tail. This pattern indicates that the residuals are right-skewed and not perfectly normally distributed. The deviation is mainly driven by a small number of very high sale-price observations, which pull the residual distribution upward. However, moderate departures from normality are common in housing price data and generally do not invalidate the model.\n\n\nCode\ncd &lt;- cooks.distance(final_model)\n\nused_row_idx &lt;- as.integer(rownames(model.frame(final_model)))\n\ncooks_df &lt;- tibble(\n  row_in_data  = used_row_idx,           \n  row_in_model = seq_along(cd),          \n  cooks_distance = as.numeric(cd)\n)\n\nn_used &lt;- length(cd)\nthreshold &lt;- 4 / n_used\n\ncooks_plot &lt;- ggplot(cooks_df, aes(x = row_in_model, y = cooks_distance)) +\n  geom_bar(stat = \"identity\", fill = \"#2C7BB6\", alpha = 0.6) +\n  geom_hline(yintercept = threshold, color = \"red\", linetype = \"dashed\") +\n  coord_cartesian(ylim = c(0, 0.02)) +  \n  labs(\n    title = \"Cook's Distance (Zoomed In)\",\n    subtitle = paste0(\"Red Dashed Line = 4/n ≈ \", round(threshold, 5)),\n    x = \"Observation Index (In-Model)\",\n    y = \"Cook's Distance\"\n  ) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, margin = margin(b = 10)),\n    plot.subtitle = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n\ncooks_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Most influential\ntop_influential &lt;- cooks_df %&gt;%\n  filter(cooks_distance &gt; threshold) %&gt;%\n  arrange(desc(cooks_distance)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(\n    sale_price = final_data$sale_price[row_in_data]   \n  ) %&gt;%\n  select(row_in_model, row_in_data, sale_price, cooks_distance)\n\ntop_influential\n\n\n# A tibble: 10 × 4\n   row_in_model row_in_data sale_price cooks_distance\n          &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1         6038        6038    3600000         0.199 \n 2         6307        6307    5477901         0.130 \n 3          194         194    3330400         0.0463\n 4         8119        8120     480000         0.0457\n 5         2467        2467    3995000         0.0400\n 6        10355       10356     281000         0.0358\n 7         7825        7826     330000         0.0326\n 8         2590        2590    3000000         0.0284\n 9         1896        1896     170000         0.0238\n10         5371        5371    3850000         0.0235\n\n\nCook’s Distance:\nThe Cook’s distance plot shows that almost all observations have very small influence values (below the 4/n threshold), indicating that the model is not dominated by a few extreme points. A few cases exhibit slightly higher Cook’s D values, suggesting the presence of mildly influential outliers, but none appear to exert excessive leverage on the regression coefficients."
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "LABS",
    "section": "",
    "text": "09-15-2025 Lab: Census Data"
  },
  {
    "objectID": "labs.html#lab-1-census-data",
    "href": "labs.html#lab-1-census-data",
    "title": "LABS",
    "section": "",
    "text": "09-15-2025 Lab: Census Data"
  },
  {
    "objectID": "labs.html#lab-2-healthcare-access-and-equity-in-pennsylvania",
    "href": "labs.html#lab-2-healthcare-access-and-equity-in-pennsylvania",
    "title": "LABS",
    "section": "LAB 2: HEALTHCARE ACCESS AND EQUITY IN PENNSYLVANIA",
    "text": "LAB 2: HEALTHCARE ACCESS AND EQUITY IN PENNSYLVANIA\n10-06-2025 Lab: Healthcare Access and Equity in Pennsylvania"
  },
  {
    "objectID": "labs.html#lab-3-midterm-project",
    "href": "labs.html#lab-3-midterm-project",
    "title": "LABS",
    "section": "LAB 3: MIDTERM PROJECT",
    "text": "LAB 3: MIDTERM PROJECT\n10-27-2025 Lab: Midterm Project"
  },
  {
    "objectID": "labs.html#lab-4",
    "href": "labs.html#lab-4",
    "title": "LABS",
    "section": "LAB 4:",
    "text": "LAB 4:\n11-17-2025 Lab:"
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\nHow many hospitals are in the dataset?\n\n223 hospitals.\n\nHow many census tracts?\n\n3,445 census tracts.\n\nWhat coordinate reference system is each dataset in?\n\nPA county boundaries CRS: GCS WGS 84 (Pseudo-Mercator) / EPSG\n\n\n\nPA census tracts CRS: PCS NAD 83 / EPSG 4269.\nPA hospitals CRS: GCS WGS 84 (Pseudo-Mercator) / EPSG 4326.\n\n\n\n\n\n\n\nCode\n# Get age demographic data from ACS.\ntract_ages &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01001_020E\", \"B01001_021E\", \"B01001_022E\",\n    \"B01001_023E\", \"B01001_024E\", \"B01001_025E\",\n    \"B01001_044E\", \"B01001_045E\", \"B01001_046E\",\n    \"B01001_047E\", \"B01001_048E\", \"B01001_049E\"\n  ),\n  state = \"PA\",\n  year = 2020,\n  output = \"wide\"\n)\n # Get population and income data from ACS.\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2020,\n  output = \"wide\"\n)\n\n# Combine 65+ age estimates.\ntract_demographics$age_65_overE &lt;-\n  tract_ages$B01001_020E + tract_ages$B01001_021E + tract_ages$B01001_022E +\n  tract_ages$B01001_023E + tract_ages$B01001_024E + tract_ages$B01001_025E +\n  tract_ages$B01001_044E + tract_ages$B01001_045E + tract_ages$B01001_046E +\n  tract_ages$B01001_047E + tract_ages$B01001_048E + tract_ages$B01001_049E\n\n# Combine 65+ age MOEs.\ntract_demographics$age_65_overM &lt;-\n  tract_ages$B01001_020M + tract_ages$B01001_021M + tract_ages$B01001_022M +\n  tract_ages$B01001_023M + tract_ages$B01001_024M + tract_ages$B01001_025M +\n  tract_ages$B01001_044M + tract_ages$B01001_045M + tract_ages$B01001_046M +\n  tract_ages$B01001_047M + tract_ages$B01001_048M + tract_ages$B01001_049M\n\n# Remove unneeded column.\ntract_demographics$TRACT &lt;- NULL\n\n# Rename tract column.\ntract_demographics &lt;- rename(tract_demographics, TRACT = NAME)\n\n# Join to tract boundaries\npa_tracts &lt;- left_join(pa_tracts, tract_demographics, by = \"GEOID\")\n\n# Summary statistics for reference.\n#summary(pa_tracts[c(\"total_popE\", \"total_popM\",\n#                    \"median_incomeE\", \"median_incomeM\",\n#                    \"age_65_overE\", \"age_65_overM\")])\n\n\n\nWhat year of ACS data is being used?\n\n2020 ACS data.\n\nHow many tracts have missing income data?\n\n64 tracts are missing income estimates and 68 tracts are missing income MOEs.\n\nWhat is the median income across all PA census tracts?\n\n$61,691.\n\n\n\n\n\n\n\nCode\n# Filter for vulnerable tracts.\n# Normalize elderly population.\npa_tracts$pct_elderly &lt;- (pa_tracts$age_65_overE / pa_tracts$total_popE) * 100\n\n# Get 3rd quantile of percent elderly.\nelderly_q3 &lt;- quantile(pa_tracts$pct_elderly, probs = 0.75, na.rm = TRUE)\n# Get 1st quantile of income estimates.\nincome_q1 &lt;- quantile(pa_tracts$median_incomeE, probs = 0.25, na.rm = TRUE)\n\n# Determine criteria for vulnerable tracts.\npa_tracts &lt;- pa_tracts %&gt;%\n  mutate(\n    # Low income for individuals equal or below the 1st quartile.\n    low_income = case_when(median_incomeE &lt;= income_q1 ~ TRUE,\n                           median_incomeE &gt; income_q1 ~ FALSE),\n    # High elderly percent when tracts are equal or above the 3rd quartile.\n    high_elderly = case_when(pct_elderly &gt;= elderly_q3 ~ TRUE,\n                             pct_elderly &lt; elderly_q3 ~ FALSE),\n    # Considered vulnerable when both of the above cases are true.\n    vulnerable = case_when(low_income == TRUE & high_elderly == TRUE ~ TRUE,\n                           low_income == FALSE & high_elderly == FALSE ~ FALSE,\n                           low_income == TRUE & high_elderly == FALSE ~ FALSE,\n                           low_income == FALSE & high_elderly == TRUE ~ FALSE,)\n  )\n\n# Create a summary for personal reference on vulnerable statistics.\nvulnerable_summary &lt;- pa_tracts %&gt;%\n  group_by(vulnerable) %&gt;%\n  summarize(\n    \"Number of Tracts\" = n(),\n    \"Median Income\" = median(median_incomeE, na.rm = TRUE),\n    \"Percent Elderly\" = median(pct_elderly, na.rm = TRUE),\n    \"Percent Vulnerable\" = (n() / 3445) * 100\n  )\n\n# Remove geometry column for kable table.\nvulnerable_summary$geometry &lt;- NULL\n\n# Create professional table.\nkable(\n  vulnerable_summary,\n  col.names = c(\"Vulnerable\", \"Number of Tracts\", \"Median Income\", \"Percent Elderly\", \"Percent Tracts\"),\n  digit = 2,\n  caption = \"&lt;b&gt;PENNSYLVANIA: VULNERABLE TRACTS&lt;/b&gt;\",\n  format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nPENNSYLVANIA: VULNERABLE TRACTS\n\n\nVulnerable\nNumber of Tracts\nMedian Income\nPercent Elderly\nPercent Tracts\n\n\n\n\nFALSE\n3,209\n63,322.0\n17.88\n93.15\n\n\nTRUE\n172\n40,340.5\n26.26\n4.99\n\n\nNA\n64\nNA\n3.27\n1.86\n\n\n\n\n\n\nWhat income threshold was chosen and why?\n\nincome threshold I chose was for the amount equal to or below 25%, or first quartile of the median income data. I believe it’s pertinent because many other factors come into play with income and affordability, especially when it comes to older populations, who have different forms of income that contribute to their day-to-day livelihoods. In addition, many elderly may have significant debts like those related to medical expenses that put them in more financial risk, so the 25% is a broader net that’s an attempt to capture the idiosyncracies of affordability and economic struggles in the US.\n\nWhat elderly population threshold was chosen and why?\n\nI used the third quantile for the elderly population percentage in PA, this means I’m targetting tracts with a larger proportion of older individuals that paint a picture of an aging populace in the areas. With an aging population, and younger populations generally moving to other regions with economic opportunity, elderly tracts’ public services may take a hit with out-migrating young workers and be more susceptible to physical and other health-related vulnerabilities.\n\nHow many tracts meet the vulnerability criteria?\n\n172 tracts meet the vulnerability criteria.\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\nApproximately 4.99% of PA’s census tracts are considered vulnerable.\n\n\n\n\n\n\n\nCode\n# Filter PA tracts by vulnerability.\nvulnerable_tracts &lt;- pa_tracts %&gt;%\n  filter(vulnerable == TRUE)\n\n# Transform to appropriate projected CRS.\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\npa_tracts &lt;- st_transform(pa_tracts, crs = 5070)\npa_hospitals &lt;- st_as_sf(pa_hospitals,\n                         coords = c(\"longitude\", \"latitude\"),\n                         crs = 4326)\npa_hospitals &lt;- st_transform(pa_hospitals, 5070)\n\n# Calculate distance from each tract centroid to nearest hospital.\n# Create vulnerable tract centroids.\nvulnerable_centroids &lt;- st_centroid(vulnerable_tracts)\n\n# Create a list of indexes of hospitals closest to vulnerable centroids\nnearest_hospital_index &lt;- st_nearest_feature(vulnerable_centroids, pa_hospitals)\n\n# Create points from the hospital index.\nnearest_hospital_point &lt;- pa_hospitals[nearest_hospital_index,]\n\n# Calculate distance between centroid and nearest hospital.\nhospital_distance &lt;- st_distance(vulnerable_centroids, nearest_hospital_point, by_element = TRUE)\n\n# Set miles units.\nvulnerable_tracts$nearest_hospital_mi &lt;- as.numeric(set_units(hospital_distance, \"mi\"))\n\n# Summary statistics for reference.\n#summary(vulnerable_tracts[c(\"total_popE\", \"total_popM\",\n#                            \"median_incomeE\", \"median_incomeM\",\n#                            \"age_65_overE\", \"age_65_overM\",\n#                            \"pct_elderly\", \"low_income\",\n#                            \"high_elderly\", \"vulnerable\")])\n\n# Determine how many tracts have hospitals more than 15 miles away.\nunderserved_number &lt;- sum(vulnerable_tracts$nearest_hospital_mi &gt; 15)\n\nprint(paste0(\"There are \", underserved_number, \" census tracts that are underserved.\"))\n\n\n[1] \"There are 11 census tracts that are underserved.\"\n\n\nI chose Albers Equal Area because it was appropriate when calculating hospital distances all over the state, if I’d chosen a PCS that was more localized like State Planes, then depending whichever cardinal direction I selected (i.e. North or South Pennsylvania), it’d have some distortion in other regions of Pennsylvania.\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.78 miles.\n\nWhat is the maximum distance?\n\n27.76 miles.\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n11 census tracts of the 172 total vulnerable tracts are more than 15 miles from the nearest hospital.\n\n\n\n\n\n\n\nCode\n# Filter tracts with hospitals more than 15 miles away.\nunderserved_tracts &lt;- vulnerable_tracts %&gt;%\n  filter(nearest_hospital_mi &gt; 15)\n\n# Create underserved column when hospitals are more than 15 miles away.\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% mutate(\n  underserved = case_when(\n    vulnerable_tracts$nearest_hospital_mi &gt; 15 ~ TRUE,\n    TRUE ~ FALSE))\n\n# Calculate percent of underserved tracts by dividing it with vulnerable tracts.\nunderserved_percent &lt;- (nrow(underserved_tracts) / nrow(vulnerable_tracts)) * 100\n\nprint(sprintf(\"%.2f%% of vulnerable census tracts are underserved.\", underserved_percent))\n\n\n[1] \"6.40% of vulnerable census tracts are underserved.\"\n\n\n\nHow many tracts are underserved?\n\n11 census tracts are underserved.\n\nWhat percentage of vulnerable tracts are underserved?\n\n6.4% of vulnerable census tracts are underserved.\n\nIs this surprising? Why or why not?\n\nNot quite. I’m from Utah and the disparity with hospital access is expected, especially since it’s twice the size of Pennsylvania. I know in the US’ East Coast I expect that the states have better hospital coverage given that they were also established as states over a century earlier than Utah, so they’ve had that time to construct more expansive infrastructure compared. That being said, it’s still important to address the lacking access of those who are underserved, even if it seems like a small percent of the population.\n\n\n\n\n\n\n\nCode\n# Spatial join tracts to counties.\n# Remove the \" County\" substring from the NAMELSADCO column values.\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(NAMELSADCO = str_remove_all(NAMELSADCO, \" County\"))\n\n# Change the values in NAMELSADCO to all capitalized for later joining.\nvulnerable_tracts$NAMELSADCO &lt;- toupper(vulnerable_tracts$NAMELSADCO)\n\n# Align CRS of both data before spatial join.\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 4326)\npa_counties &lt;- st_transform(pa_counties, crs = 4326)\n\n# Spatial left join.\ncounty_tracts &lt;- st_join(vulnerable_tracts, pa_counties, left = TRUE)\n\n# Aggregate statistics by county\naggregate_summary &lt;- county_tracts %&gt;%\n  st_drop_geometry() %&gt;% # Drop the geometry.\n  group_by(NAMELSADCO) %&gt;% # Group by county.\n  summarize(\n    # Summarize the number of vulnerable and underserved people.\n    number_vulnerable = sum(vulnerable),\n    number_underserved = sum(underserved),\n    # Summarize the percent of underserved people.\n    percent_underserved = (number_underserved / number_vulnerable) * 100,\n    # Summarize the average distance to the nearest hospital.\n    avg_nearby_hospital_dist = mean(nearest_hospital_mi, na.rm = TRUE),\n    # Summarize the total estimated population.\n    total_vulnerable_pop = sum(total_popE)\n  ) %&gt;%\n  # Rename the NAMELSADCO column to county.\n  rename(\n    county = NAMELSADCO\n  )\n\n# Change COUNTY_NAM to county.\npa_counties &lt;- rename(pa_counties, county = COUNTY_NAM)\n\n# Conduct a left join.\npa_mapping &lt;- left_join(pa_counties, aggregate_summary, by = \"county\")\n\n\n\n\nCode\n# Plot map of percent underserved by Pennsylvania counties.\nggplot(pa_mapping) +\n  geom_sf(aes(fill = percent_underserved), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Percent Underserved\",\n    option = \"magma\",\n    # Format to show percent sign on color bar.\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    title = \"Most Underserved Counties\",\n    subtitle = \"Pennsylvania, United States\",\n    caption = \"Source: ACS 2019–2023\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nSullivan, Cameron, Forest, Clearfield, and Juniata at 100% for the first three counties then 82.22% and 71.43% for the latter two counties.\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nCounties with an average hospital distance of more than 15 miles and with the most vulnerable people in that group are Bradford, Potter, Sullivan, Cameron, and Juniata that have average hospital distances more than 15 miles. Bradford consists of 1 census tract and has the least number of vulnerable people at 2,833 and Cameron consists of 7 census tracts and has the most at 17,052 vulnerable people. Other counties fall between the two ranges, but below 10,000 for the vulnerable population, so Cameron is a significant outlier.\n\nAre there any patterns in where underserved counties are located?\n\nIt looks like more of the underserved counties are in the northern portion of Pennsylvania, and seemingly clustered toward the middle in addition. I also know those areas are peppered with several forested regions like national and state forests. Development may be much more restricted due to that. There are also underserved counties at the fringes of Pennsylvania, and several counties noted as NA due to the fact that the census tracts within them were part of the 64 NA tracts.\n\n\n\n\n\n\n\nCode\n# Create and format priority counties table.\nkable_table &lt;- pa_mapping[, c(\"county\", \"number_vulnerable\",\n                                \"number_underserved\", \"percent_underserved\",\n                                \"avg_nearby_hospital_dist\", \"total_vulnerable_pop\")]\n\n# Drop geometry and rename the columns.\nkable_table &lt;- kable_table %&gt;%\n  st_drop_geometry() %&gt;%\n  rename(\n    \"County\" = county,\n    \"Number of Vulnerable Tracts\" = number_vulnerable,\n    \"Number of Underserved Tracts\" = number_underserved,\n    \"Percent Underserved\" = percent_underserved,\n    \"Average Mile(s) to Nearest Hospital\" = avg_nearby_hospital_dist,\n    \"Total Vulnerable People\" = total_vulnerable_pop\n  )\n\n# Filter and arrage the top 10 underserved counties by percent.\nkable_table &lt;- kable_table %&gt;%\n  arrange(desc(`Percent Underserved`)) %&gt;%\n  slice(1:10)\n\n# Format percent column to have % sign.\nkable_table &lt;- kable_table %&gt;%\n  mutate(\n    # Format so percent and distance values have units.\n    `Percent Underserved` = paste0(round(`Percent Underserved`, 2), \"%\"),\n    `Average Mile(s) to Nearest Hospital` = paste0(round(`Average Mile(s) to Nearest Hospital`, 2), \" mi\")\n  )\n\n# Create kable.\nkable(\n  kable_table,\n  # Center align.\n  align = \"c\",\n  # Limit percents to two decimal places.\n  digit = 2,\n  # Make sure numbers are separated by thousands places.\n  format.args = list(big.mark = \",\"),\n  caption = \"&lt;b&gt;TOP 10 UNDERSERVED PA COUNTIES: For Priority Healthcare Investment&lt;/b&gt;\"\n) %&gt;%\n  # Shade every other row for ease of reading.\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"mistyrose\", background = \"red3\")\n\n\n\nTOP 10 UNDERSERVED PA COUNTIES: For Priority Healthcare Investment\n\n\nCounty\nNumber of Vulnerable Tracts\nNumber of Underserved Tracts\nPercent Underserved\nAverage Mile(s) to Nearest Hospital\nTotal Vulnerable People\n\n\n\n\nSULLIVAN\n6\n6\n100%\n20.5 mi\n8,116\n\n\nCAMERON\n6\n6\n100%\n19.31 mi\n15,048\n\n\nFOREST\n4\n4\n100%\n18.04 mi\n10,108\n\n\nCLEARFIELD\n6\n5\n83.33%\n15.76 mi\n15,831\n\n\nJUNIATA\n7\n5\n71.43%\n14.06 mi\n17,193\n\n\nPOTTER\n5\n3\n60%\n14.61 mi\n11,862\n\n\nBRADFORD\n2\n1\n50%\n9.36 mi\n8,715\n\n\nCRAWFORD\n2\n1\n50%\n15.37 mi\n4,018\n\n\nBUCKS\n2\n0\n0%\n4.02 mi\n13,326\n\n\nTIOGA\n3\n0\n0%\n5.88 mi\n10,515"
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html#research-question",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html#research-question",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Which Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\n\nHow many hospitals are in the dataset?\n\n223 hospitals.\n\nHow many census tracts?\n\n3,445 census tracts.\n\nWhat coordinate reference system is each dataset in?\n\nPA county boundaries CRS: GCS WGS 84 (Pseudo-Mercator) / EPSG\n\n\n\nPA census tracts CRS: PCS NAD 83 / EPSG 4269.\nPA hospitals CRS: GCS WGS 84 (Pseudo-Mercator) / EPSG 4326.\n\n\n\n\n\n\n\nCode\n# Get age demographic data from ACS.\ntract_ages &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01001_020E\", \"B01001_021E\", \"B01001_022E\",\n    \"B01001_023E\", \"B01001_024E\", \"B01001_025E\",\n    \"B01001_044E\", \"B01001_045E\", \"B01001_046E\",\n    \"B01001_047E\", \"B01001_048E\", \"B01001_049E\"\n  ),\n  state = \"PA\",\n  year = 2020,\n  output = \"wide\"\n)\n # Get population and income data from ACS.\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2020,\n  output = \"wide\"\n)\n\n# Combine 65+ age estimates.\ntract_demographics$age_65_overE &lt;-\n  tract_ages$B01001_020E + tract_ages$B01001_021E + tract_ages$B01001_022E +\n  tract_ages$B01001_023E + tract_ages$B01001_024E + tract_ages$B01001_025E +\n  tract_ages$B01001_044E + tract_ages$B01001_045E + tract_ages$B01001_046E +\n  tract_ages$B01001_047E + tract_ages$B01001_048E + tract_ages$B01001_049E\n\n# Combine 65+ age MOEs.\ntract_demographics$age_65_overM &lt;-\n  tract_ages$B01001_020M + tract_ages$B01001_021M + tract_ages$B01001_022M +\n  tract_ages$B01001_023M + tract_ages$B01001_024M + tract_ages$B01001_025M +\n  tract_ages$B01001_044M + tract_ages$B01001_045M + tract_ages$B01001_046M +\n  tract_ages$B01001_047M + tract_ages$B01001_048M + tract_ages$B01001_049M\n\n# Remove unneeded column.\ntract_demographics$TRACT &lt;- NULL\n\n# Rename tract column.\ntract_demographics &lt;- rename(tract_demographics, TRACT = NAME)\n\n# Join to tract boundaries\npa_tracts &lt;- left_join(pa_tracts, tract_demographics, by = \"GEOID\")\n\n# Summary statistics for reference.\n#summary(pa_tracts[c(\"total_popE\", \"total_popM\",\n#                    \"median_incomeE\", \"median_incomeM\",\n#                    \"age_65_overE\", \"age_65_overM\")])\n\n\n\nWhat year of ACS data is being used?\n\n2020 ACS data.\n\nHow many tracts have missing income data?\n\n64 tracts are missing income estimates and 68 tracts are missing income MOEs.\n\nWhat is the median income across all PA census tracts?\n\n$61,691.\n\n\n\n\n\n\n\nCode\n# Filter for vulnerable tracts.\n# Normalize elderly population.\npa_tracts$pct_elderly &lt;- (pa_tracts$age_65_overE / pa_tracts$total_popE) * 100\n\n# Get 3rd quantile of percent elderly.\nelderly_q3 &lt;- quantile(pa_tracts$pct_elderly, probs = 0.75, na.rm = TRUE)\n# Get 1st quantile of income estimates.\nincome_q1 &lt;- quantile(pa_tracts$median_incomeE, probs = 0.25, na.rm = TRUE)\n\n# Determine criteria for vulnerable tracts.\npa_tracts &lt;- pa_tracts %&gt;%\n  mutate(\n    # Low income for individuals equal or below the 1st quartile.\n    low_income = case_when(median_incomeE &lt;= income_q1 ~ TRUE,\n                           median_incomeE &gt; income_q1 ~ FALSE),\n    # High elderly percent when tracts are equal or above the 3rd quartile.\n    high_elderly = case_when(pct_elderly &gt;= elderly_q3 ~ TRUE,\n                             pct_elderly &lt; elderly_q3 ~ FALSE),\n    # Considered vulnerable when both of the above cases are true.\n    vulnerable = case_when(low_income == TRUE & high_elderly == TRUE ~ TRUE,\n                           low_income == FALSE & high_elderly == FALSE ~ FALSE,\n                           low_income == TRUE & high_elderly == FALSE ~ FALSE,\n                           low_income == FALSE & high_elderly == TRUE ~ FALSE,)\n  )\n\n# Create a summary for personal reference on vulnerable statistics.\nvulnerable_summary &lt;- pa_tracts %&gt;%\n  group_by(vulnerable) %&gt;%\n  summarize(\n    \"Number of Tracts\" = n(),\n    \"Median Income\" = median(median_incomeE, na.rm = TRUE),\n    \"Percent Elderly\" = median(pct_elderly, na.rm = TRUE),\n    \"Percent Vulnerable\" = (n() / 3445) * 100\n  )\n\n# Remove geometry column for kable table.\nvulnerable_summary$geometry &lt;- NULL\n\n# Create professional table.\nkable(\n  vulnerable_summary,\n  col.names = c(\"Vulnerable\", \"Number of Tracts\", \"Median Income\", \"Percent Elderly\", \"Percent Tracts\"),\n  digit = 2,\n  caption = \"&lt;b&gt;PENNSYLVANIA: VULNERABLE TRACTS&lt;/b&gt;\",\n  format.args = list(big.mark = \",\")\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nPENNSYLVANIA: VULNERABLE TRACTS\n\n\nVulnerable\nNumber of Tracts\nMedian Income\nPercent Elderly\nPercent Tracts\n\n\n\n\nFALSE\n3,209\n63,322.0\n17.88\n93.15\n\n\nTRUE\n172\n40,340.5\n26.26\n4.99\n\n\nNA\n64\nNA\n3.27\n1.86\n\n\n\n\n\n\nWhat income threshold was chosen and why?\n\nincome threshold I chose was for the amount equal to or below 25%, or first quartile of the median income data. I believe it’s pertinent because many other factors come into play with income and affordability, especially when it comes to older populations, who have different forms of income that contribute to their day-to-day livelihoods. In addition, many elderly may have significant debts like those related to medical expenses that put them in more financial risk, so the 25% is a broader net that’s an attempt to capture the idiosyncracies of affordability and economic struggles in the US.\n\nWhat elderly population threshold was chosen and why?\n\nI used the third quantile for the elderly population percentage in PA, this means I’m targetting tracts with a larger proportion of older individuals that paint a picture of an aging populace in the areas. With an aging population, and younger populations generally moving to other regions with economic opportunity, elderly tracts’ public services may take a hit with out-migrating young workers and be more susceptible to physical and other health-related vulnerabilities.\n\nHow many tracts meet the vulnerability criteria?\n\n172 tracts meet the vulnerability criteria.\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\nApproximately 4.99% of PA’s census tracts are considered vulnerable.\n\n\n\n\n\n\n\nCode\n# Filter PA tracts by vulnerability.\nvulnerable_tracts &lt;- pa_tracts %&gt;%\n  filter(vulnerable == TRUE)\n\n# Transform to appropriate projected CRS.\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 5070)\npa_tracts &lt;- st_transform(pa_tracts, crs = 5070)\npa_hospitals &lt;- st_as_sf(pa_hospitals,\n                         coords = c(\"longitude\", \"latitude\"),\n                         crs = 4326)\npa_hospitals &lt;- st_transform(pa_hospitals, 5070)\n\n# Calculate distance from each tract centroid to nearest hospital.\n# Create vulnerable tract centroids.\nvulnerable_centroids &lt;- st_centroid(vulnerable_tracts)\n\n# Create a list of indexes of hospitals closest to vulnerable centroids\nnearest_hospital_index &lt;- st_nearest_feature(vulnerable_centroids, pa_hospitals)\n\n# Create points from the hospital index.\nnearest_hospital_point &lt;- pa_hospitals[nearest_hospital_index,]\n\n# Calculate distance between centroid and nearest hospital.\nhospital_distance &lt;- st_distance(vulnerable_centroids, nearest_hospital_point, by_element = TRUE)\n\n# Set miles units.\nvulnerable_tracts$nearest_hospital_mi &lt;- as.numeric(set_units(hospital_distance, \"mi\"))\n\n# Summary statistics for reference.\n#summary(vulnerable_tracts[c(\"total_popE\", \"total_popM\",\n#                            \"median_incomeE\", \"median_incomeM\",\n#                            \"age_65_overE\", \"age_65_overM\",\n#                            \"pct_elderly\", \"low_income\",\n#                            \"high_elderly\", \"vulnerable\")])\n\n# Determine how many tracts have hospitals more than 15 miles away.\nunderserved_number &lt;- sum(vulnerable_tracts$nearest_hospital_mi &gt; 15)\n\nprint(paste0(\"There are \", underserved_number, \" census tracts that are underserved.\"))\n\n\n[1] \"There are 11 census tracts that are underserved.\"\n\n\nI chose Albers Equal Area because it was appropriate when calculating hospital distances all over the state, if I’d chosen a PCS that was more localized like State Planes, then depending whichever cardinal direction I selected (i.e. North or South Pennsylvania), it’d have some distortion in other regions of Pennsylvania.\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\n4.78 miles.\n\nWhat is the maximum distance?\n\n27.76 miles.\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\n11 census tracts of the 172 total vulnerable tracts are more than 15 miles from the nearest hospital.\n\n\n\n\n\n\n\nCode\n# Filter tracts with hospitals more than 15 miles away.\nunderserved_tracts &lt;- vulnerable_tracts %&gt;%\n  filter(nearest_hospital_mi &gt; 15)\n\n# Create underserved column when hospitals are more than 15 miles away.\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% mutate(\n  underserved = case_when(\n    vulnerable_tracts$nearest_hospital_mi &gt; 15 ~ TRUE,\n    TRUE ~ FALSE))\n\n# Calculate percent of underserved tracts by dividing it with vulnerable tracts.\nunderserved_percent &lt;- (nrow(underserved_tracts) / nrow(vulnerable_tracts)) * 100\n\nprint(sprintf(\"%.2f%% of vulnerable census tracts are underserved.\", underserved_percent))\n\n\n[1] \"6.40% of vulnerable census tracts are underserved.\"\n\n\n\nHow many tracts are underserved?\n\n11 census tracts are underserved.\n\nWhat percentage of vulnerable tracts are underserved?\n\n6.4% of vulnerable census tracts are underserved.\n\nIs this surprising? Why or why not?\n\nNot quite. I’m from Utah and the disparity with hospital access is expected, especially since it’s twice the size of Pennsylvania. I know in the US’ East Coast I expect that the states have better hospital coverage given that they were also established as states over a century earlier than Utah, so they’ve had that time to construct more expansive infrastructure compared. That being said, it’s still important to address the lacking access of those who are underserved, even if it seems like a small percent of the population.\n\n\n\n\n\n\n\nCode\n# Spatial join tracts to counties.\n# Remove the \" County\" substring from the NAMELSADCO column values.\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(NAMELSADCO = str_remove_all(NAMELSADCO, \" County\"))\n\n# Change the values in NAMELSADCO to all capitalized for later joining.\nvulnerable_tracts$NAMELSADCO &lt;- toupper(vulnerable_tracts$NAMELSADCO)\n\n# Align CRS of both data before spatial join.\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 4326)\npa_counties &lt;- st_transform(pa_counties, crs = 4326)\n\n# Spatial left join.\ncounty_tracts &lt;- st_join(vulnerable_tracts, pa_counties, left = TRUE)\n\n# Aggregate statistics by county\naggregate_summary &lt;- county_tracts %&gt;%\n  st_drop_geometry() %&gt;% # Drop the geometry.\n  group_by(NAMELSADCO) %&gt;% # Group by county.\n  summarize(\n    # Summarize the number of vulnerable and underserved people.\n    number_vulnerable = sum(vulnerable),\n    number_underserved = sum(underserved),\n    # Summarize the percent of underserved people.\n    percent_underserved = (number_underserved / number_vulnerable) * 100,\n    # Summarize the average distance to the nearest hospital.\n    avg_nearby_hospital_dist = mean(nearest_hospital_mi, na.rm = TRUE),\n    # Summarize the total estimated population.\n    total_vulnerable_pop = sum(total_popE)\n  ) %&gt;%\n  # Rename the NAMELSADCO column to county.\n  rename(\n    county = NAMELSADCO\n  )\n\n# Change COUNTY_NAM to county.\npa_counties &lt;- rename(pa_counties, county = COUNTY_NAM)\n\n# Conduct a left join.\npa_mapping &lt;- left_join(pa_counties, aggregate_summary, by = \"county\")\n\n\n\n\nCode\n# Plot map of percent underserved by Pennsylvania counties.\nggplot(pa_mapping) +\n  geom_sf(aes(fill = percent_underserved), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Percent Underserved\",\n    option = \"magma\",\n    # Format to show percent sign on color bar.\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  labs(\n    title = \"Most Underserved Counties\",\n    subtitle = \"Pennsylvania, United States\",\n    caption = \"Source: ACS 2019–2023\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nSullivan, Cameron, Forest, Clearfield, and Juniata at 100% for the first three counties then 82.22% and 71.43% for the latter two counties.\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nCounties with an average hospital distance of more than 15 miles and with the most vulnerable people in that group are Bradford, Potter, Sullivan, Cameron, and Juniata that have average hospital distances more than 15 miles. Bradford consists of 1 census tract and has the least number of vulnerable people at 2,833 and Cameron consists of 7 census tracts and has the most at 17,052 vulnerable people. Other counties fall between the two ranges, but below 10,000 for the vulnerable population, so Cameron is a significant outlier.\n\nAre there any patterns in where underserved counties are located?\n\nIt looks like more of the underserved counties are in the northern portion of Pennsylvania, and seemingly clustered toward the middle in addition. I also know those areas are peppered with several forested regions like national and state forests. Development may be much more restricted due to that. There are also underserved counties at the fringes of Pennsylvania, and several counties noted as NA due to the fact that the census tracts within them were part of the 64 NA tracts.\n\n\n\n\n\n\n\nCode\n# Create and format priority counties table.\nkable_table &lt;- pa_mapping[, c(\"county\", \"number_vulnerable\",\n                                \"number_underserved\", \"percent_underserved\",\n                                \"avg_nearby_hospital_dist\", \"total_vulnerable_pop\")]\n\n# Drop geometry and rename the columns.\nkable_table &lt;- kable_table %&gt;%\n  st_drop_geometry() %&gt;%\n  rename(\n    \"County\" = county,\n    \"Number of Vulnerable Tracts\" = number_vulnerable,\n    \"Number of Underserved Tracts\" = number_underserved,\n    \"Percent Underserved\" = percent_underserved,\n    \"Average Mile(s) to Nearest Hospital\" = avg_nearby_hospital_dist,\n    \"Total Vulnerable People\" = total_vulnerable_pop\n  )\n\n# Filter and arrage the top 10 underserved counties by percent.\nkable_table &lt;- kable_table %&gt;%\n  arrange(desc(`Percent Underserved`)) %&gt;%\n  slice(1:10)\n\n# Format percent column to have % sign.\nkable_table &lt;- kable_table %&gt;%\n  mutate(\n    # Format so percent and distance values have units.\n    `Percent Underserved` = paste0(round(`Percent Underserved`, 2), \"%\"),\n    `Average Mile(s) to Nearest Hospital` = paste0(round(`Average Mile(s) to Nearest Hospital`, 2), \" mi\")\n  )\n\n# Create kable.\nkable(\n  kable_table,\n  # Center align.\n  align = \"c\",\n  # Limit percents to two decimal places.\n  digit = 2,\n  # Make sure numbers are separated by thousands places.\n  format.args = list(big.mark = \",\"),\n  caption = \"&lt;b&gt;TOP 10 UNDERSERVED PA COUNTIES: For Priority Healthcare Investment&lt;/b&gt;\"\n) %&gt;%\n  # Shade every other row for ease of reading.\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"mistyrose\", background = \"red3\")\n\n\n\nTOP 10 UNDERSERVED PA COUNTIES: For Priority Healthcare Investment\n\n\nCounty\nNumber of Vulnerable Tracts\nNumber of Underserved Tracts\nPercent Underserved\nAverage Mile(s) to Nearest Hospital\nTotal Vulnerable People\n\n\n\n\nSULLIVAN\n6\n6\n100%\n20.5 mi\n8,116\n\n\nCAMERON\n6\n6\n100%\n19.31 mi\n15,048\n\n\nFOREST\n4\n4\n100%\n18.04 mi\n10,108\n\n\nCLEARFIELD\n6\n5\n83.33%\n15.76 mi\n15,831\n\n\nJUNIATA\n7\n5\n71.43%\n14.06 mi\n17,193\n\n\nPOTTER\n5\n3\n60%\n14.61 mi\n11,862\n\n\nBRADFORD\n2\n1\n50%\n9.36 mi\n8,715\n\n\nCRAWFORD\n2\n1\n50%\n15.37 mi\n4,018\n\n\nBUCKS\n2\n0\n0%\n4.02 mi\n13,326\n\n\nTIOGA\n3\n0\n0%\n5.88 mi\n10,515"
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html#map-1-county-level-choropleth",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html#map-1-county-level-choropleth",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Map 1: County-Level Choropleth",
    "text": "Map 1: County-Level Choropleth\n\n\nCode\n# Create county-level access map.\n# Align CRS.\npa_hospitals &lt;- st_transform(pa_hospitals, crs = 4326)\npa_mapping &lt;- st_transform(pa_mapping, crs = 4326)\n\n# Choropleth map of PA counties.\nchoropleth_pa &lt;- ggplot(pa_mapping) +\n  geom_sf(\n    # Color by underserved percentage.\n    aes(fill = percent_underserved),\n    color = \"white\",\n    linewidth = 0.25\n  ) +\n  scale_fill_viridis_c(\n    # Color bar customization.\n    name = \"Percent Underserved\",\n    option = \"viridis\",\n    # Format to show percent sign in color bar.\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  geom_point(\n    data = pa_hospitals,\n    aes(x = LONGITUDE, y = LATITUDE, color = \"Hospitals\"),\n    alpha = 0.75, inherit.aes = FALSE\n  ) +\n  scale_color_manual(\n    name = \"Hospitals\",\n    values = c(\"Hospitals\" = \"red\")\n  ) +\n  labs(\n    title = \"UNDERSERVED COUNTIES & HOSPITAL LOCATIONS\",\n    subtitle = \"Pennsylvania, United States\",\n    caption = \"Source: 5-Year American Community Survey (ACS) 2019–2023\",\n  ) +\n  theme_void()\n\nchoropleth_pa + theme(\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(family = \"mono\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)"
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html#map-2-detailed-vulnerability-map",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html#map-2-detailed-vulnerability-map",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Map 2: Detailed Vulnerability Map",
    "text": "Map 2: Detailed Vulnerability Map\n\n\nCode\n# Create detailed tract-level map.\n# Align CRS.\nvulnerable_tracts &lt;- st_transform(vulnerable_tracts, crs = 4326)\npa_tracts &lt;- st_transform(pa_tracts, crs = 4326)\n\n# Filter by underserved tracts.\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  rename(\"Underserved Tracts\" = underserved)\n\n# Choropleth map of PA census tracts colored by vulnerable and underserved.\nchoropleth_map &lt;- ggplot(vulnerable_tracts) +\n  geom_sf(\n    aes(fill = `Underserved Tracts`),\n  ) +\n  geom_sf(\n    data = pa_tracts, fill = NA,\n    color = \"midnightblue\", linewidth = 0.25\n  ) +\n  geom_sf(\n    data = pa_counties, fill = NA,\n    color = \"midnightblue\", linewidth = 0.5\n  ) +\n  geom_point(\n    data = pa_hospitals,\n    aes(x = LONGITUDE, y = LATITUDE, color = \"Hospitals\"),\n    alpha = 0.75, inherit.aes = FALSE\n  ) +\n  scale_fill_manual(\n    # Custom color bar.\n    name = \"Tracts\",\n    values = c(\"TRUE\" = \"gold2\", \"FALSE\" = \"lemonchiffon2\"),\n    labels = c(\"TRUE\" = \"Underserved\", \"FALSE\" = \"Vulnerable\"),\n    # Reorder so that underserved is placed above vulnerable in the legend.\n    breaks = c(TRUE, FALSE)\n  ) +\n  scale_color_manual(\n    # Custom point addition to legend.\n    name = \"Hospitals\",\n    values = c(\"Hospitals\" = \"red2\")\n  ) +\n  labs(\n    title = \"UNDERSERVED AND VULNERABLE CENSUS TRACTS\",\n    subtitle = \"With Hospital Locations in Pennsylvania, United States\",\n    caption = \"Source: 5-Year American Community Survey (ACS) 2019–2023\",\n  ) +\n  theme_void()\n\nchoropleth_map + theme(\n  panel.background = element_rect(fill = \"azure3\", size = 0.75),\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(family = \"mono\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)"
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html#chart-distribution-analysis",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html#chart-distribution-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Chart: Distribution Analysis",
    "text": "Chart: Distribution Analysis\n\n\nCode\n# Create distribution visualization.\n# Make histogram of distance to nearest hospital.\nhistogram_hospital &lt;- ggplot(vulnerable_tracts, aes(x = nearest_hospital_mi)) +\n  geom_histogram(\n    color = \"black\",\n    fill = \"mistyrose2\"\n  ) +\n  labs(\n    title = \"DISTANCE TO NEAREST HOSPITAL\",\n    subtitle = \"Histogram\",\n    x = \"Distance to Nearest Hospital (mi)\",\n    y = \"Count\") +\n  theme_minimal() +\n  scale_x_continuous(labels = scales::label_number(suffix = \" mi\")) + # Format so tick numbers have miles at end.\n  theme(aspect.ratio = 1/3) # Maintain aspect ratio.\n\n# Make histogram of elderly population.\nhistogram_elderly &lt;- ggplot(vulnerable_tracts, aes(x = age_65_overE)) +\n  geom_histogram(\n    color = \"black\",\n    fill = \"mistyrose2\"\n  ) +\n  # Thousand separator.\n  scale_x_continuous(labels = comma) +\n  labs(\n    title = \"VULNERABLE POPULATION OF AGES 65+\",\n    subtitle = \"Histogram\",\n    x = \"Population of Ages 65+\",\n    y = \"Count\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1/3) # Maintain aspect ratio.\n\n# Create faceted plot of all three charts above in one column and three rows.\nfaceted_plot &lt;-\n  (histogram_hospital + theme(\n    # Customize margins and text.\n    plot.title = element_text(margin = margin(b = 10, unit = \"pt\"), face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, unit = \"pt\")),\n    axis.title.y = element_text(margin = margin(r = 10, unit = \"pt\"))\n  )) /\n  (histogram_elderly + theme(\n    # Customize margins and text.\n    plot.title = element_text(margin = margin(t = 20, b = 10, unit = \"pt\"), face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 10, unit = \"pt\")),\n    axis.title.y = element_text(margin = margin(r = 10, unit = \"pt\"))\n  ))\n\n# Display faceted plot.\nfaceted_plot\n\n\n\n\n\n\n\n\n\nOf the vulnerable tracts, it looks like the distances to the nearest hospitals are generally less than 5 miles and the histogram is skewed right. From this visual distribution, it looks like there’s accessibility with vulnerable populations and drivable distance to the nearest hospital. The elderly population also look like they cluster around the 500 estimate, also skewing to the right significantly once the estimate reachs 1,000 or more."
  },
  {
    "objectID": "labs/lab-2/Vu_Tessa_Assignment2.html#analysis",
    "href": "labs/lab-2/Vu_Tessa_Assignment2.html#analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Analysis",
    "text": "Analysis\n\n\nCode\n# CRS checks and transformations.\n\n# WGS84 / Pseudo-Mercator (EPSG 3857).\n#st_crs(sharps_boxes)\n\n# WGS84 / World Geodetic System 1984 (EPSG 4326).\n#st_crs(treatment_facilities)\n\n# WGS84 / Pseudo-Mercator (EPSG 3857).\n#st_crs(free_meal_sites)\n\n# No CRS. Not a geodataframe. Data has latitude and longitude.\n#st_crs(crime_2024)\n\n# WGS84 / World Geodetic System 1984 (EPSG 4326).\n#st_crs(philly_county)\n\n# WGS84 / World Geodetic System 1984 (EPSG 4326).\n#st_crs(philly_tracts)\n\n# Change data CRS to Pennsylvania South (EPSG 3365) for calculations.\nsharps_boxes &lt;- sharps_boxes %&gt;%\n  st_transform(crs = 3365)\ntreatment_facilities &lt;- treatment_facilities %&gt;%\n  st_transform(crs = 3365)\nfree_meal_sites &lt;- free_meal_sites %&gt;%\n  st_transform(crs = 3365)\nphilly_county &lt;- philly_county %&gt;%\n  st_transform(crs = 3365)\nphilly_tracts &lt;- philly_tracts %&gt;%\n  st_transform(crs = 3365)\n\n# Check CRS changes.\n#st_crs(sharps_boxes)\n#st_crs(treatment_facilities)\n#st_crs(free_meal_sites)\n#st_crs(philly_county)\n#st_crs(philly_tracts)\n\ncrime_2024 &lt;- crime_2024 %&gt;%\n  # Remove points where latitude and longitude are NA or empty.\n  .[!(is.na(.$lng) | .$lng == \"\" | is.na(.$lat) | .$lat == \"\"),] %&gt;%\n  # Change to spatial dataframe and set CRS to 4326 because of coordinate data.\n  st_as_sf(., coords = c(\"lng\", \"lat\"), crs = 4326) %&gt;%\n  # Change to needed CRS for calculations.\n  st_transform(crs = 3365)\n\n# Check CRS change.\n#st_crs(crime_2024)\n\n# Filter 2024 crime data by drug-related crimes.\ndrugs_2024 &lt;- crime_2024 %&gt;%\n  filter(`text_general_code` == \"Narcotic / Drug Law Violations\")\n\n\n\n\nCode\n# Clean data, filter, and remove unneeded columns.\n\nsharps_boxes[, c(\"OBJECTID\", \"OBJECTID_1\", \"SITE_NAME\", \"SUB_NAME\", \"STATE\")] &lt;- list(NULL)\n\n# Filter to Philadelphia\ntreatment_facilities &lt;- treatment_facilities %&gt;%\n  filter(`COUNTY` == \"PHILADELPHIA\")\ntreatment_facilities[, c(\"OBJECTID\", \"FACILITY_I\", \"GEOCODING_\", \"STREET_2\", \"CITY\", \"STATE\", \"TELEPHONE_\")] &lt;- list(NULL)\n\n# Filter to active sites.\nfree_meal_sites &lt;- free_meal_sites %&gt;%\n  filter(`status` == \"Active\")\nfree_meal_sites[, c(\"objectid\", \"phone_numb\", \"temporary_\", \"site_key\", \"website\", \"hours_mon_\", \"hours_tues\", \"hours_wed_\", \"hours_thur\", \"hours_fri_\", \"hours_sat_\", \"hours_sa_1\", \"hours_sa_2\", \"hours_sun_\", \"email\", \"seasonally\")] &lt;- list(NULL)\n\ndrugs_2024[, c(\"the_geom\", \"cartodb_id\", \"the_geom_webmercator\", \"objectid\", \"dc_dist\", \"psa\", \"dispatch_date_time\", \"dispatch_date\", \"dispatch_time\", \"hour\", \"dc_key\", \"ucr_general\")] &lt;- list(NULL)\n\nphilly_tracts[, c(\"STATEFP\", \"COUNTYFP\", \"GEOIDFQ\", \"NAME\", \"NAMELSAD\", \"STUSPS\", \"TRACTCE\", \"STATE_NAME\", \"LSAD\", \"TRACT\")] &lt;- list(NULL)\ncolnames(philly_tracts)[2] &lt;- \"county\"\n\n\n\nWhat dataset was chosen and why?\n\nAside from ACS data for PA’s counties and census tracts, I chose OpenDataPhilly’s crime incidence data, sharps drop boxes data, drug and alcohol treatment facilities, and free food program site data. This is because I want to investigate the spatial overlap and relationships with drug incidences and resources.\n\nWhat is the data source and date?\n\nThe crime data is from 2024, free meal sites from end of last year, and facilities and sharps bins data are from end of year 2023. All of these data had no official creation dates on their pages, but the aforementioned years were observed from the public GitHub repository. All data were extracted from OpenDataPhilly’s website.\n\nHow many features does it contain?\n\nThe crime data has 160,388 features, free meals site data has 304 features, treatment facilities data has 965 features, and sharps box data has 27 features.\n\nWhat CRS is it in? Was transforming needed?\n\nThe crime data has no CRS but data having coordinates indicates WGS84, free meals site data is in WGS84 / Pseudo-Mercator (EPSG 3857), treatment facilities is in WGS84 / World Geodetic System 1984 (EPSG 4326), and sharps box data is in WGS84 / Pseudo-Mercator (EPSG 3857). Both the Philadelphia tract and county data are in WGS84 / World Geodetic System 1984 (EPSG 4326). I made sure to transform all data to Pennsylvania South (EPSG 3365) for calculations. For the crime data without the CRS, I had to turn it into a geodataframe and change the CRS to WGS84 first for that function, and then transform the CRS again to Pennsylvania South.\n\n\n\nResearch Question\n\n\nDo tracts with high drug-related instances have adequate access to treatment facilities, sharps bins, and free meal programs?\n\n\nConduct spatial analysis\n\n\n\nCode\n# Aggregate and count drug offenses in Philadelphia census tracts.\n# Use pipe to input st_intersects return values into lengths() function.\nphilly_tracts$drug_offense &lt;- st_intersects(philly_tracts, drugs_2024) |&gt; lengths()\n\n# Normalize drug offense data by population total.\nphilly_tracts &lt;- philly_tracts %&gt;%\n  mutate(pct_drug_offense = case_when(\n    # Calculate as 0 when both values are 0.\n    total_popE == 0 & drug_offense == 0 ~ 0,\n    # Calculate as NA with numeric type rather than NA with logical type to avoid type mismatch.\n    total_popE == 0 ~ NA_real_,\n    # Otherwise, calculate normally.\n    TRUE ~ (drug_offense / total_popE) * 100\n  ))\n\n# Determine high drug offense tracts as above 50% of bell curve.\nphilly_tracts &lt;- philly_tracts %&gt;%\n  mutate(\"high_drug_offense\" = case_when(\n    .$pct_drug_offense &gt; 0.20539 ~ TRUE,\n    TRUE ~ FALSE\n  ))\n\n# Filter tracts with drug offenses above the median.\ndrug_tracts &lt;- philly_tracts %&gt;%\n  filter(.$high_drug_offense == TRUE)\n\n\n\n\nCode\n# Create census tract centroids for drug_tracts.\ndrug_centroids &lt;- st_centroid(drug_tracts)\n\n# Calculate distances between centroids and food accessibility.\nnearest_meal_index &lt;- st_nearest_feature(drug_centroids, free_meal_sites)\n\nnearest_meal_point &lt;- free_meal_sites[nearest_meal_index,]\n\nmeal_distance &lt;- st_distance(drug_centroids, nearest_meal_point, by_element = TRUE)\n\ndrug_tracts$nearest_meal_mi &lt;- set_units(meal_distance, \"mi\")\n\n# Calculate distances between centroids and treatment facilities.\nnearest_facility_index &lt;- st_nearest_feature(drug_centroids, treatment_facilities)\n\nnearest_facility_point &lt;- treatment_facilities[nearest_facility_index,]\n\nfacility_distance &lt;- st_distance(drug_centroids, nearest_facility_point, by_element = TRUE)\n\ndrug_tracts$nearest_facility_mi &lt;- set_units(facility_distance, \"mi\")\n\n# Calculate distances between centroids and sharps bins.\nnearest_sharps_index &lt;- st_nearest_feature(drug_centroids, sharps_boxes)\n\nnearest_sharps_point &lt;- sharps_boxes[nearest_sharps_index,]\n\nsharps_distance &lt;- st_distance(drug_centroids, nearest_sharps_point, by_element = TRUE)\n\ndrug_tracts$nearest_sharps_mi &lt;- set_units(sharps_distance, \"mi\")\n\n# Determine how many tracts' nearest free meal program is more than 0.5 mi. away.\ntracts_lacking_meals &lt;- sum(drug_tracts$nearest_meal_mi &gt; set_units(0.5, \"mi\"))\n# Determine how many tracts' nearest treatment facility is more than 0.5 mi. away.\ntracts_lacking_facilities &lt;- sum(drug_tracts$nearest_facility_mi &gt; set_units(0.5, \"mi\"))\n# Determine how many tracts' nearest sharps bin is more than 0.5 mi. away.\ntracts_lacking_sharps &lt;- sum(drug_tracts$nearest_sharps_mi &gt; set_units(0.5, \"mi\"))\n\nprint(paste0(\"There are \", tracts_lacking_meals, \" census tracts that don't have active free meal programs within 0.5 mi. walking distance.\"))\n\n\n[1] \"There are 7 census tracts that don't have active free meal programs within 0.5 mi. walking distance.\"\n\n\nCode\nprint(paste0(\"There are \", tracts_lacking_facilities, \" census tracts that don't have drug alcohol treatment facilities within 0.5 mi. walking distance.\"))\n\n\n[1] \"There are 13 census tracts that don't have drug alcohol treatment facilities within 0.5 mi. walking distance.\"\n\n\nCode\nprint(paste0(\"There are \", tracts_lacking_sharps, \" census tracts that don't have disposable sharps bins within 0.5 mi. walking distance.\"))\n\n\n[1] \"There are 47 census tracts that don't have disposable sharps bins within 0.5 mi. walking distance.\"\n\n\n\n\nCode\n# Map tracts more than 0.5 mile walking distance from free food programs.\nfar_from_meals_tracts &lt;- drug_tracts %&gt;%\n  mutate(nearest_meal_mi = drop_units(nearest_meal_mi)) %&gt;%\n  filter(nearest_meal_mi &gt; 0.5)\n\n# Transform geography data back to WGS84 for web mapping.\nfar_from_meals_tracts &lt;- far_from_meals_tracts %&gt;%\n  st_transform(crs = 4326)\n\n# Choropleth map colored by drug offenses with treatment facility points.\nfar_from_meals_choropleth &lt;- ggplot(far_from_meals_tracts) +\n  geom_sf(\n    aes(fill = `pct_drug_offense`),\n  ) +\n  geom_sf(\n    data = philly_tracts, fill = NA,\n    color = \"black\", linewidth = 0.25,\n    alpha = 0.5\n  ) +\n  geom_point(\n    data = free_meal_sites,\n    aes(x = x, y = y, color = \"Free Meals\"),\n    alpha = 0.75, inherit.aes = FALSE\n  ) +\n  scale_fill_viridis_c(\n    name = \"Percent Drug Events\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  scale_color_manual(\n    name = \"Free Meals\",\n    values = c(\"Free Meals\" = \"red2\")\n  ) +\n  labs(\n    title = \"HIGH DRUG EVENTS IN PHILADELPHIA, PA\",\n    subtitle = \"More than 0.5 mi from Free Meals\",\n    caption = \"Sources: 5-Year ACS 2023, OpenDataPhilly 2023–2025\",\n  ) +\n  theme_void()\n\nfar_from_meals_choropleth + theme(\n  panel.background = element_rect(fill = \"azure2\", size = 0.75),\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(face = \"italic\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map tracts more than 0.5 mile walking distance from treatment facilities.\nfar_from_facilities_tracts &lt;- drug_tracts %&gt;%\n  mutate(nearest_facility_mi = drop_units(nearest_facility_mi)) %&gt;%\n  filter(nearest_facility_mi &gt; 0.5)\n\n# Transform geography data back to WGS84 for web mapping.\nfar_from_facilities_tracts &lt;- far_from_facilities_tracts %&gt;%\n  st_transform(crs = 4326)\n\n# Choropleth map colored by drug offenses with treatment facility points.\nfar_from_facilities_choropleth &lt;- ggplot(far_from_facilities_tracts) +\n  geom_sf(\n    aes(fill = `pct_drug_offense`),\n  ) +\n  geom_sf(\n    data = philly_tracts, fill = NA,\n    color = \"black\", linewidth = 0.25,\n    alpha = 0.5\n  ) +\n  geom_point(\n    data = treatment_facilities,\n    aes(x = LONGITUDE, y = LATITUDE, color = \"Treatment Facilities\"),\n    alpha = 0.75, inherit.aes = FALSE\n  ) +\n  scale_fill_viridis_c(\n    name = \"Percent Drug Events\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  scale_color_manual(\n    name = \"Treatment Facilities\",\n    values = c(\"Treatment Facilities\" = \"red2\")\n  ) +\n  labs(\n    title = \"HIGH DRUG EVENTS IN PHILADELPHIA, PA\",\n    subtitle = \"More than 0.5 mi from Treatment Facilities\",\n    caption = \"Sources: 5-Year ACS 2023, OpenDataPhilly 2023–2025\",\n  ) +\n  theme_void()\n\nfar_from_facilities_choropleth + theme(\n  panel.background = element_rect(fill = \"azure2\", size = 0.75),\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(face = \"italic\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map tracts more than 0.5 mile walking distance from sharps bins.\nfar_from_sharps_tracts &lt;- drug_tracts %&gt;%\n  mutate(nearest_sharps_mi = drop_units(nearest_sharps_mi)) %&gt;%\n  filter(nearest_sharps_mi &gt; 0.5)\n\n# Align CRS.\nfar_from_sharps_tracts &lt;- st_transform(far_from_sharps_tracts, 4326)\nsharps_boxes &lt;- st_transform(sharps_boxes, 4326)\n\n# Extract coordinates from geometry to new columns for plotting.\nsharps_boxes &lt;- sharps_boxes %&gt;%\n  mutate(\n    lon = st_coordinates(geometry)[, 1],\n    lat = st_coordinates(geometry)[, 2]\n  )\n\n# Choropleth map colored by drug offenses with sharps bin points.\nsharps_choropleth &lt;- ggplot(far_from_sharps_tracts) +\n  geom_sf(\n    aes(fill = `pct_drug_offense`),\n  ) +\n  geom_sf(\n    data = philly_tracts, fill = NA,\n    color = \"black\", linewidth = 0.25,\n    alpha = 0.5\n  ) +\n  geom_point(\n    data = sharps_boxes,\n    aes(x = lon, y = lat, color = \"Sharps Bins\"),\n    alpha = 0.75, inherit.aes = FALSE\n  ) +\n  scale_fill_viridis_c(\n    name = \"Percent Drug Events\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  scale_color_manual(\n    name = \"Sharps Bins\",\n    values = c(\"Sharps Bins\" = \"red2\")\n  ) +\n  labs(\n    title = \"HIGH DRUG EVENTS IN PHILADELPHIA, PA\",\n    subtitle = \"More than 0.5 mi from Sharps Bins\",\n    caption = \"Sources: 5-Year ACS 2023, OpenDataPhilly 2023–2025\",\n  ) +\n  theme_void()\n\nsharps_choropleth + theme(\n  panel.background = element_rect(fill = \"azure2\", size = 0.75),\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(face = \"italic\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Align CRS.\ndrug_tracts &lt;- st_transform(drug_tracts, 4326)\n\n# Choropleth map colored by drug offenses with sharps bin points and treatment facilities.\nall_choropleth &lt;- ggplot(drug_tracts) +\n  geom_sf(\n    aes(fill = `pct_drug_offense`),\n  ) +\n  geom_sf(\n    data = philly_tracts, fill = NA,\n    color = \"azure2\", linewidth = 0.25,\n    alpha = 0.5\n  ) +\n  geom_point(\n    data = treatment_facilities,\n    aes(x = LONGITUDE, y = LATITUDE, color = \"Treatment Facilities\"),\n    alpha = 0.8, inherit.aes = FALSE\n  ) +\n  geom_point(\n    data = sharps_boxes,\n    aes(x = lon, y = lat, color = \"Sharps Bins\"),\n    alpha = 0.8, inherit.aes = FALSE\n  ) +\n  scale_fill_viridis_c(\n    name = \"Percent Drug Events\",\n    labels = function(x) paste0(x, \"%\")\n  ) +\n  scale_color_manual(\n    name = \"Harm Reduction\",\n    values = c(\"Treatment Facilities\" = \"orange\",\"Sharps Bins\" = \"red2\")\n  ) +\n  labs(\n    title = \"HIGH DRUG EVENTS IN PHILADELPHIA, PA\",\n    subtitle = \"With Sharps Bins and Treatment Facilities\",\n    caption = \"Sources: 5-Year ACS 2023, OpenDataPhilly 2023–2025\",\n  ) +\n  theme_void()\n\nall_choropleth + theme(\n  panel.background = element_rect(fill = \"azure4\", size = 0.75),\n  legend.box.margin = margin(l = 10, unit = \"pt\"),\n  legend.title = element_text(face = \"bold\"),\n  legend.text = element_text(face = \"italic\"),\n  plot.title = element_text(face = \"bold\"),\n  plot.subtitle = element_text(margin = margin(b = 10, t = 5, unit = \"pt\")),\n  plot.caption = element_text(margin = margin(t = 10, unit = \"pt\"), face = \"italic\", family = \"mono\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Summary statistics of data used.\n\ndata_used &lt;- drug_tracts %&gt;%\n  transmute(\n  \"GEOID\" = GEOID,\n  \"Population Estimate\" = total_popE,\n  \"Population MOE\" = total_popM,\n  \"Drug Incidence\" = drug_offense,\n  \"Percent Drug Incidence\" = pct_drug_offense,\n  \"Nearest Meal (mi)\" = nearest_meal_mi,\n  \"Nearest Facility (mi)\" = nearest_facility_mi,\n  \"Nearest Sharps (mi)\" = nearest_sharps_mi\n)\n\n# Align CRS.\ndata_used &lt;- st_transform(data_used, 4326)\nfree_meal_sites &lt;- st_transform(free_meal_sites, 4326)\nsharps_boxes &lt;- st_transform(sharps_boxes, 4326)\ntreatment_facilities &lt;- st_transform(treatment_facilities, 4326)\n\n# Only get geometry of point data.\nselected_meals &lt;- free_meal_sites %&gt;%\n  transmute(geometry)\n\nselected_sharps &lt;- sharps_boxes %&gt;%\n  select(geometry)\n\nselected_facilities &lt;- treatment_facilities %&gt;%\n  select(geometry)\n\n# Task is repetitive, so create a function to help with counting all the meal, facility, and sharps bin point data for each tract.\ncount_points &lt;- function(point_data, tract_data, id = \"GEOID\", name = \"Count\"){\n  result &lt;- point_data %&gt;%\n    # Join using within predicate.\n    st_join(tract_data, join = st_within) %&gt;%\n    # Drop point geometry to avoid duplication when joining back with tracts later.\n    st_drop_geometry() %&gt;%\n    # Group by census tract GEOID.\n    group_by(.data[[id]]) %&gt;%\n    # Count instances of points in each GEOID.\n    summarize(Count = n())\n  \n  # Change column name to user input name.\n  names(result)[names(result) == \"Count\"] &lt;- name\n  \n  # Return the result.\n  result\n}\n\n# Use function on all point datasets.\nnumber_meals &lt;- count_points(free_meal_sites, data_used, \"GEOID\", \"Number of Meals\")\nnumber_sharps &lt;- count_points(sharps_boxes, data_used, \"GEOID\", \"Number of Sharps\")\nnumber_facilities &lt;- count_points(treatment_facilities, data_used, \"GEOID\", \"Number of Facilities\")\n\n# Add counts back into tract data using left join.\ndata_used &lt;- data_used %&gt;%\n  left_join(number_meals, by = \"GEOID\") %&gt;%\n  left_join(number_sharps, by = \"GEOID\") %&gt;%\n  left_join(number_facilities, by = \"GEOID\") %&gt;%\n  mutate(\n    # Fill empty cells with 0.\n    `Number of Meals` = replace_na(`Number of Meals`, 0),\n    `Number of Sharps` = replace_na(`Number of Sharps`, 0),\n    `Number of Facilities` = replace_na(`Number of Facilities`, 0)\n  )\n\n# Create dataframe from geodataframe for all data used.\ndata_used_df &lt;- data_used %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    GEOID = NULL\n  )\n\n\n\n\nCode\n# No scientific notation.\noptions(scipen = 999)\n\n# Tidy summary statistics for kable.\nsummary_stats &lt;- data_used_df %&gt;%\n  # Use map function to use summary function on each column,\n  # generating descriptive statistics for the values and storing in a dataframe.\n  map(summary) %&gt;%\n  # Convert output list to tibble, tidying and stacking rows.\n  map_df(tidy) %&gt;%\n  # Use original column names as variable values and place \"variable\" as the first column.\n  mutate(variable = names(data_used_df), .before = 1)\n\n\n\n\nCode\n# Kable of summary statistics.\nkable(\n  as.data.frame(summary_stats),\n  align = \"c\",\n  digit = 2,\n  format.args = list(big.mark = \",\"),\n  col.names = c(\"VARIABLE\", \"MIN\", \"Q1\", \"MEDIAN\", \"MEAN\", \"Q3\", \"MAX\"),\n  caption = \"&lt;b&gt;DRUG-RELATED DATA IN PHILADELPHIA, PA: Summary Statistics&lt;/b&gt;\"\n) %&gt;%\n  # Shade every other row for ease of reading.\n  kable_styling(bootstrap_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nDRUG-RELATED DATA IN PHILADELPHIA, PA: Summary Statistics\n\n\nVARIABLE\nMIN\nQ1\nMEDIAN\nMEAN\nQ3\nMAX\n\n\n\n\nPopulation Estimate\n50.00\n2,537.00\n3,207.00\n3,561.02\n4,410.00\n8,835.00\n\n\nPopulation MOE\n44.00\n446.00\n690.00\n752.34\n965.00\n1,800.00\n\n\nDrug Incidence\n4.00\n10.00\n15.00\n36.02\n23.00\n519.00\n\n\nPercent Drug Incidence\n0.21\n0.33\n0.46\n1.24\n0.63\n14.77\n\n\nNearest Meal (mi)\n0.02\n0.12\n0.19\n0.25\n0.28\n1.57\n\n\nNearest Facility (mi)\n0.02\n0.22\n0.31\n0.36\n0.45\n1.13\n\n\nNearest Sharps (mi)\n0.00\n0.42\n1.49\n1.49\n2.19\n3.72\n\n\nNumber of Meals\n0.00\n0.00\n1.00\n1.34\n2.00\n7.00\n\n\nNumber of Sharps\n0.00\n0.00\n0.00\n0.31\n0.00\n5.00\n\n\nNumber of Facilities\n0.00\n0.00\n0.00\n0.69\n1.00\n8.00\n\n\n\n\n\nMany census tracts with higher percents of drug-related incidents lie in North Philly, Near-Northeast Philly, and West Philly primarily above Market Street. It seems like Philadelphia is taking the drug-related concerns seriously, the drug-alcohol treatment facilities and active free meal programs are peppered all around the county and have good coverage over tracts with higher drug-related events.\nIt also looks like most meals and facilities are within walking distance. However, sharps bins don’t have too much spatial overlap and are lacking statistically, with average distances beyond a mile. It may be worth implementing more sharps bin sites around tracts with more drug-use events.\nWhile facilities may have disposals, many drug-users range from recreational to addicted and might fear arrest at facilities and even at sharps bins themselves, so it’s important to address the issue without any demonizing, dehumanizing, and criminalizing.\nThis quick look at resource distribution has policy relevance to community investment, harm reduction, siting, food programming, and quality of life for everyone, not just those directly affected by drug-use."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 PORTFOLIO",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: Learning reflections and key concepts.\nLabs: Completed assignments and analyses.\nFinal Project: Capstone modeling challenge.\n\n\n\n\nAnalytically driven and adaptable with 2 years of research and 4 years of professional GIS experience primarily in vector data. Applied and academic knowledge in surveying project sites, remote sensing, drone navigation, and photogrammetry methods. Experience with Java, Python, and R for OOP, data science, geocomputation, spatial analysis, web-mapping, app development, and visualization.\nAs a result of my education and work history I have a deep interest in urban prototyping and instrumentation to record cities, deploying IoT sensors to collect real-time data for research in urban science, transportation, and environmental issues. Extending from that curiosity underlies a steadfast belief in everyone’s rights to privacy and security in a world that constantly wants to monitor, police, and sell our data coercively and unconsensually—this fundamental foundation takes precedence to deploying any sensing device.\nThis course will allow me to refine my existing technical skillset and challenge me to apply them to urban and social predicaments that plague us—cities, municipalities, communities, the environment, people. What’s important to me is that this class focuses on not becoming lost in the STEM aesthetics at the cost of humanitarian rights, educating future graduates about the damage that predictive analytics has wrought, especially in policing and how many tools wielded can disproportionately affect minoritized identities and peripheral communities.\n\n\n\n\nPersonal Email: tessavu@proton.me\nStudent Email: tessavu@upenn.edu\nGitHub: @tess-vu"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 PORTFOLIO",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 PORTFOLIO",
    "section": "",
    "text": "Weekly Notes: Learning reflections and key concepts.\nLabs: Completed assignments and analyses.\nFinal Project: Capstone modeling challenge."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 PORTFOLIO",
    "section": "",
    "text": "Analytically driven and adaptable with 2 years of research and 4 years of professional GIS experience primarily in vector data. Applied and academic knowledge in surveying project sites, remote sensing, drone navigation, and photogrammetry methods. Experience with Java, Python, and R for OOP, data science, geocomputation, spatial analysis, web-mapping, app development, and visualization.\nAs a result of my education and work history I have a deep interest in urban prototyping and instrumentation to record cities, deploying IoT sensors to collect real-time data for research in urban science, transportation, and environmental issues. Extending from that curiosity underlies a steadfast belief in everyone’s rights to privacy and security in a world that constantly wants to monitor, police, and sell our data coercively and unconsensually—this fundamental foundation takes precedence to deploying any sensing device.\nThis course will allow me to refine my existing technical skillset and challenge me to apply them to urban and social predicaments that plague us—cities, municipalities, communities, the environment, people. What’s important to me is that this class focuses on not becoming lost in the STEM aesthetics at the cost of humanitarian rights, educating future graduates about the damage that predictive analytics has wrought, especially in policing and how many tools wielded can disproportionately affect minoritized identities and peripheral communities."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 PORTFOLIO",
    "section": "",
    "text": "Personal Email: tessavu@proton.me\nStudent Email: tessavu@upenn.edu\nGitHub: @tess-vu"
  },
  {
    "objectID": "final-project.html",
    "href": "final-project.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "final-project.html#final-project",
    "href": "final-project.html#final-project",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "TBA"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "I am a data analyst for the Utah Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. My supervisor has asked me to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on the Week 2 discussion of algorithmic bias, I need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#scenario",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "I am a data analyst for the Utah Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. My supervisor has asked me to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on the Week 2 discussion of algorithmic bias, I need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#data-retrieval",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\n\n\nCode\n# Write your get_acs() code here\nutah_reliability &lt;- get_acs(\n  geography = \"county\",\n  state = my_state,\n  # Median household income and total population.\n  variables = c(median_inc = \"B19013_001\", total_pop = \"B01003_001\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nutah_reliability &lt;- mutate(utah_reliability, NAME = str_remove(NAME, \" County, Utah\"))\n\n# Display the first few rows\nglimpse(utah_reliability)\n\n\nRows: 29\nColumns: 6\n$ GEOID       &lt;chr&gt; \"49001\", \"49003\", \"49005\", \"49007\", \"49009\", \"49011\", \"490…\n$ NAME        &lt;chr&gt; \"Beaver\", \"Box Elder\", \"Cache\", \"Carbon\", \"Daggett\", \"Davi…\n$ median_incE &lt;dbl&gt; 80268, 72769, 72719, 53734, 61250, 101285, 70821, 67056, 5…\n$ median_incM &lt;dbl&gt; 11189, 2873, 2016, 4102, 14033, 1754, 3448, 3821, 7536, 67…\n$ total_popE  &lt;dbl&gt; 7102, 58291, 134428, 20338, 638, 363032, 19779, 9898, 5121…\n$ total_popM  &lt;dbl&gt; NA, NA, NA, NA, 124, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#data-quality-assessment",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\n\n\nCode\n# Calculate MOE percentage and reliability categories using mutate()\nutah_reliability &lt;- mutate(utah_reliability, MOE_percent = (median_incM / median_incE) * 100) %&gt;%\n                mutate(utah_reliability, reliability_cat =\n                         case_when(\n                           MOE_percent &lt; 5 ~ \"High Confidence\",\n                           MOE_percent &lt;= 10 ~ \"Moderate Confidence\",\n                           MOE_percent &gt; 10 ~ \"Low Confidence\"\n                           )) %&gt;%\n                mutate(utah_reliability, low_flag = reliability_cat == \"Low Confidence\")\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\ncount_summary &lt;- utah_reliability %&gt;%\n  group_by(reliability_cat) %&gt;%\n  summarize(frequency = n()) %&gt;%\n  mutate(frequency = frequency) %&gt;%\n  mutate(percent_counties = frequency / sum(frequency))\n\ncount_summary\n\n\n# A tibble: 3 × 3\n  reliability_cat     frequency percent_counties\n  &lt;chr&gt;                   &lt;int&gt;            &lt;dbl&gt;\n1 High Confidence            10            0.345\n2 Low Confidence              9            0.310\n3 Moderate Confidence        10            0.345"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#high-uncertainty-counties",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\n\n\nCode\n# Create table of top 5 counties by MOE percentage\ntop_5 &lt;- data.frame()\n\ntop_5 &lt;- utah_reliability %&gt;%\n  arrange(desc(MOE_percent)) %&gt;%\n  slice(0:5) %&gt;%\n  select(2:8)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(\n  top_5,\n  col.names = c(\"County\", \"Median Income\", \"Median Income MOE\", \"Total Population\", \"Total Population MOE\", \"Percent MOE\", \"MOE Confidence\"),\n  digit = 2,\n  caption = \"&lt;b&gt;TOP 5 UTAH COUNTIES: HIGHEST MEDIAN INCOME MARGIN-OF-ERROR&lt;/b&gt;\"\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nTOP 5 UTAH COUNTIES: HIGHEST MEDIAN INCOME MARGIN-OF-ERROR\n\n\nCounty\nMedian Income\nMedian Income MOE\nTotal Population\nTotal Population MOE\nPercent MOE\nMOE Confidence\n\n\n\n\nPiute\n33359\n12379\n1764\n124\n37.11\nLow Confidence\n\n\nWayne\n64870\n16573\n2532\nNA\n25.55\nLow Confidence\n\n\nDaggett\n61250\n14033\n638\n124\n22.91\nLow Confidence\n\n\nKane\n70327\n11967\n7814\nNA\n17.02\nLow Confidence\n\n\nBeaver\n80268\n11189\n7102\nNA\n13.94\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\nThe top five counties with the highest MOEs are rural—Piute, Wayne, Daggett, and Kane counties lack a major interstate highway in a state with cities that rely on car-transport within urban areas as well as within the vast swaths of undeveloped desert land. Beaver county at fifth does have the I-15 freeway. Another factor is the environmental topography that could influence towns’ infrastructure expansion as well as travel, all five counties’ are mountainous regions for the majority of their landscape, making it difficult and expensive to develop."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#focus-area-selection",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\n\n\nCode\n# Use filter() to select 2-3 counties from your utah_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties &lt;- utah_reliability %&gt;%\n  filter(NAME == \"Salt Lake\" | NAME == \"Morgan\" | NAME == \"Piute\") %&gt;%\n  arrange(MOE_percent)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(\n  selected_counties[c(\"GEOID\", \"NAME\", \"median_incE\", \"MOE_percent\", \"reliability_cat\")],\n  col.names = c(\"GEOID\", \"County\", \"Median Income\", \"MOE Percentage\", \"MOE Confidence\"),\n  digit = 2,\n  caption = \"&lt;b&gt;SELECTED UTAH COUNTIES&lt;/b&gt;\"\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nSELECTED UTAH COUNTIES\n\n\nGEOID\nCounty\nMedian Income\nMOE Percentage\nMOE Confidence\n\n\n\n\n49035\nSalt Lake\n90011\n1.17\nHigh Confidence\n\n\n49029\nMorgan\n120854\n7.78\nModerate Confidence\n\n\n49031\nPiute\n33359\n37.11\nLow Confidence\n\n\n\n\n\nComment on the output: This table output shows Salt Lake, Morgan, and Piute counties, which have high, moderate, and low confidence, respectively. Salt Lake has a 1.17% MOE percentage and Morgan has a 7.78% MOE percentage, Piute has a significant jump from Morgan by 29.33% units at 37.11%. Salt Lake and Morgan are counties that are actually adjacent to one another, whereas Piute is in central, rural Utah."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#tract-level-demographics",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\n\n\nCode\n# Define your race/ethnicity variables with descriptive names\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\ntract_level &lt;- get_acs(\n  geography = \"tract\",\n  state = my_state,\n  # Median household income and total population.\n  variables = c(white = \"B03002_003\", black = \"B03002_004\", hispanic = \"B03002_012\", total = \"B03002_001\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n) %&gt;%\n  filter(str_detect(GEOID, \"^49035\") | str_detect(GEOID, \"^49029\") | str_detect(GEOID, \"^49031\"))\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ntract_level &lt;- tract_level %&gt;%\n  mutate(\n    \"White Percentage\" = (whiteE / totalE) * 100,\n    \"Black Percentage\" = (blackE / totalE) * 100,\n    \"Hispanic Percentage\" = (hispanicE / totalE) * 100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\ntract_level &lt;- tract_level %&gt;%\n  separate(\n    col = NAME,\n    into = c(\"TRACT\", \"COUNTY\", \"STATE\"),\n    sep = \";\"\n  )\n\ntract_level\n\n\n# A tibble: 255 × 15\n   GEOID      TRACT COUNTY STATE whiteE whiteM blackE blackM hispanicE hispanicM\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 490299701… Cens… \" Mor… \" Ut…   5822    369     64     70       214       116\n 2 490299701… Cens… \" Mor… \" Ut…   4031    362      5     10       159       118\n 3 490299702… Cens… \" Mor… \" Ut…   1679    232      0     12         6        12\n 4 490319601… Cens… \" Piu… \" Ut…   1674    134      3      7        75        57\n 5 490351001… Cens… \" Sal… \" Ut…   1853    370    119    125       663       384\n 6 490351002… Cens… \" Sal… \" Ut…   1150    228     13     16       115        65\n 7 490351003… Cens… \" Sal… \" Ut…   1358    329    216    124      3319      1218\n 8 490351003… Cens… \" Sal… \" Ut…   1371    267     85    107      2603       475\n 9 490351003… Cens… \" Sal… \" Ut…    812    231    364    230      2358       454\n10 490351005… Cens… \" Sal… \" Ut…   2788    617    102    115      2010       730\n# ℹ 245 more rows\n# ℹ 5 more variables: totalE &lt;dbl&gt;, totalM &lt;dbl&gt;, `White Percentage` &lt;dbl&gt;,\n#   `Black Percentage` &lt;dbl&gt;, `Hispanic Percentage` &lt;dbl&gt;"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#demographic-analysis",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\n\n\nCode\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\ntract_level &lt;- tract_level %&gt;%\n  arrange(desc(`Hispanic Percentage`))\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\ntract_level_summary &lt;- tract_level %&gt;%\n  group_by(COUNTY) %&gt;%\n  summarize(\"Number of Tracts\" = n(),\n            \"White Average Percent\" = mean(`White Percentage`, na.rm = TRUE),\n            \"Black Average Percent\" = mean(`Black Percentage`, na.rm = TRUE),\n            \"Hispanic Average Percent\" = mean(`Hispanic Percentage`, na.rm = TRUE)\n            )\n\n# Create a nicely formatted table of your results using kable()\nkable(\n  tract_level_summary,\n  caption = \"&lt;b&gt;SELECTED UTAH COUNTIES: Average Racial Percents&lt;/b&gt;\"\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nSELECTED UTAH COUNTIES: Average Racial Percents\n\n\nCOUNTY\nNumber of Tracts\nWhite Average Percent\nBlack Average Percent\nHispanic Average Percent\n\n\n\n\nMorgan County\n3\n93.48180\n0.3784084\n2.480240\n\n\nPiute County\n1\n94.89796\n0.1700680\n4.251701\n\n\nSalt Lake County\n251\n69.57696\n1.7770290\n18.331447"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\n\n\nCode\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\noptions(scipen = 999)\n\n# Avoid Inf values by changing 0 values to 0.01.\nMOE_percentages &lt;- tract_level %&gt;%\n  mutate(across(c(whiteM, whiteE, blackM, blackE, hispanicM, hispanicE),\n                ~ifelse(. == 0, 0.1, .)))\n\nMOE_percentages &lt;- MOE_percentages %&gt;%\n  mutate(\n    \"White MOE\" = (whiteM / whiteE) * 100,\n    \"Black MOE\" = (blackM / blackE) * 100,\n    \"Hispanic MOE\" = (hispanicM / hispanicE) * 100\n  )\n\n# Use logical operators (| for OR) in an ifelse() statement\nMOE_percentages &lt;- MOE_percentages %&gt;%\n  mutate(MOE_percentages,\n         \"MOE Flag\" = if_else(\n           `White MOE` &gt; 15 | `Black MOE` &gt; 15 | `Hispanic MOE` &gt; 15, TRUE, FALSE\n         )\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\ndata_quality_summary &lt;- MOE_percentages %&gt;%\n  group_by(COUNTY) %&gt;%\n  summarize(\n    \"Number of Tracts\" = n(),\n    \"Data Quality Issues\" = sum(`MOE Flag`, na.rm = TRUE)\n    )\n\ndata_quality_summary\n\n\n# A tibble: 3 × 3\n  COUNTY              `Number of Tracts` `Data Quality Issues`\n  &lt;chr&gt;                            &lt;int&gt;                 &lt;int&gt;\n1 \" Morgan County\"                     3                     3\n2 \" Piute County\"                      1                     1\n3 \" Salt Lake County\"                251                   251"
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#pattern-analysis",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\n\n\nCode\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n# Use group_by() and summarize() to create this comparison\n\nflagged_MOE &lt;- MOE_percentages %&gt;%\n  group_by(`MOE Flag`) %&gt;%\n  summarize(\n    \"Avg White\" = mean(whiteE, na.rm = TRUE), \"Avg % White\" = mean(`White Percentage`, na.rm = TRUE),\n    \"Avg Black\" = mean(blackE, na.rm = TRUE), \"Avg % Black\" = mean(`Black Percentage`, na.rm = TRUE),\n    \"Avg Hispanic\" = mean(hispanicE, na.rm = TRUE), \"Avg % Hispanic\" = mean(`Hispanic Percentage`, na.rm = TRUE)\n    )\n\n# Create a professional table showing the patterns\nkable(\n  flagged_MOE,\n  digit = 2,\n  caption = \"&lt;b&gt;SALT LAKE, MORGAN, AND PIUTE COUNTIES: Average Population and Percents by Race&lt;/b&gt;\"\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nSALT LAKE, MORGAN, AND PIUTE COUNTIES: Average Population and Percents by Race\n\n\nMOE Flag\nAvg White\nAvg % White\nAvg Black\nAvg % Black\nAvg Hispanic\nAvg % Hispanic\n\n\n\n\nTRUE\n3252.52\n69.96\n77.52\n1.75\n884.83\n18.09\n\n\n\n\n\n\n\nCode\n# Check minimums.\nmin(MOE_percentages[[\"White MOE\"]])\n\n\n[1] 6.309819\n\n\nCode\nmin(MOE_percentages[[\"Black MOE\"]])\n\n\n[1] 54.38596\n\n\nCode\nmin(MOE_percentages[[\"Hispanic MOE\"]])\n\n\n[1] 17.59717\n\n\nCode\n# Flag tracts with MOES below 15%.\nlow_MOE &lt;- MOE_percentages %&gt;%\n  mutate(MOE_percentages,\n         \"Low MOE\" = if_else(\n           `White MOE` &lt; 15 | `Black MOE` &lt; 15 | `Hispanic MOE` &lt; 15, TRUE, FALSE\n         )\n  )\n\n# Group low MOEs.\nlow_MOE_summary &lt;- low_MOE %&gt;%\n  group_by(`Low MOE`) %&gt;%\n  summarize(\n    \"Avg White\" = mean(whiteE, na.rm = TRUE), \"Avg % White\" = mean(`White Percentage`, na.rm = TRUE),\n    \"Avg Black\" = mean(blackE, na.rm = TRUE), \"Avg % Black\" = mean(`Black Percentage`, na.rm = TRUE),\n    \"Avg Hispanic\" = mean(hispanicE, na.rm = TRUE), \"Avg % Hispanic\" = mean(`Hispanic Percentage`, na.rm = TRUE)\n    )\n  \nlow_MOE_summary\n\n\n# A tibble: 2 × 7\n  `Low MOE` `Avg White` `Avg % White` `Avg Black` `Avg % Black` `Avg Hispanic`\n  &lt;lgl&gt;           &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 FALSE           2930.          63.8        96.2          2.15          1117.\n2 TRUE            3736.          79.0        49.5          1.17           536.\n# ℹ 1 more variable: `Avg % Hispanic` &lt;dbl&gt;\n\n\nPattern Analysis: All of the census tracts in Salt Lake, Morgan, and Piute counties had MOEs above 15%, and judging off of that it seems like the census had significant MOEs for the three races. However, when looking at the minimum MOE percentages, the white population’s range starts at 6.31%, and Hispanic at 17.60% and Black at 54.39%, which are both substantial jumps from the white communities. Possible explanations might be due to redlining, and from anecdotal knowledge, the diversity in Salt Lake county tends to be clustered on the west side and periphery with higher Black, Hispanic, and Asian populations. Another explanation could be that the other counties are smaller and have a much larger white population."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary: 1. Overall Pattern Identification: What are the systematic patterns across all my analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on my findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nLooking at the data without any geographic or chart visualization, it seems like many of the counties with high margins-of-error tend to be more rural, and a good number of these rural counties have as little as three census tracts compared to more urban counties that have nearly a hundred or more than two-hundred census tracts. Even counties marked with moderate confidence like Morgan, which is actually adjacent and northeast of Salt Lake county which has high confidence, have very little census tracts. When looking at the counties that have low confidence, it seems like the trend is that the confidence level has a positive relationship with income and population, and a negative relationship with margin-of-error as it rises. However, this trend seems to hold true for more densely populated counties, because Morgan county has a median income around 120,000 with moderate reliability and less than 12,000 residents, and Beaver county has a median income around 80,000 with low confidence and less than 8,000 residents. Income also has a strong relationship with race, so it can be inferred that Black and Hispanic communities may share similar relationship characteristics to the aforementioned statement, but this is in regards to more diverse, urban areas versus rural areas.\nBecause of these observations, the communities at greatest risk of algorithmic bias would be Black, Hispanic, rural, and low-income populations, and potentially other unmentioned races like Native Americans. Most of the US’ diversity is a result from immigrants landing in coastal states and Black slaves who were very condensed in the South, and over the decades that diversity has moved to more inland states; Utah in particular had an overwhelming 98% white population in the entire state around the 1970s, and as of recently a little over five decades later, Utah is overall 90% white.\nIn addition, Utah’s environment makes it particularly expensive to build in due to the desert and rocky environment that make construction difficult. It actually also makes it difficult to travel in as well, even with private vehicles due to the terrain. Much of Utah’s topography is mountainous and desert, and rural communities do not have access to broadband—this means that the digital gap could be an obstacle to rural individuals, who receive the census surveys through mail, and who may have difficult mailing routes for the US Postal Service to reach. Most rural areas or developing towns, if they do possess internet cables, tend to be older and much slow infrastructure like copper, coaxial, or even satellite. These telecommunications technologies are very outdated compared to the current fiber optic standard. So census by mail is the best way to reach rural and Native reservations, and mail is not as timely to collect because of the physical and long-distance aspects that are required.\nTaking the liberty of observing the averages of margins-of-error less than 15% in section 4.2, it seems like the lower margin-of-error occurrences are within even less diverse tracts to others, with the white percent change being +15.23%, and then -0.98% and -12.50% percent changes for Black and Hispanic percents, respectively. From anecdotal knowledge, as a born-and-raised Utahn in Salt Lake county, the valley has the picturesque Wasatch mountains to the east and more flat and dusty mountain ranges to the far west as well as the Kennecott Copper Mine. The county is also split longitudinally by the main interstate highway I-15, and due to historical redlining and more frequent efforts to uplift communities near the Wasatch mountains, many of the wealthy and white residents live on the east side of I-15 and many of Salt Lake’s diversity are clustered on the west side of I-15, namely the Black, Hispanic, and Asian communities.\nThe Department of Health and Human Services could work with the Department of Commerce, which oversees telecommunications, to develop a program that incentivizes or subsidizes telecommunications companies to build out fiber broadband networks with at least 250 to 300 Mbps. Because fiber is expensive to build out, companies can use older fiber cable technology to balance between the 500 Mbps to 1 GB commonly found in more urban regions. While rural areas receive the census by mail, the vast majority of ruralites own smartphones and do rely on their internet being reliable even if it’s not very fast. So they won’t have a lot of trouble navigating internet browsers, but filling out the census on mobile phone could be a potential hurdle, so local public facilities to them like libraries could facilitate in-person or virtual resources to fill out the census survey. The suggestion might result in a marginal change because most Americans use smartphones, but slow internet can often be a deterrent when filling out forms.\nThe margins-of-error that are higher in certain races in Utah could also be due to cultural differences, not a lot of immigrants, who make up the majority of BIPOC demographics, are able to read English well or they may be less likely to fill the census for a variety of reasons. In the 2020 ACS, only twelve languages were available to read the survey questions, and other languages outside the twelve require extra steps to receive translations; it could be worth pursuing creating a help center in public libraries as well for those who speak minority languages, but whether or not individuals come in for that public service is another thing entirely, so providing any online videos and glossaries in minority languages could be helpful, or allowing individuals to specify if they need a paper glossary with key translations mailed to them in their language."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#specific-recommendations",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\n\n\nCode\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nsummary_table &lt;- utah_reliability %&gt;%\n  summarize(\n    \"County\" = utah_reliability[[\"NAME\"]],\n    \"Median Income\" = utah_reliability[[\"median_incE\"]],\n    \"MOE Percentage\" = utah_reliability[[\"MOE_percent\"]],\n    \"Reliability Category\" = utah_reliability[[\"reliability_cat\"]]\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\nsummary_table &lt;- summary_table %&gt;%\n  mutate(\n    \"Algorithm Recommendations\" = case_when(\n      `Reliability Category` == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      `Reliability Category` == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      `Reliability Category` == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Format as a professional table with kable()\nkable(\n  summary_table,\n  digit = 2,\n  caption = \"&lt;b&gt;UTAH COUNTIES: Median Income and Reliabillity for Algorithmic Decision-Making&lt;/b&gt;\"\n) %&gt;%\n  kable_styling(latex_options = \"striped\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  row_spec(0, color = \"white\", background = \"black\")\n\n\n\nUTAH COUNTIES: Median Income and Reliabillity for Algorithmic Decision-Making\n\n\nCounty\nMedian Income\nMOE Percentage\nReliability Category\nAlgorithm Recommendations\n\n\n\n\nBeaver\n80268\n13.94\nLow Confidence\nRequires manual review or additional data\n\n\nBox Elder\n72769\n3.95\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCache\n72719\n2.77\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCarbon\n53734\n7.63\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nDaggett\n61250\n22.91\nLow Confidence\nRequires manual review or additional data\n\n\nDavis\n101285\n1.73\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDuchesne\n70821\n4.87\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEmery\n67056\n5.70\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGarfield\n56481\n13.34\nLow Confidence\nRequires manual review or additional data\n\n\nGrand\n59171\n11.41\nLow Confidence\nRequires manual review or additional data\n\n\nIron\n63005\n6.97\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nJuab\n88048\n7.68\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKane\n70327\n17.02\nLow Confidence\nRequires manual review or additional data\n\n\nMillard\n69403\n4.16\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorgan\n120854\n7.78\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nPiute\n33359\n37.11\nLow Confidence\nRequires manual review or additional data\n\n\nRich\n69250\n12.42\nLow Confidence\nRequires manual review or additional data\n\n\nSalt Lake\n90011\n1.17\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Juan\n52108\n10.34\nLow Confidence\nRequires manual review or additional data\n\n\nSanpete\n64356\n5.29\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSevier\n66972\n7.06\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSummit\n126392\n6.33\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTooele\n95545\n4.57\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUintah\n67983\n7.48\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nUtah\n91263\n1.09\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWasatch\n104855\n7.16\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nWashington\n71976\n4.74\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWayne\n64870\n25.55\nLow Confidence\nRequires manual review or additional data\n\n\nWeber\n82291\n2.15\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\n\nCounties suitable for immediate algorithmic implementation: Box Elder, Cache, Davis, Duchesne, Millard, Salt Lake, Tooele, Utah, Washington, and Weber counties have high confidence data, especially since all of these counties are either major urban regions in the state or they have a high population around 20,000 or more individuals. However, even if these counties are suitable for algorithmic implementation, it still needs to be stressed that automated decision-making should have flagging features built into them for potential review. The volume of data recorded in these counties are numerous and make any manual process in collecting and wrangling the data time-consuming, monotonous, and prone to human error. So balancing algorithmic implementation to do the heavy-lifting alongside human reviews when any flagging safeguards are breached could be crucial to maintain or further improve high confidence data.\nCounties requiring additional oversight: Carbon, Emery, Iron, Juab, Morgan, Sanpete, Sevier, Summit, Uintah, and Wasatch counties have moderate confidence data. Keeping an eye on how margins-of-error have changed over time in the census might be a worthwhile endeavor, if certain counties are gradually improving on their margins then oversight can ensure that their data reliability can transition to high confidence, and if certain counties have a history of their margins-of-error being stagnant, have higher variability year after year compared to other counties, or have been non a downward trend then it could be an option to introduce manual review in some parts of the data collection and cleaning process. Even sending out an additional survey might adjust the data confidence beneficially.\nCounties needing alternative approaches: Beaver, Daggett, Garfield, Grand, Kane, Piute, Rich, San Juan, and Wayne counties have low confidence data. These counties also happen to be in the more rural parts of Utah, which tend to have harsher desert environments and with rougher terrain when travelling, they also encompass all of the Native American reservations in Utah. These counties also have much smaller populations and population densities that could make them worth manually reviewing and sending out additional surveys to make sure people send them in. While USPS mail delivery procedures are largely the same, mailing to reservations have an additional unique challenge other than remoteness, which is that many residences don’t have address names, so making sure that rural areas and reservations receive the census by sending more surveys could be part of a solution. The Department could also introduce a pilot program to provide P.O. boxes for reservations if Indigenous peoples in the region are open to the idea, although it’ll be many long years before seeing whether or not it made substantial improvements to census data accuracy and lowering MOEs over time."
  },
  {
    "objectID": "labs/lab-1/Vu_Tessa_Assignment1.html#questions-for-further-investigation",
    "href": "labs/lab-1/Vu_Tessa_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow would observations and interpretations change if other races were included, like Asians and Native Americans?\nBecause the latest 5-year ACS for Utah had reported significant errors, how would that survey compare to previous surveys?\nHow would this look spatially visualized? A deeper dive into neighborhoods with high MOE census tracts could be beneficial.\nWhat is the margin-of-error’s relationship with population density and Utah’s topography, in addition to median income?"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Apply spatial predictive modeling techniques using the “Street Lights - One Out” 311 service request type as the predictor variable as well as other built environment variables, avoiding any demographic predictors.\nThe above 311 violation was selected over the others because this lab is focusing on trying to create the most “ethical” predictive model. The reason why a single street light out was determined as the most ethical is because it is the most balanced when it comes to the violations; all street lights being out or abandoned cars may likely skew toward underserved and disinvested areas."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#chicago-boundaries-and-built-environment",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#chicago-boundaries-and-built-environment",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "1. Chicago Boundaries and Built Environment",
    "text": "1. Chicago Boundaries and Built Environment\nCity Limits Boundaries\nNeighborhood Boundaries\nPolice District Boundaries\nMajor Roads"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#service-requests-2017",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#service-requests-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "2. 311 Service Requests (2017)",
    "text": "2. 311 Service Requests (2017)\n2017 Chicago 311 Service Requests “Street Lights - One Out” Dataset"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#crimes-2017",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#crimes-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "3. Crimes (2017)",
    "text": "3. Crimes (2017)\n2017 crimes filtered for forced entry burglaries"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#vacant-buildings-2017",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#vacant-buildings-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "4. Vacant Buildings (2017)",
    "text": "4. Vacant Buildings (2017)\n2017 Vacant Buildings Dataset"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#land-use-zoning",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#land-use-zoning",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "5. Land-Use / Zoning",
    "text": "5. Land-Use / Zoning\nLand-Use / Zoning Dataset filtered for business/commercial, downtown, residential, industrial, and parks/open space filtered by creation date up to end of 2017."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#cta-l-stations",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#cta-l-stations",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "6. CTA L Stations",
    "text": "6. CTA L Stations\nCTA L Stations Dataset"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#buildings-2017",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#buildings-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "7. Buildings (2017)",
    "text": "7. Buildings (2017)\nBuildings Dataset"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#crimes-for-temporal-validation-2018",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#crimes-for-temporal-validation-2018",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "8. Crimes for Temporal Validation (2018)",
    "text": "8. Crimes for Temporal Validation (2018)\n2018 crimes filtered for forced entry burglaries to match 2017 data"
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-i-data-loading-exploration",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-i-data-loading-exploration",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART I: Data Loading & Exploration",
    "text": "PART I: Data Loading & Exploration\n\n1. Setup\n\n\nCode\n# Load relevant packages.\nlibrary(tidyverse)      # Data manipulation.\nlibrary(dplyr)          # Data manipulation.\nlibrary(stringr)        # Data manipulation.\nlibrary(sf)             # Spatial operations.\nlibrary(here)           # Relative file paths.\nlibrary(viridis)        # Color scales.\nlibrary(RColorBrewer)    # Color palettes.\nlibrary(terra)          # Raster operations (replaces \"raster\").\nlibrary(spdep)          # Spatial dependence.\nlibrary(FNN)            # Fast nearest neighbors.\nlibrary(MASS)           # Negative binomial regression.\nlibrary(patchwork)      # Plot composition (replaces grid/gridExtra).\nlibrary(knitr)          # Tables.\nlibrary(kableExtra)     # Table formatting.\nlibrary(classInt)       # Classification intervals.\nlibrary(ggplot2)        # Plotting.\nlibrary(kableExtra)     # Professional tables.\nlibrary(car)            # Use VIF.\nlibrary(httr)           # API querying.\n\n# Spatstat split into sub-packages.\nlibrary(spatstat.geom)    # Spatial geometries.\nlibrary(spatstat.explore) # Spatial exploration/KDE.\n\n# Set options.\noptions(scipen = 999)  # No scientific notation.\nset.seed(5080)         # Reproducibility.\n\n# Create consistent themes for visualizations.\ntheme_chi_map &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.title = element_text(face = \"bold\",\n                                size = base_size + 1,\n                                hjust = 0.5\n                                ),\n      plot.subtitle = element_text(face = \"italic\",\n                                   color = \"gray30\",\n                                   size = base_size - 1,\n                                   hjust = 0.5\n                                   ),\n      legend.position = \"bottom\",\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      axis.text = element_blank(),\n      axis.title = element_blank(),\n      legend.title = element_text(face = \"italic\",\n                                  size = base_size - 1,\n                                  hjust = 0.5\n                                  ),\n      legend.title.position = \"top\",\n      legend.text = element_text(face = \"italic\",\n                                 size = base_size - 3,\n                                 hjust = 0.5\n                                 ),\n      legend.key.width = unit(2, \"cm\"),\n      legend.key.height = unit(0.5, \"cm\")\n    )\n}\n\n\n\n\n2. Load Chicago Spatial Data\n\n\nCode\n# Load Chicago's boundaries.\nchicago_boundary &lt;-\n  st_read(\"data/chicago_boundary.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load Chicago's police districts.\npolice_districts &lt;-\n  st_read(\"data/police_districts.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load Chicago's neighborhood boundaries.\nneighborhoods &lt;-\n  st_read(\"data/neighborhoods.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load Chicago's major streets.\nstreets &lt;-\n  st_read(\"data/major_streets/major_streets.shp\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load building data.\nbuildings &lt;- st_read(\"data/buildings_2017.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n\n\n\n3. Load Burglary Data\n\n\nCode\n# Load 2017 burglary data.\nburglaries &lt;-\n  st_read(\"data/burglaries_2017.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load 2018 burglary data.\nburglaries_2018 &lt;-\n  st_read(\"data/burglaries_2018.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n\n\n\n4. Load 311 Data\n\n\nCode\n# Load 2017 311 data for single street light outage.\nlight_outage &lt;-\n  st_read(\"data/single_light_outage_2017.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n\n\n\n5. Load Ancillary / Contextual Data\n\n\nCode\n# Load vacant buildings data.\nvacant &lt;-\n  st_read(\"data/vacant_buildings.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load CTA L station data.\nstation &lt;-\n  st_read(\"data/L_stations.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n# Load zoning data.\nzoning &lt;-\n  st_read(\"data/zoning.geojson\", quiet = TRUE) %&gt;%\n  st_transform(\"ESRI:102271\")\n\n\n\ni. Clean Zoning Data\n\n\nCode\n# Clean zoning data.\n# Put different zoning sub-classes into one overall class.\nzoning &lt;- zoning %&gt;%\n  mutate(type = case_when(\n    # Downtown\n    str_starts(zone_class, \"D\") ~ \"Downtown\",\n    \n    # Residential\n    str_starts(zone_class, \"R\")  ~ \"Residential\",\n    \n    # Business/Commercial\n    str_starts(zone_class, \"B|C\") ~ \"Business/Commercial\",\n    \n    # Industrial\n    str_starts(zone_class, \"M\") ~ \"Industrial\",\n    \n    # Park/Open Space\n    str_starts(zone_class, \"POS\") ~ \"Park/Open Space\"\n  )\n)\n\n\n\n\nii. Clean Major Streets Data\n\n\nCode\n# Clean major streets data.\n# Merge all streets into one vector.\nstreets &lt;- st_union(streets)\n\n\n\n\niii. Clean Building Data\n\n\nCode\n# Clean building data.\nbuildings &lt;- buildings %&gt;%\n  mutate(\n    # Make sure the years are numeric.\n    year_built = as.numeric(year_built),\n    \n    # Calculate Age for 2017.\n    building_age = 2017 - year_built,\n  )\n\n\nIt is to be noted that this lab also uses abandoned / vacant buildings as an additional contextual predictor, however, a clear distinction must be made that the dataset used is not the 311 unconfirmed complaints of vacant buildings that may be driven by emotional biases. Instead, the predictor is confirmed and issued vacant building violations. Other ancillary data include zoning, major roads, CTA’s L stations, as well as buildings to derive spatial features from. Zoning types may encourage different types of behavior like residential vs. business/commercial, major roads and CTA’s L stations create dense urban flow in some areas and provide those engaging in criminal behavior access to different areas, building density measures the urbanity of the areas and the higher the density the larger the number of potential targets, building age and its U-shape can attract burglars especially since its value alters.\n\n\n\n6. Visualize Data\n\n\n\nChicago Neighborhoods (From Wikipedia)\n\n\n\n\nCode\n# Burglaries.\n# Point density map.\nburglary_point_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_sf(data = burglaries,\n          color = \"#ff4100\",\n          size = 0.1,\n          alpha = 0.5\n          ) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Point Density | n = \", prettyNum(nrow(burglaries), big.mark = \",\"))\n    ) +\n  theme_chi_map()\n\n# Kernel density map.\nburglary_kernel_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.6,\n    bins = 8\n    ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Surface Density\",\n    subtitle = \"Kernel Density | bins = 8\"\n    ) +\n  theme_chi_map()\n\n# Hexbin map.\nburglary_hexbin &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_hex(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    binwidth = c(1000, 1000),\n    bins = 30,\n    alpha = 0.6\n    ) +\n  scale_fill_viridis_c(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Hexbin Density\",\n    subtitle = \"Hexagonal Binning | bins = 30\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nburglary_point_density + burglary_hexbin + burglary_kernel_density +\n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries\",\n    subtitle = \"Neighborhoods in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Street lights.\n# Point density map.\nlight_point_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_sf(data = light_outage,\n          color = \"#ff4100\",\n          size = 0.1,\n          alpha = 0.5\n          ) +\n  labs(\n    title = \"Single Street Light Outage Locations\",\n    subtitle = paste0(\"Point Density | n = \", prettyNum(nrow(light_outage), big.mark = \",\"))\n    ) +\n  theme_chi_map()\n\n# Kernel density map.\nlight_kernel_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(light_outage)),\n    aes(X, Y),\n    alpha = 0.6,\n    bins = 8\n    ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Surface Density\",\n    subtitle = \"Kernel Density | bins = 8\"\n    ) +\n  theme_chi_map()\n\n# Hexbin map.\nlight_hexbin &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_hex(\n    data = data.frame(st_coordinates(light_outage)),\n    aes(X, Y),\n    binwidth = c(1000, 1000), # Set proportionally equal hexagons.\n    bins = 30,\n    alpha = 0.6\n    ) +\n  scale_fill_viridis_c(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Hexbin Density\",\n    subtitle = \"Hexagonal Binning | bins = 30\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nlight_point_density + light_hexbin + light_kernel_density +\n  plot_annotation(\n    title = \"Spatial Distribution of 311 Requests: Single Street Light Outage\",\n    subtitle = \"Neighborhoods in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Vacancies.\n# Point density map.\nvacant_point_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_sf(data = vacant,\n          color = \"#ff4100\",\n          size = 0.1,\n          alpha = 0.5\n          ) +\n  labs(\n    title = \"Vacant Building Locations\",\n    subtitle = paste0(\"Point Density | n = \", prettyNum(nrow(vacant), big.mark = \",\"))\n    ) +\n  theme_chi_map()\n\n# Kernel density map.\nvacant_kernel_density &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(vacant)),\n    aes(X, Y),\n    alpha = 0.6,\n    bins = 8\n    ) +\n  scale_fill_viridis_d(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Surface Density\",\n    subtitle = \"Kernel Density | bins = 8\"\n    ) +\n  theme_chi_map()\n\n# Hexbin map.\nvacant_hexbin &lt;- ggplot() + \n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = \"gray30\"\n          ) +\n  geom_hex(\n    data = data.frame(st_coordinates(vacant)),\n    aes(X, Y),\n    binwidth = c(1000, 1000), # Set proportionally equal hexagons.\n    bins = 30,\n    alpha = 0.6\n    ) +\n  scale_fill_viridis_c(\n    option = \"rocket\",\n    direction = -1,\n    guide = \"none\"\n    ) +\n  labs(\n    title = \"Hexbin Density\",\n    subtitle = \"Hexagonal Binning | bins = 30\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nvacant_point_density + vacant_hexbin + vacant_kernel_density +\n  plot_annotation(\n    title = \"Spatial Distribution of Vacant Buildings\",\n    subtitle = \"Neighborhoods in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Major streets with neighborhoods.\nneighborhood_streets_map &lt;- ggplot() +\n  geom_sf(data = neighborhoods,\n          fill = \"gray95\",\n          color = NA\n          ) +\n  geom_sf(data = streets,\n          color = \"#F03E36\",\n          linewidth = 0.4\n          ) +\n  geom_sf(data = neighborhoods,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  labs(title = \"With Neighborhoods\") +\n  theme_chi_map()\n\n# Major streets with police districts.\ndistrict_streets_map &lt;- ggplot() +\n  geom_sf(data = police_districts,\n          fill = \"gray95\",\n          color = NA\n          ) +\n  geom_sf(data = streets,\n          color = \"#F03E36\",\n          linewidth = 0.4\n          ) +\n  geom_sf(data = police_districts,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  labs(title = \"With Police Districts\") +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nneighborhood_streets_map + district_streets_map +\n  plot_annotation(\n    title = \"Major Streets Network\",\n    subtitle = \"Chicago, Illinois\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\n7. Section Analysis\nPart I goes over the initial setup for the entirety of the lab as well as the data cleaning process that involved categorizing the land-use and creating new variables like building age and unifying the street vector data, the latter of which was at first overlooked until it came time to spatially engineer the distances to major roads—computationally less heavy and more efficient to compute distances to a single vector rather than thousands of line segments across Chicago.\nWhen it came to zoning, land-use often has multiple sub-classes and varies by city, with Chicago possessing over fifty sub-classes, which were then categorized into a broader level that includes downtown, residential, industrial, parks, and businesses.\nThe EDA portion in this section is especially important, providing a mid-depth understanding of the spatial distribution of different variables.\nBurglaries (7,509) had a lot of density in Chicago’s central-north (regionally known as West Side), central-south (regionally known as Southwest Side), and southeastern (regionally known as South Side) areas. The neighborhoods at the darkest cores of the maps are West Town, Chicago Lawn, West Englewood, Woodlawn, Greater Grand Crossing, and South Shore.\nWhen taking a look at the single street light outage’s (75,084) spatial distribution, it confirms the assumption that this specific 311 complaint would be the most “ethical” as the densities span a wider portion of Chicago.\nVacant buildings (5,012) are more sparse with a lot of density in West Side’s Humboldt Park and Southwest Side’s New City, West Englewood, and Chicago Lawn.\nMajor streets are mapped alongside one another with the neighborhood and police district boundaries overlaid. It’s pretty clear from the street networks that they line up with CTA’s transit lines and also show where the urban core is in Central’s Loop neighborhood boundary."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-ii-fishnet-grid-creation",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-ii-fishnet-grid-creation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART II: Fishnet Grid Creation",
    "text": "PART II: Fishnet Grid Creation\n\n1. Fishnet Grid Cells\n\n\nCode\n# Burglaries.\n# Create 500m x 500m grid.\nfishnet &lt;- st_make_grid(\n  neighborhoods,\n  cellsize = 500,\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago.\nfishnet &lt;- fishnet[neighborhoods, ]\n\n\n\n\n2. Aggregate Datasets\n\n\nCode\n# Spatial join.\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count_burglaries = n())\n\n# Join back to fishnet (cells with 0 requests will be NA).\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(count_burglaries = replace_na(count_burglaries, 0))\n\n# Spatial join.\nburglaries_2018_fishnet &lt;- st_join(burglaries_2018, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count_burglaries_2018 = n())\n\n# Join back to fishnet (cells with 0 requests will be NA).\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_2018_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(count_burglaries_2018 = replace_na(count_burglaries_2018, 0))\n\n\n\n\nCode\n# Spatial join.\nlight_fishnet &lt;- st_join(light_outage, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count_outages = n())\n\n# Join back to fishnet (cells with 0 requests will be NA).\nfishnet &lt;- fishnet %&gt;%\n  left_join(light_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(count_outages = replace_na(count_outages, 0))\n\n\n\n\nCode\n# Aggregate vacant buildings.\n# Spatial join.\nvacant_fishnet &lt;- st_join(vacant, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count_vacant = n())\n\n# Join back to fishnet (cells with 0 requests will be NA).\nfishnet &lt;- fishnet %&gt;%\n  left_join(vacant_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(count_vacant = replace_na(count_vacant, 0))\n\n\n\n\nCode\n# Aggregate CTA L stations.\n# Spatial join.\nstation_fishnet &lt;- st_join(station, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(count_station = n())\n\n# Join back to fishnet (cells with 0 requests will be NA).\nfishnet &lt;- fishnet %&gt;%\n  left_join(station_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(count_station = replace_na(count_station, 0))\n\n\n\n\nCode\n# Calculate zoning percentages.\n# Get the area of each fishnet cell.\nfishnet_zoning_area &lt;- fishnet %&gt;%\n  mutate(cell_area = st_area(.)) %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(uniqueID, cell_area) # Conflict with another library's select function.\n\n# Intersect zoning and fishnet, calculate area, and group by the zoning \"type\".\nzoning_intersect &lt;- st_intersection(fishnet, zoning) %&gt;%\n  mutate(intersect_area = st_area(.)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID, type) %&gt;%\n  summarize(total_intersect_area = sum(intersect_area), .groups = \"drop\") %&gt;%\n  left_join(fishnet_zoning_area, by = \"uniqueID\") %&gt;%\n  mutate(percent_area_zoning = as.numeric(total_intersect_area / cell_area))\n\n# Pivot to get one row per grid cell and one column per \"type\".\nzoning_features &lt;- zoning_intersect %&gt;%\n  pivot_wider(\n    id_cols = uniqueID,\n    names_from = type,\n    values_from = percent_area_zoning,\n    values_fill = 0\n  )\n\n# Rename features because the slashes will error the regression.\nzoning_features &lt;- zoning_features %&gt;%\n  rename(\n    pct_Residential = Residential,\n    pct_Business_Commercial = `Business/Commercial`,\n    pct_Park_Open_Space = `Park/Open Space`,\n    pct_Industrial = Industrial,\n    pct_Downtown = Downtown\n  )\n\n# Join back to fishnet and replace all NAs with 0.\nfishnet &lt;- fishnet %&gt;%\n  left_join(zoning_features, by = \"uniqueID\") %&gt;%\n  mutate(across(starts_with(\"pct_\"), ~replace_na(., 0)))\n\n\n\n\nCode\n# Calculate distance from cell to streets.\ndist_matrix &lt;- st_distance(fishnet, streets)\n\n# Join to fishnet.\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    dist_to_streets = as.numeric(dist_matrix[, 1])\n  )\n\n\n\n\nCode\n# Calculate building percentages.\n# Get the area of each fishnet cell.\nfishnet_building_area &lt;- fishnet %&gt;%\n  mutate(cell_area = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(uniqueID, cell_area) # Conflict with another library's select function.\n\n# Intersect buildings and fishnet and calculate area.\nbuilding_features &lt;- st_intersection(fishnet, buildings) %&gt;%\n  mutate(intersect_area = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(\n    avg_building_age = mean(building_age),\n    total_building_area = sum(intersect_area),\n    avg_building_area = mean(intersect_area),\n    .groups = \"drop\"\n    )\n\n# Join back to fishnet and replace all NAs with 0.\nfishnet &lt;- fishnet %&gt;%\n  left_join(building_features, by = \"uniqueID\") %&gt;%\n  left_join(fishnet_building_area, by = \"uniqueID\") %&gt;%\n  mutate(\n    building_density = total_building_area / cell_area,\n    avg_building_age = replace_na(avg_building_age, 0),\n    building_density = replace_na(building_density, 0),\n    avg_building_area = replace_na(avg_building_area, 0)\n  )\n\n\n\n\n3. Visualize Spatial Distributions\n\n\nCode\n# Burglaries.\n# Visualize aggregated counts.\nburglary_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = count_burglaries),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\",\n    breaks = c(0, 1, 2, 5, 10, 20, 35)\n    ) +\n  labs(\n    title = \"Burglary Counts\"\n    ) +\n  theme_chi_map()\n\n# Street lights.\n# Visualize aggregated counts.\nlight_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = count_outages),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Outages\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\",\n    breaks = c(0, 5, 20, 50, 100, 160, 255)\n    ) +\n  labs(\n    title = \"Single Street Light Outages\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nburglary_fishnet_map + light_fishnet_map +\n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries and 311 Requests\",\n    subtitle = \"500m x 500m Fishnet Grid Cells in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Vacancies.\n# Visualize aggregated counts.\nvacant_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = count_vacant),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Vacancies\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\",\n    breaks = c(0, 1, 5, 10, 25, 50, 83)\n    ) +\n  labs(\n    title = \"Vacancy Counts\"\n    ) +\n  theme_chi_map()\n\n# Stations.\n# Visualize aggregated counts.\nstation_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = count_station),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Stations\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\"\n    ) +\n  labs(\n    title = \"CTA L Station Counts\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nvacant_fishnet_map + station_fishnet_map +\n  plot_annotation(\n    title = \"Spatial Distribution of Vacant Buildings and CTA L Stations\",\n    subtitle = \"500m x 500m Fishnet Grid Cells in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Buildings.\n# Visualize average age.\nbuilding_age_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = avg_building_age),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Average Building Age (Years)\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\",\n    breaks = c(0, 25, 50, 75, 100, 125),\n    labels = function(x) paste0(x, \"yrs\")\n    ) +\n  labs(\n    title = \"Average Building Age\"\n    ) +\n  theme_chi_map()\n\n# Visualize density.\nbuilding_density_fishnet_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = building_density),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"Building Density (Percent %)\",\n    option = \"rocket\",\n    direction = -1,\n    trans = \"sqrt\",\n    breaks = c(0, 0.05, 0.10, 0.20, 0.30, 0.50),\n    labels = scales::percent_format(accuracy = 1) # Conflict with another package.\n    ) +\n  labs(\n    title = \"Building Density\"\n    ) +\n  theme_chi_map()\n\n# Combine plots using patchwork.\nbuilding_age_fishnet_map + building_density_fishnet_map +\n  plot_annotation(\n    title = \"Spatial Distribution of Building Age and Density\",\n    subtitle = \"500m x 500m Fishnet Grid Cells in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create individual maps for each zoning type.\nresidential_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = pct_Residential),\n          color = NA) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75) +\n  scale_fill_viridis_c(\n    name = \"Percent (%)\",\n    option = \"rocket\",\n    direction = -1,\n    labels = scales::percent,\n    limits = c(0, 1)\n  ) +\n  labs(title = \"Residential\") +\n  theme_chi_map()\n\nbiz_com_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = pct_Business_Commercial),\n          color = NA) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75) +\n  scale_fill_viridis_c(\n    name = \"Percent (%)\",\n    option = \"rocket\",\n    direction = -1,\n    labels = scales::percent,\n    limits = c(0, 1)\n  ) +\n  labs(title = \"Business/Commercial\") +\n  theme_chi_map()\n\npark_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = pct_Park_Open_Space),\n          color = NA) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75) +\n  scale_fill_viridis_c(\n    name = \"Percent (%)\",\n    option = \"rocket\",\n    direction = -1,\n    labels = scales::percent,\n    limits = c(0, 1)\n  ) +\n  labs(title = \"Park/Open Space\") +\n  theme_chi_map()\n\nindustrial_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = pct_Industrial),\n          color = NA) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75) +\n  scale_fill_viridis_c(\n    name = \"Percent (%)\",\n    option = \"rocket\",\n    direction = -1,\n    labels = scales::percent,\n    limits = c(0, 1)\n  ) +\n  labs(title = \"Industrial\") +\n  theme_chi_map()\n\ndowntown_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = pct_Downtown),\n          color = NA) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75) +\n  scale_fill_viridis_c(\n    name = \"Percent (%)\",\n    option = \"rocket\",\n    direction = -1,\n    labels = scales::percent,\n    limits = c(0, 1)\n  ) +\n  labs(title = \"Downtown\") +\n  theme_chi_map()\n\n# Combine with patchwork.\ncombined_map &lt;- (residential_map + biz_com_map + park_map) /\n                (industrial_map + downtown_map) +\n  plot_annotation(\n    title = \"Zoning Spatial Distribution by Type\",\n    subtitle = \"500m x 500m Fishnet Grid Cells in Chicago, Illinois (2017)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    ) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\nCode\ncombined_map\n\n\n\n\n\n\n\n\n\n\n\n4. Kernel Density Estimation (KDE) Baseline\n\n\nCode\n# Convert burglaries to ppp (point pattern) format for spatstat.\nburglaries_ppp &lt;- as.ppp(\n  st_coordinates(burglaries),\n  W = as.owin(st_bbox(chicago_boundary))\n)\n\n# Calculate KDE with 1km bandwidth.\nkde_burglaries &lt;- density.ppp(\n  burglaries_ppp,\n  sigma = 1000,  # 1km bandwidth.\n  edge = TRUE    # Edge correction.\n)\n\n# Convert to terra raster (modern approach, not raster::raster).\nkde_raster &lt;- rast(kde_burglaries)\n\n# Extract KDE values to fishnet cells.\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    kde_value_burglaries = terra::extract(\n      kde_raster,\n      vect(fishnet),\n      fun = mean,\n      na.rm = TRUE\n    )[, 2]  # Extract just the values column.\n  )\n\n\n\n\nCode\n# Burglaries.\nburglary_kde_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = kde_value_burglaries),\n          color = NA\n          ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"#21918c\",\n          linewidth = 0.75\n          ) +\n  scale_fill_viridis_c(\n    name = \"KDE Value (Burglaries per m²)\",\n    option = \"rocket\",\n    direction = -1,\n    breaks = c(0, 0.00001, 0.00002, 0.00003, 0.000035),\n    labels = scales::label_number(accuracy = 0.000001) # Package conflict.\n    ) +\n  labs(\n    title = \"Kernel Density Estimation (KDE): Burglary Locations\",\n    subtitle = \"Simple Spatial Smoothing | Chicago, Illinois (2017)\"\n    ) +\n  theme_chi_map()\n\nburglary_kde_map\n\n\n\n\n\n\n\n\n\n\n\n5. Section Analysis\nPart II deals with the 500 meter by 500 meter fishnet grid creation to get a more uniform look at Chicago as opposed to block groups or census tracts, which have different areas and also very different urban and population characteristics.\nThis makes raw counts like burglaries comparable to one another as opposed to comparing them over wildly different areas (e.g. 5 burglaries in the state of Utah vs. 5 burglaries in a city block, although rarely ever that intense) as with the well-known Modifiable Areal Unit Problem (MAUP).\nBurglary and light outage counts were created in this section along with calculating the distance from each cell to the nearest major street line, now unified from the previous section to easily compute it.\nThe zoning portion was significantly more involved conceptually and technically, the process calculated how much each zoning type intersected with a cell. It was determined this would be better than counting, because a miniscule industrial corner being counted as 1 unit for a cell wasn’t as conceptually sound as creating a percentage depending on the polygon overlap.\nThe fishnet spatial distributions for burglaries, outages, and vacancies are largely similar to the previous section’s maps when looking at the overall density, however, the fishnet maps are more coarse due to the resolution that was set.\nCTA’s L stations will be single dots of course due to the nature of the dataset, but it does overlay with the major streets spatially.\nLooking at the building age, it’s clear that Chicago is a very old city. The central Loop looks to have some spots of newer buildings likely due to the fact it’s the urban core with business hubs that may have more demand for new infill buildings.\nThen there’s a subtle but noticeable dark ring that surrounds the central Loop with buildings generally over 100 years old, and even further out toward the periphery are rings of buildings around perhaps 40 to 75 years old. Building density also spatially follows the same pattern being from the same dataset, but it does look like density diminishes going farther west and south of the center.\nThe very light spot in the south is due to Big Marsh Park, which has a lot of greenery and a lake. Then there’s the bright spot in the northwest, which is O’Hare International Airport.\nLots of residential zoning outside the downtown zoning, and the business/commercial zones spatially overlap with the residential one. Note that there are indeed business/commercial and residential zoning in the city center, but that was alloted to the downtown zoning. Parks are spotted throughout Chicago. Industrial zones are more on the peripheral or spotty going inland.\nThe KDE burglary map shows the value for burglaries per meter squared and has a similar spatial pattern in Part I’s burglary EDA visuals."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-iii-spatial-features",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-iii-spatial-features",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART III: Spatial Features",
    "text": "PART III: Spatial Features\n\n1. Nearest Neighbor: CTA L Stations\n\n\nCode\n# Calculate mean distance to 3 nearest CTA L stations.\n\n# Get coordinates.\nfishnet_station_coords &lt;- st_coordinates(st_centroid(fishnet))\nstation_coords &lt;- st_coordinates(station) %&gt;%\n  na.omit(station_coords)\n\n# Calculate k nearest neighbors and distances.\nnn_station_result &lt;- get.knnx(station_coords, fishnet_station_coords, k = 3)\n\n# Add to fishnet.\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    nn_station = rowMeans(nn_station_result$nn.dist)\n  )\n\n\n\n\n2. Hotspot Distance: Single Street Light Outages\n\n\nCode\n# Function to calculate Local Moran's I.\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights.\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I.\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters.\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n\n\n\nCode\n# Apply to single street light outages.\nfishnet &lt;- calculate_local_morans(fishnet, \"count_outages\", k = 5)\n\nfishnet &lt;- fishnet %&gt;%\n  rename(local_i_outages = local_i,\n         p_value_outages = p_value,\n         moran_outages = moran_class,\n         is_significant_outages = is_significant\n         )\n\n\n\n\nCode\n# Street lights.\n# Visualize hot spots.\nlight_lisa_map &lt;- ggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_outages), \n    color = NA\n    ) +\n  geom_sf(data = chicago_boundary,\n          fill = NA,\n          color = \"gray60\",\n          linewidth = 0.75\n          ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n      ),\n    name = \"Cluster Type\"\n    ) +\n  labs(\n    title = \"Moran's I: Single Street Light Outage\",\n    ) +\n  theme_chi_map() +\n  theme(\n    legend.position = \"right\",\n    legend.text = element_text(hjust = 0)\n    )\n\nlight_lisa_map\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Street lights.\n# Get centroids of \"High-High\" cells (hot spots).\noutage_hotspots &lt;- fishnet %&gt;%\n  filter(moran_outages == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot.\nif (nrow(outage_hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot_outages = as.numeric(\n        st_distance(st_centroid(fishnet), outage_hotspots %&gt;% st_union())\n      )\n    )\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot_outages = 0)\n  cat(\"\\u26A0 No significant hot spots found.\\n\")\n}\n\n\n\n\n3. Join Block Groups for Cross-Validation\n\n\nCode\n# Join district information to fishnet.\nfishnet &lt;- st_join(\n  fishnet,\n  police_districts,\n  join = st_within,\n  left = TRUE\n) %&gt;%\n  filter(!is.na(dist_label)) # Remove cells outside districts.\n\n# Join neighborhood information to fishnet.\nfishnet &lt;- st_join(\n  fishnet,\n  neighborhoods,\n  join = st_within,\n  left = TRUE\n) %&gt;%\n  filter(!is.na(community)) # Remove cells outside neighborhoods\n\n\n\n\n4. Section Analysis\nPart III is where the spatial features were engineered to produce predictor variables for the model. The spatial predictors are more sophisticated and complex in comparison to using counts of urban data. K-Nearest Neighbor feature was created from the stations with k = 3 for a good balance in a large city with a more robust and expansive public transit system compared to others. This is much more reflective of everyday life than distance to the nearest station as different stations may have differing routes for individuals.\nHotspot distances were also calculated from the outages, this is also more reflective of everyday life because individuals may feel more unsafe in areas with more light outages, and burglars may find more opportunity in areas with more outages.\nThe LISA map shows that there are only high-high and low-low clusters, and low-high outliers, but not high-low outliers. It looks like the hot clusters and sprinkled outliers are sprawled across Northwest Side, West Side, South Side, and Southwest Side. Cold clusters are in Far Southeast Side, in Big Marsh Park, and Far North Side, specifically the airport.\nThis section also saw to it that the neighborhoods and police districts were joined into the fishnet variable. The reasoning is explained in Part V-3."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-iv-count-regression-models",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-iv-count-regression-models",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART IV: Count Regression Models",
    "text": "PART IV: Count Regression Models\n\n1. Poisson Regression\n\n\nCode\n# Create clean modeling dataset.\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    dist_label,\n    community,\n    count_burglaries,\n    count_vacant,\n    dist_to_hotspot_outages,\n    nn_station,\n    pct_Business_Commercial,\n    pct_Downtown,\n    pct_Industrial,\n    pct_Park_Open_Space,\n    pct_Residential,\n    dist_to_streets,\n    avg_building_age,\n    building_density\n  ) %&gt;%\n  na.omit() # Remove any remaining NAs.\n\n\n\n\nCode\n# Create summary statistics.\nsummary_stats &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    count_burglaries, \n    dist_to_hotspot_outages, \n    count_vacant, \n    nn_station, \n    dist_to_streets, \n    building_density, \n    avg_building_age, \n    pct_Business_Commercial, \n    pct_Downtown, \n    pct_Industrial, \n    pct_Park_Open_Space\n  ) %&gt;%\n  summarise(across(everything(), \n                   list(\n                     Min = ~min(., na.rm = TRUE),\n                     Q1 = ~quantile(., 0.25, na.rm = TRUE),\n                     Median = ~median(., na.rm = TRUE),\n                     Mean = ~mean(., na.rm = TRUE),\n                     Q3 = ~quantile(., 0.75, na.rm = TRUE),\n                     Max = ~max(., na.rm = TRUE)\n                   ))) %&gt;%\n  pivot_longer(\n    everything(), \n    names_to = c(\"Variable\", \"Statistic\"),\n    names_pattern = \"(.*)_(Min|Q1|Median|Mean|Q3|Max)$\" # Make the pattern permanent, or else it'll mix up the order.\n  ) %&gt;%\n  pivot_wider(names_from = Statistic, values_from = value)\n\n# Rename to make the table more neat and professional. Mark percentages for nicer value labeling.\nsummary_stats &lt;- summary_stats %&gt;%\n  mutate(\n    # Flag percent variables to format the values later.\n    is_percent = grepl(\"^pct_\", Variable) | Variable == \"building_density\",\n    Variable = case_when(\n      Variable == \"count_burglaries\" ~ \"Burglary Count\",\n      Variable == \"dist_to_hotspot_outages\" ~ \"Distance to Outage Hotspots (Meters)\",\n      Variable == \"count_vacant\" ~ \"Vacancy Count\",\n      Variable == \"nn_station\" ~ \"Distance to Nearest Station (Meters)\",\n      Variable == \"dist_to_streets\" ~ \"Distance to Major Street (Meters)\",\n      Variable == \"building_density\" ~ \"Building Density (Percent %)\",\n      Variable == \"avg_building_age\" ~ \"Average Building Age\",\n      Variable == \"pct_Business_Commercial\" ~ \"% Business/Commercial\",\n      Variable == \"pct_Downtown\" ~ \"% Downtown\",\n      Variable == \"pct_Industrial\" ~ \"% Industrial\",\n      Variable == \"pct_Park_Open_Space\" ~ \"% Park/Open Space\",\n      TRUE ~ Variable\n    )\n  )\n\n# Format the values in the table.\nsummary_stats &lt;- summary_stats %&gt;%\n  mutate(\n    is_distance = grepl(\"Distance\", Variable),  # Flag distance variables\n    across(c(Min, Q1, Median, Mean, Q3, Max), \n           ~case_when(\n             is_percent ~ paste0(format(round(. * 100, 2), big.mark = \",\"), \"%\"),\n             is_distance ~ paste0(format(round(., 2), big.mark = \",\"), \"m\"),\n             TRUE ~ format(round(., 2), big.mark = \",\")\n           ))\n  ) %&gt;%\n  dplyr::select(-is_percent, -is_distance) # Remove the flag columns so they don't show up in the Kable.\n\n# Create Kable.\nsummary_stats %&gt;%\n  mutate(Variable = cell_spec(Variable, bold = TRUE)) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    caption = \"Summary Statistics of Final Variables\",\n    col.names = c(\"Variable\", \"Minimum\", \"1st Quartile\", \"Median\", \"Mean\", \"3rd Quartile\", \"Maximum\"),\n    align = c(\"l\", rep(\"c\", 6)) # Left-align left column, repeat center-align for next 6 columns.\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), # Make the large Kable visually compact.\n    full_width = FALSE,\n    position = \"center\"\n  ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\")\n\n\n\nSummary Statistics of Final Variables\n\n\nVariable\nMinimum\n1st Quartile\nMedian\nMean\n3rd Quartile\nMaximum\n\n\n\n\nBurglary Count\n0.00\n0.00\n2.00\n3.41\n5.00\n34.00\n\n\nDistance to Outage Hotspots (Meters)\n0.00m\n500.00m\n1,414.21m\n2,322.97m\n3,041.38m\n13,124.40m\n\n\nVacancy Count\n0.00\n0.00\n0.00\n2.33\n1.00\n68.00\n\n\nDistance to Nearest Station (Meters)\n147.21m\n1,339.27m\n2,591.07m\n3,349.37m\n4,702.88m\n12,296.05m\n\n\nDistance to Major Street (Meters)\n0.00m\n0.00m\n0.00m\n61.42m\n0.00m\n2,264.87m\n\n\nBuilding Density (Percent %)\n0.0%\n6.89%\n14.43%\n12.94%\n18.94%\n56.03%\n\n\nAverage Building Age\n0.00\n62.62\n82.97\n73.48\n100.75\n127.74\n\n\n% Business/Commercial\n0.0%\n0.00%\n3.24%\n8.26%\n14.81%\n50.89%\n\n\n% Downtown\n0.0%\n0.00%\n0.00%\n0.79%\n0.00%\n77.77%\n\n\n% Industrial\n0.0%\n0.00%\n0.00%\n4.27%\n0.60%\n100.00%\n\n\n% Park/Open Space\n0.0%\n0.00%\n0.00%\n5.58%\n2.04%\n100.00%\n\n\n\n\n\n\n\nCode\n# Fit Poisson regression.\nmodel_poisson &lt;- glm(count_burglaries ~\n                       # 311 data.\n                       dist_to_hotspot_outages +\n                       # Contextual count data.\n                       count_vacant +\n                       # Contextual spatial features.\n                       nn_station +  dist_to_streets + building_density +\n                       # Age polynomial.\n                       avg_building_age + I(avg_building_age^2) +\n                       # Zoning. Leave residential out as reference.\n                       pct_Business_Commercial + pct_Downtown + pct_Industrial + pct_Park_Open_Space,\n                     data = fishnet_model,\n                     family = \"poisson\"\n                     )\n\n# Summary.\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = count_burglaries ~ dist_to_hotspot_outages + count_vacant + \n    nn_station + dist_to_streets + building_density + avg_building_age + \n    I(avg_building_age^2) + pct_Business_Commercial + pct_Downtown + \n    pct_Industrial + pct_Park_Open_Space, family = \"poisson\", \n    data = fishnet_model)\n\nCoefficients:\n                            Estimate   Std. Error z value             Pr(&gt;|z|)\n(Intercept)             -3.073808634  0.333799112  -9.209 &lt; 0.0000000000000002\ndist_to_hotspot_outages -0.000180071  0.000013223 -13.618 &lt; 0.0000000000000002\ncount_vacant             0.008573989  0.001454178   5.896     0.00000000372181\nnn_station              -0.000002367  0.000009780  -0.242             0.808775\ndist_to_streets         -0.001445527  0.000647913  -2.231             0.025678\nbuilding_density         2.728651596  0.289326230   9.431 &lt; 0.0000000000000002\navg_building_age         0.087348829  0.007560226  11.554 &lt; 0.0000000000000002\nI(avg_building_age^2)   -0.000437146  0.000042510 -10.283 &lt; 0.0000000000000002\npct_Business_Commercial  1.039951228  0.146905295   7.079     0.00000000000145\npct_Downtown             0.725126200  0.182070394   3.983     0.00006814549675\npct_Industrial          -0.973508683  0.192591788  -5.055     0.00000043089228\npct_Park_Open_Space     -0.754484807  0.204272772  -3.694             0.000221\n                           \n(Intercept)             ***\ndist_to_hotspot_outages ***\ncount_vacant            ***\nnn_station                 \ndist_to_streets         *  \nbuilding_density        ***\navg_building_age        ***\nI(avg_building_age^2)   ***\npct_Business_Commercial ***\npct_Downtown            ***\npct_Industrial          ***\npct_Park_Open_Space     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5617.1  on 1276  degrees of freedom\nResidual deviance: 2925.3  on 1265  degrees of freedom\nAIC: 5772.5\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n2. Check for Overdispersion\n\n\nCode\n# Calculate dispersion parameter.\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\n\n\n\n3. Negative Binomial Distribution\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(count_burglaries ~\n                     # 311 data.\n                     dist_to_hotspot_outages +\n                     # Contextual count data.\n                     count_vacant +\n                     # Contextual spatial features.\n                     dist_to_streets + building_density +\n                     # Age polynomial.\n                     avg_building_age + I(avg_building_age^2) +\n                     # Zoning. Leave residential out as reference.\n                     pct_Business_Commercial + pct_Downtown + pct_Industrial + pct_Park_Open_Space,\n                     data = fishnet_model,\n                   )\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = count_burglaries ~ dist_to_hotspot_outages + \n    count_vacant + dist_to_streets + building_density + avg_building_age + \n    I(avg_building_age^2) + pct_Business_Commercial + pct_Downtown + \n    pct_Industrial + pct_Park_Open_Space, data = fishnet_model, \n    init.theta = 2.252719149, link = log)\n\nCoefficients:\n                           Estimate  Std. Error z value             Pr(&gt;|z|)\n(Intercept)             -2.93989971  0.40976071  -7.175  0.00000000000072479\ndist_to_hotspot_outages -0.00016227  0.00001978  -8.205  0.00000000000000023\ncount_vacant             0.01088869  0.00305940   3.559             0.000372\ndist_to_streets         -0.00191060  0.00101168  -1.889             0.058953\nbuilding_density         3.22875110  0.47301016   6.826  0.00000000000873359\navg_building_age         0.08027292  0.00965518   8.314 &lt; 0.0000000000000002\nI(avg_building_age^2)   -0.00039559  0.00005593  -7.073  0.00000000000151664\npct_Business_Commercial  1.24510516  0.26453601   4.707  0.00000251695918053\npct_Downtown             0.84752237  0.34966093   2.424             0.015357\npct_Industrial          -0.81280466  0.26579963  -3.058             0.002228\npct_Park_Open_Space     -0.55988433  0.28484747  -1.966             0.049350\n                           \n(Intercept)             ***\ndist_to_hotspot_outages ***\ncount_vacant            ***\ndist_to_streets         .  \nbuilding_density        ***\navg_building_age        ***\nI(avg_building_age^2)   ***\npct_Business_Commercial ***\npct_Downtown            *  \npct_Industrial          ** \npct_Park_Open_Space     *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.2527) family taken to be 1)\n\n    Null deviance: 2584.1  on 1276  degrees of freedom\nResidual deviance: 1254.4  on 1266  degrees of freedom\nAIC: 5055.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.253 \n          Std. Err.:  0.172 \n\n 2 x log-likelihood:  -5031.330 \n\n\n\n\nCode\n# Create new dataframe for Kable.\nmodel_comparison_stats &lt;- tibble(\n  Statistic = c(\n    \"Poisson Model AIC\",\n    \"Negative Binomial AIC\",\n    \"Poisson Dispersion\"\n  ),\n  Value = c(\n    round(AIC(model_poisson), 2),\n    round(AIC(model_nb), 2),\n    round(dispersion, 2)\n  )\n)\n\n# Format so that there are thousand separators.\nmodel_comparison_stats &lt;- model_comparison_stats %&gt;%\n  mutate(\n    Value = prettyNum(Value, big.mark = \",\")\n  )\n\n# Create Kable for AIC comparison.\nmodel_comparison_stats %&gt;%\n  mutate(\n    Statistic = cell_spec(Statistic, bold = TRUE)\n  ) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    caption = \"Akaike Information Criteria (AIC) and Dispersion\",\n    align = c(\"l\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), # Make the Kable visually compact.\n    full_width = FALSE,\n    position = \"center\"\n  ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  footnote(\n    general = \"Dispersion &gt; 1.5 suggests overdispersion.\\nLower AIC is better.\",\n    general_title = \"Note:\"\n  )\n\n\n\nAkaike Information Criteria (AIC) and Dispersion\n\n\n\n\n\n\nStatistic\nValue\n\n\n\n\nPoisson Model AIC\n5,772.46\n\n\nNegative Binomial AIC\n5,055.33\n\n\nPoisson Dispersion\n2.72\n\n\n\nNote:\n\n\n\n Dispersion &gt; 1.5 suggests overdispersion.\nLower AIC is better.\n\n\n\n\n\n\n\n\n\n4. VIF Test\n\n\nCode\n# VIF test for the Negative Binomial model.\nvif_results &lt;- vif(model_nb)\n\n# VIF to a data frame.\nvif_table &lt;- data.frame(\n  Variable = names(vif_results),\n  VIF = vif_results\n) %&gt;%\n  arrange(desc(VIF)) %&gt;% # Descending sort VIF values.\n  mutate(\n    Concern = case_when(\n      VIF &gt; 10 ~ \"High\",\n      VIF &gt; 5 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    )\n  )\n\n# Rename to make the table more neat and professional.\nvif_table &lt;- vif_table %&gt;%\n  mutate(Variable = case_when(\n    Variable == \"count_vacant\" ~ \"Vacancy Count\",\n    Variable == \"dist_to_streets\" ~ \"Distance to Major Street\",\n    Variable == \"building_density\" ~ \"Building Density\",\n    Variable == \"I(avg_building_age^2)\" ~ \"Average Building Age²\",\n    Variable == \"avg_building_age\" ~ \"Average Building Age\",\n    Variable == \"pct_Business_Commercial\" ~ \"% Business/Commercial\",\n    Variable == \"pct_Downtown\" ~ \"% Downtown\",\n    Variable == \"pct_Industrial\" ~ \"% Industrial\",\n    Variable == \"pct_Park_Open_Space\" ~ \"% Park/Open Space\",\n    Variable == \"dist_to_hotspot_outages\" ~ \"Distance to Outage Hotspots\",\n    TRUE ~ Variable\n    )\n  )\n\n# Create the Kable.\nvif_table %&gt;%\n  mutate(Variable = cell_spec(Variable, bold = TRUE)) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    digits = 2,\n    caption = \"Variance Inflation Factors (VIF)\",\n    col.names = c(\"Variable\", \"VIF\", \"Multicollinearity\"),\n    align = c(\"l\", \"c\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), # Make the large Kable visually compact.\n    full_width = FALSE,\n    position = \"center\"\n  ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  footnote(general = \"VIF &gt; 10 indicates high multicollinearity.\\nVIF &gt; 5 indicates moderate concern.\\nAge polynomials are mathematically related and negligible.\",\n           general_title = \"Note:\")\n\n\n\nVariance Inflation Factors (VIF)\n\n\n\n\n\n\n\nVariable\nVIF\nMulticollinearity\n\n\n\n\nAverage Building Age\n46.10\nHigh\n\n\nAverage Building Age²\n46.05\nHigh\n\n\n% Business/Commercial\n1.21\nLow\n\n\nBuilding Density\n1.21\nLow\n\n\n% Park/Open Space\n1.11\nLow\n\n\nDistance to Outage Hotspots\n1.09\nLow\n\n\nVacancy Count\n1.07\nLow\n\n\nDistance to Major Street\n1.06\nLow\n\n\n% Downtown\n1.06\nLow\n\n\n% Industrial\n1.05\nLow\n\n\n\nNote:\n\n\n\n\n VIF &gt; 10 indicates high multicollinearity.\nVIF &gt; 5 indicates moderate concern.\nAge polynomials are mathematically related and negligible.\n\n\n\n\n\n\n\n\n\n\n5. Section Analysis\nPart IV built the statistical count models, Poisson and Negative Binomial, which are meant to handle response variables that are counts, like in this case. Poisson was the initial model crafted, and after seeing that dispersion was an issue, the Negative Binomial approach was pursued, much like determining spatial dependence with Moran’s I on OLS regressions to pursue alternative spatial regressions.\nThe first table exhibits the summary statistics of the final variables used for the regression models.\nHowever, the more important aspect of the section is the interpretation of the models. And the following discusses the Poisson output:\n\nBeing farther from an outage hotspot decreases burglaries.\nMore vacant buildings are associated with more burglaries.\nBeing nearby at least 3 stations is very insignificant, but is associated with an increase in burglaries.\nBeing near a major street is also associated with an increase in burglaries, significantly more than stations, but it might be because the major streets are already capturing what the stations are capturing when it comes to access and escape options for burglaries.\nHigher building density is associated with more burglaries, and this is the most significant of all the predictors. This is likely due to the fact there are both more and a variety of potential burglary targets.\nBuilding age is positive but the squared age is negative, so older buildings are high risk for burglary. Older buildings could have less security and structural damage that makes them easy to enter, but that is up to a certain point. Historical buildings may be less targeted due to the fact that they may be under preservation and under tourist scrutiny for example.\nIndustrial and park zonings are less of targets, which is unsurprising because they likely don’t have anything of value to burglars, unlike residential, downtown, and business/commercial zonings.\nNote that the intercept would be representing the residential zoning, so this means that residential areas are less likely to be burglary targets than business/commercial and downtown zoning, but more likely compared to parks and industrial zoning.\n\nHowever, dispersion was greater than 1.5, meaning that it violated a Poisson assumption and inflated p-value significance. So a negative binomial was pursued afterward, and because the nn_stations variable was so insignificant in the Poisson model, it was removed from the negative binomial.\nAs for the negative binomial output, the relationship types remained the same and no signs flipped, so the general observations from the Poisson output above can be applied to the negative binomial output.\nIn the end, the difference in AIC was over 700, indicating that the negative binomial fit vastly better than the Poisson.\nA VIF test was also conducted to look for multicollinearity, and there was none, reinforcing the predictor and coefficient reliability."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-v-spatial-cross-validation-2017",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-v-spatial-cross-validation-2017",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART V: Spatial Cross-Validation (2017)",
    "text": "PART V: Spatial Cross-Validation (2017)\n\n1. Leave-One-Group-Out (LOGO) Cross-Validation\n\n\nCode\n# Get unique districts.\ndistricts_unique &lt;- unique(fishnet_model$dist_label)\ncv_results_districts &lt;- tibble()\n\nfor (i in seq_along(districts_unique)) {\n  \n  test_districts &lt;- districts_unique[i]\n  \n  # Split data.\n  train_data &lt;- fishnet_model %&gt;% filter(dist_label != test_districts)\n  test_data &lt;- fishnet_model %&gt;% filter(dist_label == test_districts)\n  \n  # Fit model on training data.\n  model_cv &lt;- glm.nb(count_burglaries ~\n                       # 311 data.\n                       dist_to_hotspot_outages +\n                       # Contextual count data.\n                       count_vacant +\n                       # Contextual spatial features.\n                       dist_to_streets + building_density +\n                       # Age polynomial.\n                       avg_building_age + I(avg_building_age^2) +\n                       # Zoning. Leave residential out as reference.\n                       pct_Business_Commercial + pct_Downtown + pct_Industrial + pct_Park_Open_Space,\n                     data = train_data,\n                     )\n  \n  # Predict on test data.\n  test_data &lt;- test_data %&gt;%\n    mutate(\n      prediction = predict(model_cv, test_data, type = \"response\")\n    )\n  \n  # Calculate metrics.\n  mae &lt;- mean(abs(test_data$count_burglaries - test_data$prediction))\n  rmse &lt;- sqrt(mean((test_data$count_burglaries - test_data$prediction)^2))\n  \n  # Store results.\n  cv_results_districts &lt;- bind_rows( # Bind function to \"stack\" tibble below.\n    cv_results_districts,\n    tibble(\n      test_district = test_districts,\n      mae = mae,\n      rmse = rmse\n    )\n  )\n}\n\n\n\n\nCode\n# Get unique neighborhoods\nneighborhoods_unique &lt;- unique(fishnet_model$community)\ncv_results_neighborhoods &lt;- tibble()\n\nfor (i in seq_along(neighborhoods_unique)) {\n  \n  test_neighborhoods &lt;- neighborhoods_unique[i]\n  \n  # Split data.\n  train_data &lt;- fishnet_model %&gt;% filter(community != test_neighborhoods)\n  test_data &lt;- fishnet_model %&gt;% filter(community == test_neighborhoods)\n  \n  # Fit model on training data.\n  model_cv &lt;- glm.nb(count_burglaries ~\n                       # 311 data.\n                       dist_to_hotspot_outages +\n                       # Contextual count data.\n                       count_vacant +\n                       # Contextual spatial features.\n                       dist_to_streets + building_density +\n                       # Age polynomial.\n                       avg_building_age + I(avg_building_age^2) +\n                       # Zoning. Leave residential out as reference.\n                       pct_Business_Commercial + pct_Downtown + pct_Industrial + pct_Park_Open_Space,\n                     data = train_data,\n                     )\n  \n  # Predict on test data.\n  test_data &lt;- test_data %&gt;%\n    mutate(\n      prediction = predict(model_cv, test_data, type = \"response\")\n    )\n  \n  # Calculate metrics.\n  mae &lt;- mean(abs(test_data$count_burglaries - test_data$prediction))\n  rmse &lt;- sqrt(mean((test_data$count_burglaries - test_data$prediction)^2))\n  \n  # Store results.\n  cv_results_neighborhoods &lt;- bind_rows(\n    cv_results_neighborhoods,\n    tibble(\n      test_neighborhood = test_neighborhoods,\n      mae = mae,\n      rmse = rmse\n    )\n  )\n}\n\n\n\n\n2. Cross-Validation Tables\n\n\nCode\n# Scrollable Kable for districts.\ncv_results_districts %&gt;%\n  arrange(desc(rmse)) %&gt;%\n  mutate(\n    test_district = cell_spec(test_district, bold = TRUE),\n    mae = round(mae, 2),\n    rmse = round(rmse, 2)\n  ) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    caption = \"Cross-Validation Results by Police Districts\",\n    col.names = c(\"Test District\", \"MAE\", \"RMSE\"),\n    align = c(\"l\", \"c\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), # Make the large Kable visually compact.\n    full_width = FALSE,\n    position = \"center\"\n  ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  scroll_box(height = \"500px\", width = \"500px\") %&gt;%  # Make it scrollable.\n  footnote(\n    general = paste0(\"Mean MAE: \", round(mean(cv_results_districts$mae), 2),\n                    \"\\nMean RMSE: \", round(mean(cv_results_districts$rmse), 2)),\n    general_title = \"Overall Performance:\"\n  )\n\n\n\n\nCross-Validation Results by Police Districts\n\n\n\n\n\n\n\nTest District\nMAE\nRMSE\n\n\n\n\n3RD\n5.05\n7.00\n\n\n1ST\n3.78\n4.91\n\n\n12TH\n3.26\n4.60\n\n\n2ND\n3.21\n4.41\n\n\n9TH\n3.10\n4.24\n\n\n24TH\n2.49\n3.96\n\n\n18TH\n3.24\n3.96\n\n\n14TH\n2.86\n3.91\n\n\n6TH\n2.93\n3.77\n\n\n19TH\n3.09\n3.74\n\n\n10TH\n2.75\n3.64\n\n\n8TH\n2.38\n3.48\n\n\n4TH\n1.28\n3.31\n\n\n11TH\n2.48\n3.30\n\n\n25TH\n2.42\n3.04\n\n\n7TH\n2.24\n3.00\n\n\n5TH\n1.76\n2.61\n\n\n22ND\n2.05\n2.54\n\n\n17TH\n2.04\n2.49\n\n\n15TH\n1.86\n2.46\n\n\n20TH\n1.63\n1.96\n\n\n16TH\n1.14\n1.86\n\n\n\nOverall Performance:\n\n\n\n\n Mean MAE: 2.59\nMean RMSE: 3.55\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Scrollable Kable for neighborhoods\ncv_results_neighborhoods %&gt;%\n  arrange(desc(rmse)) %&gt;%\n  mutate(\n    test_neighborhood = cell_spec(test_neighborhood, bold = TRUE),\n    mae = round(mae, 2),\n    rmse = round(rmse, 2)\n  ) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    caption = \"Cross-Validation Results by Neighborhoods\",\n    col.names = c(\"Neighborhood\", \"MAE\", \"RMSE\"),\n    align = c(\"l\", \"c\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), # Make the large Kable visually compact.\n    full_width = FALSE,\n    position = \"center\"\n  ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  scroll_box(height = \"500px\", width = \"500px\") %&gt;%  # Make it scrollable.\n  footnote(\n    general = paste0(\"Mean MAE: \", round(mean(cv_results_neighborhoods$mae), 2),\n                    \"\\nMean RMSE: \", round(mean(cv_results_neighborhoods$rmse), 2)),\n    general_title = \"Overall Performance:\"\n  )\n\n\n\n\nCross-Validation Results by Neighborhoods\n\n\n\n\n\n\n\nNeighborhood\nMAE\nRMSE\n\n\n\n\nSOUTH SHORE\n8.43\n11.24\n\n\nWOODLAWN\n7.28\n9.11\n\n\nWEST TOWN\n6.50\n7.87\n\n\nWASHINGTON PARK\n5.22\n6.86\n\n\nCHICAGO LAWN\n4.67\n5.73\n\n\nNEW CITY\n3.95\n5.22\n\n\nLOOP\n4.16\n5.20\n\n\nSOUTH CHICAGO\n3.36\n5.08\n\n\nHYDE PARK\n3.32\n5.01\n\n\nGAGE PARK\n3.53\n4.64\n\n\nNEAR NORTH SIDE\n3.85\n4.63\n\n\nWEST ELSDON\n4.08\n4.61\n\n\nNORTH CENTER\n3.70\n4.10\n\n\nHERMOSA\n3.35\n4.06\n\n\nGRAND BOULEVARD\n3.33\n4.05\n\n\nNORTH LAWNDALE\n3.00\n4.00\n\n\nAUBURN GRESHAM\n3.08\n4.00\n\n\nWEST RIDGE\n2.17\n3.99\n\n\nROSELAND\n3.01\n3.91\n\n\nHUMBOLDT PARK\n3.42\n3.85\n\n\nROGERS PARK\n3.02\n3.79\n\n\nLAKE VIEW\n3.09\n3.78\n\n\nAVONDALE\n3.22\n3.61\n\n\nWEST ENGLEWOOD\n2.73\n3.59\n\n\nCHATHAM\n2.98\n3.58\n\n\nBEVERLY\n3.40\n3.57\n\n\nPORTAGE PARK\n3.25\n3.53\n\n\nBRIGHTON PARK\n2.58\n3.50\n\n\nSOUTH LAWNDALE\n2.65\n3.49\n\n\nEAST GARFIELD PARK\n2.44\n3.40\n\n\nGREATER GRAND CROSSING\n1.96\n3.25\n\n\nARCHER HEIGHTS\n2.54\n3.25\n\n\nMCKINLEY PARK\n2.46\n3.04\n\n\nIRVING PARK\n2.67\n2.97\n\n\nASHBURN\n1.85\n2.93\n\n\nBELMONT CRAGIN\n2.34\n2.91\n\n\nNEAR SOUTH SIDE\n2.39\n2.90\n\n\nALBANY PARK\n2.39\n2.89\n\n\nCALUMET HEIGHTS\n2.22\n2.86\n\n\nLOWER WEST SIDE\n2.27\n2.85\n\n\nJEFFERSON PARK\n2.32\n2.73\n\n\nKENWOOD\n2.47\n2.65\n\n\nAUSTIN\n1.94\n2.60\n\n\nNEAR WEST SIDE\n2.11\n2.54\n\n\nLINCOLN PARK\n2.19\n2.44\n\n\nWASHINGTON HEIGHTS\n1.94\n2.44\n\n\nMONTCLARE\n1.95\n2.36\n\n\nENGLEWOOD\n1.81\n2.35\n\n\nAVALON PARK\n1.96\n2.28\n\n\nPULLMAN\n1.41\n2.25\n\n\nBURNSIDE\n2.23\n2.23\n\n\nLOGAN SQUARE\n1.82\n2.23\n\n\nWEST PULLMAN\n1.78\n2.15\n\n\nDOUGLAS\n1.73\n2.07\n\n\nMORGAN PARK\n1.58\n2.00\n\n\nGARFIELD RIDGE\n1.64\n1.97\n\n\nUPTOWN\n1.56\n1.96\n\n\nDUNNING\n1.44\n1.92\n\n\nWEST LAWN\n1.39\n1.91\n\n\nBRIDGEPORT\n1.49\n1.88\n\n\nLINCOLN SQUARE\n1.47\n1.83\n\n\nEAST SIDE\n1.34\n1.80\n\n\nNORWOOD PARK\n1.53\n1.78\n\n\nEDGEWATER\n1.40\n1.72\n\n\nCLEARING\n1.27\n1.61\n\n\nMOUNT GREENWOOD\n1.16\n1.37\n\n\nEDISON PARK\n1.34\n1.36\n\n\nARMOUR SQUARE\n1.30\n1.30\n\n\nHEGEWISCH\n0.59\n1.24\n\n\nFOREST GLEN\n0.93\n1.08\n\n\nNORTH PARK\n0.83\n1.08\n\n\nRIVERDALE\n0.44\n0.81\n\n\nSOUTH DEERING\n0.39\n0.73\n\n\nWEST GARFIELD PARK\n0.41\n0.54\n\n\nOAKLAND\n0.17\n0.17\n\n\nOHARE\n0.05\n0.15\n\n\n\nOverall Performance:\n\n\n\n\n Mean MAE: 2.46\nMean RMSE: 3.11\n\n\n\n\n\n\n\n\n\n\n\n\nPolice Districts (From CPD)\n\n\n\n\nCode\n# Join RMSE values to districts.\ndistricts_rmse &lt;- police_districts %&gt;%\n  left_join(cv_results_districts,\n            by = c(\"dist_label\" = \"test_district\"))\n\n# Join RMSE values to neighborhoods.\nneighborhoods_rmse &lt;- neighborhoods %&gt;%\n  left_join(cv_results_neighborhoods,\n            by = c(\"community\" = \"test_neighborhood\"))\n\n# Create custom color ramp.\nweitzman &lt;- colorRampPalette(c(\"#778ac5\", \"#ff4100\"))\n\n# District max and min RMSE for labeling.\ndistrict_max &lt;- districts_rmse %&gt;%\n  slice_max(rmse, n = 1)\ndistrict_min &lt;- districts_rmse %&gt;%\n  slice_min(rmse, n = 1)\n\n# Neighborhood max and min RMSE for labeling.\nneighborhood_max &lt;- neighborhoods_rmse %&gt;%\n  slice_max(rmse, n = 1)\nneighborhood_min &lt;- neighborhoods_rmse %&gt;%\n  slice_min(rmse, n = 1)\n\n# Police district RMSE.\nrmse_districts &lt;- ggplot(districts_rmse) +\n  geom_sf(aes(fill = rmse),\n          color = \"#f5f4f0\",\n          linewidth = 0.75\n          ) +\n  scale_fill_gradientn(\n    colors = weitzman(256),\n    name = \"Police District RMSE\",\n    trans = \"sqrt\",\n    breaks = c(1.88, 2.79, 3.55, 3.98, 5.00, 6.99)\n    ) +\n  geom_sf_label(\n    data = district_max,\n    aes(label = paste0(dist_label, \": \", round(rmse, 2))),\n    fill = \"#2d2a26\", color = \"#f5f4f0\", size = 3, label.size = 0,\n    fontface = \"bold\"\n    ) +\n  geom_sf_label(\n    data = district_min,\n    aes(label = paste0(dist_label, \": \", round(rmse, 2))),\n    fill = \"#2d2a26\", color = \"#f5f4f0\", size = 3, label.size = 0,\n    fontface = \"bold\"\n    ) +\n  labs(\n    title = \"Police Districts\",\n    subtitle = \"Chicago, Illinois | Min and Max Districts\"\n    ) +\n  theme_chi_map() +\n  theme(\n    plot.background  = element_rect(fill = \"#f5f4f0\", color = NA),\n    panel.background = element_rect(fill = \"#f5f4f0\", color = NA)\n  )\n\n# Neighborhood RMSE.\nrmse_neighborhoods &lt;- ggplot(neighborhoods_rmse) +\n  geom_sf(aes(fill = rmse),\n          color = \"#f5f4f0\",\n          linewidth = 0.75\n          ) +\n  scale_fill_gradientn(\n    colors = weitzman(256),\n    name = \"Neighborhood RMSE\",\n    trans = \"sqrt\",\n    breaks = c(0.13, 1.93, 2.86, 3.90, 6.00, 11.40)\n    ) +\n  geom_sf_label(\n    data = neighborhood_max,\n    aes(label = paste0(community, \": \", round(rmse, 2))),\n    fill = \"#2d2a26\", color = \"#f5f4f0\", size = 3, label.size = 0,\n    fontface = \"bold\"\n  ) +\n  geom_sf_label(\n    data = neighborhood_min,\n    aes(label = paste0(community, \": \", round(rmse, 2))),\n    fill = \"#2d2a26\", color = \"#f5f4f0\", size = 3, label.size = 0,\n    fontface = \"bold\"\n  ) +\n  labs(\n    title = \"Neighborhoods\",\n    subtitle = \"Chicago, Illinois | Min and Max Neighborhoods\"\n    ) +\n  theme_chi_map() +\n  theme(\n    plot.background  = element_rect(fill = \"#f5f4f0\", color = NA),\n    panel.background = element_rect(fill = \"#f5f4f0\", color = NA)\n  )\n\n# Combine plots using patchwork.\nrmse_districts + rmse_neighborhoods +\n  plot_annotation(\n    title = \"Spatial Distribution of LOGO Cross-Validation\",\n    subtitle = \"Root Mean Squared Error (RMSE)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        ),\n      plot.background  = element_rect(fill = \"#f5f4f0\", color = NA),\n      panel.background = element_rect(fill = \"#f5f4f0\", color = NA),\n      )\n    )\n\n\n\n\n\n\n\n\n\n\n\n3. Section Analysis\nTo explain Part III-4 why the police districts and neighborhoods were joined to the fishnet: The goal, again, is to make this predictive model as “ethical” as possible. Even if testing folds is majorly for technical purposes, testing folds over police districts implies that the predictive model is being geared toward policing rather than neighborhoods, which has more implications for community-building and resource/service allocation.\nThe cross-validation with the 2017 data was used to evaluate how robust the model is using a Leave-One-Group-Out (LOGO) method instead of the classic k-fold cross-validation. The importance of this is to remove either a district or neighborhood out, train on the other districts, then predict that held-out district to determine the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\nAlso, according to the CV tables, the three highest RMSEs are districts 3, 1, and 12, and neighborhoods South Shore, Woodlawn, and West Town. The districts with the highest errors are in the Central, South Side, and West Sides, and the neighborhoods with the highest errors are within West Side and South Side. The biggest overlap is especially in South Side were the South Shore, Woodlawn, and 3rd District overlap around the waterfront area.\nOn the other end of that spectrum, the districts with the lowest RMSEs are 16, 20, and 15, and neighborhoods with the lowest errors are O’Hare, Oakland, and West Garfield Park. These regions are in Far North Side where the airport is, which is expected, as well as West Side when looking at districts. However, looking at neighborhoods, the ones that performed the best are Oakland in South Side, West Garfield Park in West Side, and O’Hare in Far North Side, as previously explained.\nDistrict-scale vs. neighborhood-scale differ, this particular model performs a lot better with the neighborhoods being typically 2 to 3 burglaries off when it comes to predicting, as opposed to the districts being typically 3 burglaries off.\nThis could be due to a variety of factors, but the performance by differ due to the fact neighborhoods are more uniform when it comes to urban characteristics, which is exactly what the predictor variables are based on. It might have not performed as well for police districts because they tend to be delineated according to call volume and crime rates and how many resources police are able to dispatch to handle them (i.e. drawing police boundaries may not be as strongly influenced by urban form)."
  },
  {
    "objectID": "labs/lab-4/Vu_Tessa_Assignment4.html#part-vi-model-evaluation",
    "href": "labs/lab-4/Vu_Tessa_Assignment4.html#part-vi-model-evaluation",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "PART VI: Model Evaluation",
    "text": "PART VI: Model Evaluation\n\n1. Generate Final Predictions\n\n\nCode\n# Fit final model on all data.\nfinal_model &lt;- glm.nb(count_burglaries ~\n                        # 311 data.\n                        dist_to_hotspot_outages +\n                        # Contextual count data.\n                        count_vacant +\n                        # Contextual spatial features.\n                        dist_to_streets + building_density +\n                        # Age polynomial.\n                        avg_building_age + I(avg_building_age^2) +\n                        # Zoning. Leave residential out as reference.\n                        pct_Business_Commercial + pct_Downtown + pct_Industrial + pct_Park_Open_Space,\n                     data = fishnet_model\n                     )\n\n# Add predictions back to fishnet.\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts).\nkde_sum &lt;- sum(fishnet$kde_value_burglaries, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$count_burglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value_burglaries / kde_sum) * count_sum\n  )\n\n\n\n\n2. Compare Model vs. KDE Baseline\n\n\nCode\n# Create three maps.\nactual_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = count_burglaries),\n          color = NA\n          ) +\n  scale_fill_viridis_c(name = \"Actual and Predicted Count\",\n                       option = \"rocket\",\n                       direction = -1,\n                       limits = c(0, 15)\n                       ) +\n  labs(title = \"Actual Burglaries\") +\n  theme_chi_map()\n\nmodel_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = prediction_nb),\n          color = NA\n          ) +\n  scale_fill_viridis_c(name = \"Actual and Predicted Count\",\n                       option = \"rocket\",\n                       direction = -1,\n                       limits = c(0, 15)\n                       ) +\n  labs(title = \"Model Predictions (Negative Binomial)\") +\n  theme_chi_map()\n\nkde_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = prediction_kde),\n          color = NA\n          ) +\n  scale_fill_viridis_c(name = \"Actual and Predicted Count\",\n                       option = \"rocket\",\n                       direction = -1,\n                       limits = c(0, 15)\n                       ) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_chi_map()\n\nactual_map + model_map + kde_map +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Performance: Negative Binomial vs. Simple KDE\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n  ) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics.\ncomparison_2017 &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    NB_MAE = mean(abs(count_burglaries - prediction_nb)),\n    NB_RMSE = sqrt(mean((count_burglaries - prediction_nb)^2)),\n    KDE_MAE = mean(abs(count_burglaries - prediction_kde)),\n    KDE_RMSE = sqrt(mean((count_burglaries - prediction_kde)^2))\n  )\n\n# Create Kable.\ncomparison_2017 %&gt;%\n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") %&gt;%\n  separate(Metric, into = c(\"Approach\", \"Metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = Metric, values_from = Value) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    digits = 2,\n    caption = \"Model In-Sample Performance Comparison (2017)\",\n    align = c(\"l\", \"c\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n    ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  footnote(\n    general = \"Lower values are better.\",\n    general_title = \"Note:\"\n  )\n\n\n\nModel In-Sample Performance Comparison (2017)\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nNB\n2.11\n3.21\n\n\nKDE\n1.98\n2.90\n\n\n\nNote:\n\n\n\n\n Lower values are better.\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics.\ncomparison_2018 &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    NB_MAE = mean(abs(count_burglaries_2018 - prediction_nb)),\n    NB_RMSE = sqrt(mean((count_burglaries_2018 - prediction_nb)^2)),\n    KDE_MAE = mean(abs(count_burglaries_2018 - prediction_kde)),\n    KDE_RMSE = sqrt(mean((count_burglaries_2018 - prediction_kde)^2))\n  )\n\n# Create Kable.\ncomparison_2018 %&gt;%\n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Value\") %&gt;%\n  separate(Metric, into = c(\"Approach\", \"Metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = Metric, values_from = Value) %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    digits = 2,\n    caption = \"Model Out-of-Sample Performance Comparison (2018)\",\n    align = c(\"l\", \"c\", \"c\"),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n    ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  footnote(\n    general = \"Lower values are better.\",\n    general_title = \"Note:\"\n  )\n\n\n\nModel Out-of-Sample Performance Comparison (2018)\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nNB\n1.97\n2.96\n\n\nKDE\n2.02\n2.97\n\n\n\nNote:\n\n\n\n\n Lower values are better.\n\n\n\n\n\n\n\n\n\n\n3. Where Does the Model Work Well?\n\n\nCode\n# Calculate errors.\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = count_burglaries - prediction_nb,\n    error_kde = count_burglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n\n\n\nCode\n# NB map errors.\nnb_error_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = error_nb),\n          color = NA\n          ) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#011f5b\", mid = \"white\", high = \"#990000\",\n    midpoint = 0,\n    limits = c(-10, 10),\n    breaks = c(-10, -5, 0, 5, 10)\n    ) +\n  labs(title = \"NB Model Errors (Actual – Predicted)\") +\n  theme_chi_map()\n\n# NB map absolute errors.\nnb_abs_error_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = abs_error_nb),\n          color = NA\n          ) +\n  scale_fill_viridis_c(name = \"NB Absolute Error\",\n                       option = \"rocket\",\n                       direction = -1,\n                       breaks = c(0, 5, 10, 15, 20)\n                       ) +\n  labs(title = \"NB Model Absolute Errors\") +\n  theme_chi_map()\n\n# KDE map errors.\nkde_error_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = error_kde),\n          color = NA\n          ) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#011f5b\", mid = \"white\", high = \"#990000\",\n    midpoint = 0,\n    limits = c(-10, 10),\n    breaks = c(-10, -5, 0, 5, 10)\n    ) +\n  labs(title = \"KDE Model Errors (Actual – Predicted)\") +\n  theme_chi_map()\n\n# KDE map absolute errors.\nkde_abs_error_map &lt;- ggplot() +\n  geom_sf(data = fishnet,\n          aes(fill = abs_error_kde),\n          color = NA\n          ) +\n  scale_fill_viridis_c(name = \"KDE Absolute Error\",\n                       option = \"rocket\",\n                       direction = -1,\n                       breaks = c(0, 5, 10, 15, 20)\n                       ) +\n  labs(title = \"KDE Model Absolute Errors\") +\n  theme_chi_map()\n\n(nb_error_map + nb_abs_error_map) / (kde_error_map + kde_abs_error_map) +\n  plot_annotation(\n    title = \"Model Errors\",\n    subtitle = \"Negative Binomial (NB) vs. Kernel Density Estimation (KDE)\",\n    theme = theme(\n      plot.title = element_text(\n        size = 16, \n        face = \"bold\",\n        hjust = 0.5\n        ),\n      plot.subtitle = element_text(\n        size = 14,\n        face = \"italic\",\n        hjust = 0.5,\n        color = \"gray30\"\n        )\n      )\n    ) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n4. Model Summary Table\n\n\nCode\n# Create final summary table.\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\n# Rename to make the table more neat and professional.\nmodel_summary &lt;- model_summary %&gt;%\n  mutate(term = case_when(\n    term == \"(Intercept)\" ~ \"Intercept\",\n    term == \"I(avg_building_age^2)\" ~ \"Average Building Age²\",\n    term == \"count_vacant\" ~ \"Vacancy Count\",\n    term == \"dist_to_streets\" ~ \"Distance to Major Street\",\n    term == \"building_density\" ~ \"Building Density\",\n    term == \"I(avg_building_age^2)\" ~ \"Average Building Age²\",\n    term == \"avg_building_age\" ~ \"Average Building Age\",\n    term == \"pct_Business_Commercial\" ~ \"% Business/Commercial\",\n    term == \"pct_Downtown\" ~ \"% Downtown\",\n    term == \"pct_Industrial\" ~ \"% Industrial\",\n    term == \"pct_Park_Open_Space\" ~ \"% Park/Open Space\",\n    term == \"dist_to_hotspot_outages\" ~ \"Distance to Outage Hotspots\",\n    TRUE ~ term\n    )\n  )\n\nmodel_summary %&gt;%\n  kable(\n    format = \"html\", # HTML formatting for visualization flexibility.\n    escape = FALSE,\n    digits = 2,\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Standard Error\", \"Z-Score\", \"P-Value\"),\n    align = c(\"l\", rep(\"c\", 4)),\n    row.names = FALSE\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE,\n    position = \"center\"\n    ) %&gt;%\n  row_spec(0, bold = TRUE, color = \"white\", background = \"#21918c\", align = \"c\") %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  footnote(\n    general = \"Rate Ratios &gt; 1 have a positive association with burglary counts.\",\n    general_title = \"Note:\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStandard Error\nZ-Score\nP-Value\n\n\n\n\nIntercept\n0.05\n0.41\n-7.18\n0.00\n\n\nDistance to Outage Hotspots\n1.00\n0.00\n-8.20\n0.00\n\n\nVacancy Count\n1.01\n0.00\n3.56\n0.00\n\n\nDistance to Major Street\n1.00\n0.00\n-1.89\n0.06\n\n\nBuilding Density\n25.25\n0.47\n6.83\n0.00\n\n\nAverage Building Age\n1.08\n0.01\n8.31\n0.00\n\n\nAverage Building Age²\n1.00\n0.00\n-7.07\n0.00\n\n\n% Business/Commercial\n3.47\n0.26\n4.71\n0.00\n\n\n% Downtown\n2.33\n0.35\n2.42\n0.01\n\n\n% Industrial\n0.44\n0.27\n-3.06\n0.00\n\n\n% Park/Open Space\n0.57\n0.28\n-1.97\n0.05\n\n\n\nNote:\n\n\n\n\n\n\n Rate Ratios &gt; 1 have a positive association with burglary counts.\n\n\n\n\n\n\n\n\n\n\n\n\n5. Section Analysis and Overall Discussion\nPart VI evaluates the final model compared to the baseline KDE that was created earlier in Part II-4, the results are surprisingly closer than expected. Of course, it was expected that KDE would likely perform better with in-sampling due to more overfit compared to the final NB model, which ended up performing better out-of-sample compared to the KDE model.\nThe differences in MAE and RMSE are more prominent when looking at the in-sample table, and the out-of-sample table values have very marginal differences, but in the end the NB model does generalize better by a slim amount. This can be observed when looking at the faceted map and how they have, visually, extremely close spatial pattern overlap.\nIt is also crucial to address the practical use of this prediction model: it has potential and can be useful under human supervision, and this is of an opinion that is cemented, regardless of how well a model does. The oversight is needed even if it performs spatially and temporally well, because the model can be used to disperse resources to communities, but may over- or under-predict still; this means it can help detect hotspots, serving as an early warning for urban planners to consider Chicago’s future form.\nIt can be argued that this lab’s goal striving for the least damage is contradicted by the simple fact that this is a burglary predictive model, but it can also be argued that the variables themselves are related to the built environment only. So, the tool can instead aid public servants to take a deeper look into alternative, urban planning or local, citizen-driven solutions to alleviate forced entry burglaries rather than sending police to communities that are already intensely scrutinized."
  },
  {
    "objectID": "map-challenge.html",
    "href": "map-challenge.html",
    "title": "30-DAY MAP CHALLENGE",
    "section": "",
    "text": "Whenever time permits, although time doesn’t often permit."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#why-this-matters-to-the-office-of-property-assessment",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#why-this-matters-to-the-office-of-property-assessment",
    "title": "Model Citizens Consulting",
    "section": "Why This Matters to the Office of Property Assessment",
    "text": "Why This Matters to the Office of Property Assessment\nImproving the Office of Property Assessment’s Automated Valuation Model (AVM) can potentially:\n\nMore stable values aligned with market rates for properties being sold.\nAlleviate burden from gentrification in previously disinvested neighborhoods."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#the-problem-the-goal",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#the-problem-the-goal",
    "title": "Model Citizens Consulting",
    "section": "The Problem & The Goal",
    "text": "The Problem & The Goal\nWhat We Want\n\nEquitable taxation for residents.\nFinancial stability for Philadelphia.\nHelp set the future for more reliable open data."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#where-are-expensive-homes",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#where-are-expensive-homes",
    "title": "Model Citizens Consulting",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?"
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#findings",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#findings",
    "title": "Model Citizens Consulting",
    "section": "Findings",
    "text": "Findings\n\nHigher Prices: Center City, University City, the riverfront, and affluent Northwest pockets.\nPotentially due to easy access to transit and amenities.\nLower Prices: North of Broad Street into parts of West and North Philadelphia.\nPotentially reflecting long-term disinvestment.\nSale price is place-dependent in Philadelphia, mostly due to neighborhood qualities."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#what-drives-prices",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#what-drives-prices",
    "title": "Model Citizens Consulting",
    "section": "What Drives Prices?",
    "text": "What Drives Prices?\n\n\n\n\nLarger homes = an increase in price, but only up to a certain point."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#model-comparison-performance",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#model-comparison-performance",
    "title": "Model Citizens Consulting",
    "section": "Model Comparison & Performance",
    "text": "Model Comparison & Performance\n\n\n\n\n\nRMSE = 138,279.40 → Predicted sale price differs by about ± $138,279 from actual market sale price.\nR² = 0.746 → Explains 75% of variance in home prices."
  },
  {
    "objectID": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#hardest-to-predict",
    "href": "midterm-project/Cao_Deng_Luu_Rigsby_Stauffer_Vu_Slides.html#hardest-to-predict",
    "title": "Model Citizens Consulting",
    "section": "Hardest To Predict",
    "text": "Hardest To Predict"
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html",
    "href": "weekly-notes/week-01/week-01-notes.html",
    "title": "COURSE INTRODUCTION",
    "section": "",
    "text": "Main Concepts\n\nData science skills and public policy knowledge are often used in tandem, the latter of which inherently possesses massive quantifiable and qualitative data, so data science concepts and tools are important to clean, wrangle, and make all the big data easier to understand for people who are familiar with the technical and political intricacies behind them, and especially for people who are unfamiliar with it all. Communicating this crossover is crucial to inform constituents and government representatives on existing or creating local, state, and/or federal legislation.\nReproducible research is important, which means consistent and concise documentation, relevant file and variable naming, pushing updates with change comments. A little goes a long way, commit little changes constantly to avoid large commits that take time to look through and debug.\nApplying technical skills to public issues, which not only means making details as transparent as possible, but also utilizing accessible and open-source software and other tools that many communities and individuals can use and pull from.\n\nTechnical Skills\n\nSyntax and semantic familiarity with R and Markdown\nKeeping track of folder paths for organization.\nConsistent notation in script files for documentation.\nCrafting relevant and readable variable names for external audiences.\nRepitition is the mother of learning for programming languages."
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01/week-01-notes.html#key-concepts-learned",
    "title": "COURSE INTRODUCTION",
    "section": "",
    "text": "Main Concepts\n\nData science skills and public policy knowledge are often used in tandem, the latter of which inherently possesses massive quantifiable and qualitative data, so data science concepts and tools are important to clean, wrangle, and make all the big data easier to understand for people who are familiar with the technical and political intricacies behind them, and especially for people who are unfamiliar with it all. Communicating this crossover is crucial to inform constituents and government representatives on existing or creating local, state, and/or federal legislation.\nReproducible research is important, which means consistent and concise documentation, relevant file and variable naming, pushing updates with change comments. A little goes a long way, commit little changes constantly to avoid large commits that take time to look through and debug.\nApplying technical skills to public issues, which not only means making details as transparent as possible, but also utilizing accessible and open-source software and other tools that many communities and individuals can use and pull from.\n\nTechnical Skills\n\nSyntax and semantic familiarity with R and Markdown\nKeeping track of folder paths for organization.\nConsistent notation in script files for documentation.\nCrafting relevant and readable variable names for external audiences.\nRepitition is the mother of learning for programming languages."
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01/week-01-notes.html#coding-techniques",
    "title": "COURSE INTRODUCTION",
    "section": "CODING TECHNIQUES",
    "text": "CODING TECHNIQUES\n\n“group_by()” and “summarize()” functions go hand-in-hand to essentially isolate specific pockets of data within the tables and derive certain calculations from them, like averages, medians, standard deviations, etc.\n“select()” and “filter()” functions are for columns and rows, respectively, and are inverted in ArcGIS.\n“mutate()” function is used to create new variables, specifically new columns or modifying existing columns in data frames.\nQuarto combines multiple languages and makes it readable, so familiarity with using Markdown to format websites and communicate visualization and data is important. Don’t shy away from playing with bold, italics, embedding links and images, etc. to refine portfolio design."
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01/week-01-notes.html#questions-challenges",
    "title": "COURSE INTRODUCTION",
    "section": "QUESTIONS & CHALLENGES",
    "text": "QUESTIONS & CHALLENGES\n\nWhat happens in the background when rendering in Quarto, why can’t the website be deployed just from the GitHub website? Is it to establish the “docs” folder when doing it locally, because there was an error where the folder didn’t exist when trying to publish the portfolio just from the website.\nNeed more practice with understanding local and GitHub file structures and how they relate to one another as well as R functions. There’s a basic understanding with coding syntax and semantics, but what complexities occur in the background with data storage and the abstractness of it?"
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01/week-01-notes.html#connections-to-policy",
    "title": "COURSE INTRODUCTION",
    "section": "CONNECTIONS TO POLICY",
    "text": "CONNECTIONS TO POLICY\n\nThere’s a great potential for using predictive analytics to provide useful information and turn it into public-serving action, but they’re also tools that can be purposefully or unintentionally misused to exacerbate biases and existing social stratification, like predictive analytics for crime and policing.\nData science is useful for cleaning raw data, which will inevitably be collected in massive numbers due to the inherently large nature of cities and people."
  },
  {
    "objectID": "weekly-notes/week-01/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01/week-01-notes.html#reflection",
    "title": "COURSE INTRODUCTION",
    "section": "REFLECTION",
    "text": "REFLECTION\n\nThe most interesting part was adjusting and troubleshooting conflicts and errors encountered with GitHub, R, and Markdown, which is integral to real-life circumstances using data science tools.\nIndividually using GitHub will provide the skills to collaboratively work on it with others, because applying these skills is often done in a group due to the collective nature of public and urban policy, and if not in a group, it will inevitably be shared with an audience.\nTrial and error is central to learning programming languages and holistically understanding how different data, files, and functions tie-in and work together. When coming across issues with getting the notes page to publish and show, much of the journey was adjusting file locations and syntax around the .qmd and .yml files."
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html",
    "href": "weekly-notes/week-03/week-03-class-lab.html",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "",
    "text": "Code\n# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"3aaee31789e10b674a531e9f236c35d5394b19ed\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#setup-and-data-loading",
    "href": "weekly-notes/week-03/week-03-class-lab.html#setup-and-data-loading",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "",
    "text": "Code\n# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"3aaee31789e10b674a531e9f236c35d5394b19ed\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-0-finding-census-variable-codes",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-0-finding-census-variable-codes",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n\nCode\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\n\nCode\nhead(acs_vars_2022)\n\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n\nCode\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n\nCode\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n\nCode\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n\n[1] \"Household income variables:\"\n\n\nCode\nhead(household_income)\n\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nCode\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables.\n\n\nCode\n# Search for median income\nage_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*age\"))\n\nhead(age_vars, 10)\n\n\n# A tibble: 10 × 4\n   name        label                           concept                 geography\n   &lt;chr&gt;       &lt;chr&gt;                           &lt;chr&gt;                   &lt;chr&gt;    \n 1 B01002A_001 Estimate!!Median age --!!Total: Median Age by Sex (Whi… block gr…\n 2 B01002A_002 Estimate!!Median age --!!Male   Median Age by Sex (Whi… block gr…\n 3 B01002A_003 Estimate!!Median age --!!Female Median Age by Sex (Whi… block gr…\n 4 B01002B_001 Estimate!!Median age --!!Total: Median Age by Sex (Bla… block gr…\n 5 B01002B_002 Estimate!!Median age --!!Male   Median Age by Sex (Bla… block gr…\n 6 B01002B_003 Estimate!!Median age --!!Female Median Age by Sex (Bla… block gr…\n 7 B01002C_001 Estimate!!Median age --!!Total: Median Age by Sex (Ame… block gr…\n 8 B01002C_002 Estimate!!Median age --!!Male   Median Age by Sex (Ame… block gr…\n 9 B01002C_003 Estimate!!Median age --!!Female Median Age by Sex (Ame… block gr…\n10 B01002D_001 Estimate!!Median age --!!Total: Median Age by Sex (Asi… block gr…\n\n\nCode\n# Alternative: search by concept\nage_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Age\"))\n\nhead(age_concept, 10)\n\n\n# A tibble: 10 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_001 Estimate!!Total:                         Sex by Age (W… tract    \n 2 B01001A_002 Estimate!!Total:!!Male:                  Sex by Age (W… tract    \n 3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years    Sex by Age (W… tract    \n 5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years  Sex by Age (W… tract    \n 6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years  Sex by Age (W… tract    \n 7 B01001A_007 Estimate!!Total:!!Male:!!18 and 19 years Sex by Age (W… tract    \n 8 B01001A_008 Estimate!!Total:!!Male:!!20 to 24 years  Sex by Age (W… tract    \n 9 B01001A_009 Estimate!!Total:!!Male:!!25 to 29 years  Sex by Age (W… tract    \n10 B01001A_010 Estimate!!Total:!!Male:!!30 to 34 years  Sex by Age (W… tract    \n\n\n\n\n0.5 Advanced Search Techniques\nYour Task: Learn more sophisticated search methods.\n\n\nCode\n# Search for multiple terms at once\nhousing_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*(rent|value)\"))\n\nprint(\"Housing cost variables:\")\n\n\n[1] \"Housing cost variables:\"\n\n\nCode\nhead(housing_vars, 10)\n\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B07002PR_004 Estimate!!Median age --!!Total:!!Moved from d… Median… &lt;NA&gt;     \n 2 B07002_004   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 3 B07002_005   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 4 B07011PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n 5 B07011_004   Estimate!!Median income in the past 12 months… Median… tract    \n 6 B07011_005   Estimate!!Median income in the past 12 months… Median… tract    \n 7 B07402PR_004 Estimate!!Median age --!!Total living in area… Median… &lt;NA&gt;     \n 8 B07402_004   Estimate!!Median age --!!Total living in area… Median… county   \n 9 B07402_005   Estimate!!Median age --!!Total living in area… Median… county   \n10 B07411PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n\n\nCode\n# Search excluding certain terms\nincome_not_family &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\") & \n         !str_detect(label, \"family\"))\n\nprint(\"Income variables (not family income):\")\n\n\n[1] \"Income variables (not family income):\"\n\n\nCode\nhead(income_not_family)\n\n\n# A tibble: 6 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n6 B06011_001   Estimate!!Median income in the past 12 months … Median… tract    \n\n\nCode\n# Case-insensitive search using regex\neducation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, regex(\"bachelor\", ignore_case = TRUE)))\n\nprint(\"Education variables:\")\n\n\n[1] \"Education variables:\"\n\n\nCode\nhead(education_vars, 5)\n\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06009PR_005 Estimate!!Total:!!Bachelor's degree             Place … &lt;NA&gt;     \n2 B06009PR_011 Estimate!!Total:!!Born in Puerto Rico:!!Bachel… Place … &lt;NA&gt;     \n3 B06009PR_017 Estimate!!Total:!!Born in the United States:!!… Place … &lt;NA&gt;     \n4 B06009PR_023 Estimate!!Total:!!Native; born elsewhere:!!Bac… Place … &lt;NA&gt;     \n5 B06009PR_029 Estimate!!Total:!!Foreign born:!!Bachelor's de… Place … &lt;NA&gt;     \n\n\n\n\n0.6 Interactive Exploration\nYour Task: Use RStudio’s viewer for easier searching.\n\n\nCode\n# Open the full variable list in RStudio viewer\n# This opens a searchable data table\n#View(acs_vars_2022)\n\n# Pro tip: You can also search specific table groups\n# B01 = Age and Sex\n# B19 = Income  \n# B25 = Housing\ntable_b19 &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(name, \"^B19\"))  # ^ means \"starts with\"\n\nprint(\"All B19 (Income) table variables:\")\n\n\n[1] \"All B19 (Income) table variables:\"\n\n\nCode\nhead(table_b19, 10)\n\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n\n0.7 Verify Your Variable Choices\nYour Task: Test your variables by getting a small sample of data.\n\n\nCode\n# Test the variables you found\ntest_vars &lt;- c(\n  total_pop = \"B01003_001\",      # Total population\n  median_income = \"B19013_001\",  # Median household income\n  median_age = \"B01002_001\"      # Median age\n)\n\n# Get data for just one state to test\ntest_data &lt;- get_acs(\n  geography = \"state\",\n  variables = test_vars,\n  state = \"PA\",\n  year = 2022\n)\n\n# Check that you got what you expected\ntest_data\n\n\n# A tibble: 3 × 5\n  GEOID NAME         variable        estimate   moe\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 42    Pennsylvania median_age          40.8   0.1\n2 42    Pennsylvania total_pop     12989208    NA  \n3 42    Pennsylvania median_income    73170   347  \n\n\n\n\n0.8 Common Variable Patterns\nReference guide for future use:\n\n\nCode\n# Common patterns to remember:\ncommon_variables &lt;- tribble(\n  ~concept, ~typical_code, ~description,\n  \"Total Population\", \"B01003_001\", \"Total population\",\n  \"Median Age\", \"B01002_001\", \"Median age of population\", \n  \"Median HH Income\", \"B19013_001\", \"Median household income\",\n  \"White Population\", \"B03002_003\", \"White alone population\",\n  \"Black Population\", \"B03002_004\", \"Black/African American alone\",\n  \"Hispanic Population\", \"B03002_012\", \"Hispanic or Latino population\",\n  \"Bachelor's Degree\", \"B15003_022\", \"Bachelor's degree or higher\",\n  \"Median Rent\", \"B25058_001\", \"Median contract rent\",\n  \"Median Home Value\", \"B25077_001\", \"Median value owner-occupied\"\n)\n\nprint(\"Common Census Variables:\")\n\n\n[1] \"Common Census Variables:\"\n\n\nCode\ncommon_variables\n\n\n# A tibble: 9 × 3\n  concept             typical_code description                  \n  &lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;                        \n1 Total Population    B01003_001   Total population             \n2 Median Age          B01002_001   Median age of population     \n3 Median HH Income    B19013_001   Median household income      \n4 White Population    B03002_003   White alone population       \n5 Black Population    B03002_004   Black/African American alone \n6 Hispanic Population B03002_012   Hispanic or Latino population\n7 Bachelor's Degree   B15003_022   Bachelor's degree or higher  \n8 Median Rent         B25058_001   Median contract rent         \n9 Median Home Value   B25077_001   Median value owner-occupied  \n\n\nKey Tips for Variable Hunting:\n\nStart with concepts - search for the topic you want (income, age, housing)\nLook for “Median” vs “Mean” - median is usually more policy-relevant\nCheck the universe - some variables are for “households,” others for “population”\nTest with small data before running large queries\nBookmark useful variables for future projects (type them in your weekly notes!)"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-1-single-variable-eda",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-1-single-variable-eda",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n\nCode\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste(\", \", state_choice)))\n\n# Basic inspection\nglimpse(county_data)\n\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n\nCode\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n\nCode\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nCode\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n\n[1] \"Outlier counties:\"\n\n\nCode\nincome_outliers\n\n\n# A tibble: 3 × 2\n  county_name                     median_incomeE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Bucks County, Pennsylvania              107826\n2 Chester County, Pennsylvania            118574\n3 Montgomery County, Pennsylvania         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n\nCode\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Population\",\n    x = \"Median Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\nCode\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = total_popE) +\n  geom_boxplot(fill = \"lightgreen\", width = 0.5) +\n  labs(\n    title = \"Median Population Distribution with Outliers\",\n    y = \"Median Population\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\nCode\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(total_popE, 0.25, na.rm = TRUE),\n    Q3 = quantile(total_popE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = total_popE &lt; (Q1 - 1.5 * IQR) | total_popE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, total_popE)\n\nprint(\"Outlier counties:\")\n\n\n[1] \"Outlier counties:\"\n\n\nCode\nincome_outliers\n\n\n# A tibble: 7 × 2\n  county_name                       total_popE\n  &lt;chr&gt;                                  &lt;dbl&gt;\n1 Allegheny County, Pennsylvania       1245310\n2 Bucks County, Pennsylvania            645163\n3 Chester County, Pennsylvania          536474\n4 Delaware County, Pennsylvania         575312\n5 Lancaster County, Pennsylvania        553202\n6 Montgomery County, Pennsylvania       856399\n7 Philadelphia County, Pennsylvania    1593208"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-2-two-variable-relationships",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-2-two-variable-relationships",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n\nCode\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n\nCode\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n\nCode\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship?\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\n\nCode\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(, color = \"red\", alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"darkgreen\") +\n  labs(\n    title = \"Median Age vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Median Age\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_dark() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate correlation\ncorrelation &lt;- cor(county_data$median_ageE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n\n[1] \"Correlation coefficient: -0.317\""
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-3-data-quality-visualization",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-3-data-quality-visualization",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n\nCode\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n\n\n\nCode\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n\nCode\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\n\nCode\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    age_moe_pct = (median_ageM / median_ageE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = age_moe_pct) +\n  geom_point(color = \"purple\", size = 2, alpha = 0.7) +\n  geom_hline(yintercept = 5, color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Median Age\",\n    x = \"Median Age\",\n    y = \"Margin of Error (%)\",\n    caption = \"Blue Line = 5% Reliability Threshold\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nCode\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = age_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by Age MOE\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_classic() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n\nCode\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n\nCode\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile?\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-5-data-joins-and-integration",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-5-data-joins-and-integration",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n\nCode\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n\nCode\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\n\nOriginal data rows: 67 \n\n\nCode\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\n\nCombined data rows: 67 \n\n\nCode\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\n\nMissing education data: 0 \n\n\nCode\n# View the combined data\nhead(combined_data)\n\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n\nCode\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n\nCode\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\n\nhead(full_data)\n\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n\nCode\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-6-publication-ready-visualization",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-6-publication-ready-visualization",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n\nCode\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\np2\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np3\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Plot 4: Data reliability by population\nincome_moe_pct = (county_data$median_incomeM / county_data$median_incomeE) * 100\n\ncounty_reliability\n\n\n# A tibble: 67 × 11\n   GEOID NAME    total_popE total_popM median_incomeE median_incomeM median_ageE\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n 1 42001 Adams …     104604         NA          78975           3334        43.8\n 2 42003 Allegh…    1245310         NA          72537            869        40.6\n 3 42005 Armstr…      65538         NA          61011           2202        47  \n 4 42007 Beaver…     167629         NA          67194           1531        44.9\n 5 42009 Bedfor…      47613         NA          58337           2606        47.3\n 6 42011 Berks …     428483         NA          74617           1191        39.9\n 7 42013 Blair …     122640         NA          59386           2058        42.9\n 8 42015 Bradfo…      60159         NA          60650           2167        43.9\n 9 42017 Bucks …     645163         NA         107826           1516        44  \n10 42019 Butler…     194562         NA          82932           2164        43.4\n# ℹ 57 more rows\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name &lt;chr&gt;, age_moe_pct &lt;dbl&gt;,\n#   pop_category &lt;chr&gt;\n\n\nCode\np4 &lt;- ggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\np4\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\n\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "weekly-notes/week-03/week-03-class-lab.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n\nCode\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n\nCode\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\n\nCode\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n\nCode\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "weekly-notes/week-03/week-03-class-lab.html#key-references-and-acknowledgments",
    "href": "weekly-notes/week-03/week-03-class-lab.html#key-references-and-acknowledgments",
    "title": "EXPLORATORY DATA VISUALIZATION (EDA) & VISUALIZATION",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html",
    "href": "weekly-notes/week-04/week-04-class-lab.html",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\ndistricts &lt;- st_read(\"data/districts.geojson\")\n\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\n\nCode\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\n\nReading layer `hospitals' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\n\nCode\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))\n\n\n\n\n\n\nGoal: Practice spatial filtering with different predicates\n\n\nYour Task: Choose any PA county and find all counties that border it.\n\n\nCode\n# Step 1: Look at available county names\nunique(pa_counties$COUNTY_NAM)\n\n\n [1] \"MONTGOMERY\"     \"BRADFORD\"       \"BUCKS\"          \"TIOGA\"         \n [5] \"UNION\"          \"VENANGO\"        \"WASHINGTON\"     \"WAYNE\"         \n [9] \"MCKEAN\"         \"MERCER\"         \"MIFFLIN\"        \"MONTOUR\"       \n[13] \"NORTHAMPTON\"    \"NORTHUMBERLAND\" \"PERRY\"          \"PIKE\"          \n[17] \"POTTER\"         \"SCHUYLKILL\"     \"SNYDER\"         \"SOMERSET\"      \n[21] \"SULLIVAN\"       \"LEBANON\"        \"BUTLER\"         \"CAMBRIA\"       \n[25] \"CAMERON\"        \"CARBON\"         \"CENTRE\"         \"CLARION\"       \n[29] \"CLEARFIELD\"     \"CLINTON\"        \"COLUMBIA\"       \"CRAWFORD\"      \n[33] \"CUMBERLAND\"     \"DAUPHIN\"        \"INDIANA\"        \"JEFFERSON\"     \n[37] \"JUNIATA\"        \"LANCASTER\"      \"WESTMORELAND\"   \"WYOMING\"       \n[41] \"YORK\"           \"PHILADELPHIA\"   \"LEHIGH\"         \"LUZERNE\"       \n[45] \"LYCOMING\"       \"LAWRENCE\"       \"DELAWARE\"       \"ELK\"           \n[49] \"ERIE\"           \"FAYETTE\"        \"FOREST\"         \"FRANKLIN\"      \n[53] \"FULTON\"         \"GREENE\"         \"HUNTINGDON\"     \"ADAMS\"         \n[57] \"ALLEGHENY\"      \"ARMSTRONG\"      \"BEAVER\"         \"BEDFORD\"       \n[61] \"BLAIR\"          \"SUSQUEHANNA\"    \"WARREN\"         \"BERKS\"         \n[65] \"CHESTER\"        \"MONROE\"         \"LACKAWANNA\"    \n\n\nCode\n# Step 2: Pick one county (change this to your choice!)\nmy_county &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"BLAIR\") # Change \"CENTRE\" to your county\n\n# Step 3: Find neighbors using st_touches\nmy_neighbors &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_touches)\n\n# Step 4: How many neighbors does your county have?\ncat(\"Number of neighboring counties:\", nrow(my_neighbors), \"\\n\")\n\n\nNumber of neighboring counties: 5 \n\n\nCode\nprint(\"Neighbor names:\")\n\n\n[1] \"Neighbor names:\"\n\n\nCode\nprint(my_neighbors$COUNTY_NAM)\n\n\n[1] \"CAMBRIA\"    \"CENTRE\"     \"CLEARFIELD\" \"HUNTINGDON\" \"BEDFORD\"   \n\n\n\n\n\nYour Task: Create a map showing your county and its neighbors in different colors.\n\n\nCode\n# Create the map\nggplot() +\n  geom_sf(data = pa_counties, fill = \"lightgray\", color = \"white\") +\n  geom_sf(data = my_neighbors, fill = \"lightblue\", alpha = 0.7) +\n  geom_sf(data = my_county, fill = \"darkblue\") +\n  labs(\n    title = paste(\"Neighbors of\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = paste(nrow(my_neighbors), \"neighboring counties\")\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: What happens if you use st_intersects instead of st_touches? Why is the count different?\n\n\nCode\n# Use st_intersects\nintersecting_counties &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_intersects)\n\ncat(\"With st_touches:\", nrow(my_neighbors), \"counties\\n\")\n\n\nWith st_touches: 5 counties\n\n\nCode\ncat(\"With st_intersects:\", nrow(intersecting_counties), \"counties\\n\")\n\n\nWith st_intersects: 6 counties\n\n\nCode\ncat(\"Difference:\", nrow(intersecting_counties) - nrow(my_neighbors), \"\\n\")\n\n\nDifference: 1 \n\n\nQuestion: Why is there a difference of 1? What does this tell you about the difference between st_touches and st_intersects?\nThe difference of 1 is likely because st_intersects includes the county chosen as opposed to st_touches, which excludes the chosen county Blair.\n\n\n\n\n\nGoal: Practice buffering and measuring accessibility\n\n\nYour Task: Create 15-mile (24140 meter) service areas around all hospitals in your county.\n\n\nCode\n# Step 1: Filter hospitals in your county\n# First do a spatial join to assign counties to hospitals\nhospitals_with_county &lt;- hospitals %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Filter for your county's hospitals\nmy_county_hospitals &lt;- hospitals_with_county %&gt;%\n  filter(COUNTY_NAM == \"BLAIR\") # Change to match your county\n\ncat(\"Number of hospitals in county:\", nrow(my_county_hospitals), \"\\n\")\n\n\nNumber of hospitals in county: 4 \n\n\nCode\n# Step 2: Project to accurate CRS for buffering\nmy_county_hospitals_proj &lt;- my_county_hospitals %&gt;%\n  st_transform(3365)  # Pennsylvania State Plane South\n\n# Step 3: Create 15-mile buffers (24140 meters = 15 miles)\nhospital_service_areas &lt;- my_county_hospitals_proj %&gt;%\n  st_buffer(dist = 79200)  # 15 miles in feet for PA State Plane\n\n# Step 4: Transform back for mapping\nhospital_service_areas &lt;- st_transform(hospital_service_areas, st_crs(pa_counties))\n\n\n\n\n\nYour Task: Create a map showing hospitals and their service areas.\n\n\nCode\nggplot() +\n  geom_sf(data = my_county, fill = \"white\", color = \"black\") +\n  geom_sf(data = hospital_service_areas, fill = \"lightblue\", alpha = 0.3) +\n  geom_sf(data = my_county_hospitals, color = \"red\", size = 2) +\n  labs(\n    title = paste(\"Hospital Service Areas in\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = \"Red points = Hospitals, Blue areas = 15-mile service zones\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: What percentage of your county is within 15 miles of a hospital?\n\n\nCode\n# Union all service areas into one polygon\ncombined_service_area &lt;- hospital_service_areas %&gt;%\n  st_union()\n\n# Calculate areas (need to be in projected CRS)\nmy_county_proj &lt;- st_transform(my_county, 3365)\ncombined_service_proj &lt;- st_transform(combined_service_area, 3365)\n\n# Find intersection\ncoverage_area &lt;- st_intersection(my_county_proj, combined_service_proj)\n\n# Calculate percentages\ncounty_area &lt;- as.numeric(st_area(my_county_proj))\ncovered_area &lt;- as.numeric(st_area(coverage_area))\ncoverage_pct &lt;- (covered_area / county_area) * 100\n\ncat(\"County area:\", round(county_area / 1e6, 1), \"sq km\\n\")\n\n\nCounty area: 14690.2 sq km\n\n\nCode\ncat(\"Covered area:\", round(covered_area / 1e6, 1), \"sq km\\n\")\n\n\nCovered area: 14690.2 sq km\n\n\nCode\ncat(\"Coverage:\", round(coverage_pct, 1), \"%\\n\")\n\n\nCoverage: 100 %\n\n\nQuestion: Is your county well-served by hospitals? What areas might be underserved?\nThe county seems to be well-served, but looking at the coverage of Blair’s surrounding counties could be a good further step because this doesn’t account for boundary spillover; people generally don’t confine the services they seek strictly within county or state boundaries.\n\n\n\n\n\nGoal: Practice spatial joins and aggregation\n\n\nYour Task: Figure out which congressional districts overlap with each county.\n\n\nCode\n# Spatial join: districts to counties\ndistricts_by_county &lt;- districts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_districts = n_distinct(OBJECTID),\n    district_ids = paste(unique(MSLINK), collapse = \", \")\n  ) %&gt;%\n  arrange(desc(n_districts))\n\n# Which counties have the most districts?\nhead(districts_by_county, 10)\n\n\n# A tibble: 10 × 3\n   COUNTY_NAM   n_districts district_ids            \n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                   \n 1 MONTGOMERY             7 19, 2, 14, 20, 4, 15, 21\n 2 BERKS                  6 8, 19, 2, 14, 4, 5      \n 3 ALLEGHENY              5 11, 12, 17, 1, 7        \n 4 PHILADELPHIA           5 19, 14, 20, 15, 21      \n 5 WESTMORELAND           5 11, 12, 17, 3, 7        \n 6 BUCKS                  4 19, 2, 14, 20           \n 7 CHESTER                4 14, 4, 5, 15            \n 8 DAUPHIN                4 8, 3, 5, 6              \n 9 JUNIATA                4 8, 12, 3, 6             \n10 LANCASTER              4 8, 4, 5, 6              \n\n\n\n\n\nYour Task: Get demographic data for census tracts and aggregate to districts.\n\n\nCode\n# Get tract-level demographics\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    white_pop = \"B03002_003\",\n    black_pop = \"B03002_004\",\n    hispanic_pop = \"B03002_012\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(tract_demographics, by = \"GEOID\")\n\n# Spatial join to districts and aggregate\ndistrict_demographics &lt;- tracts_with_data %&gt;%\n  st_join(districts) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(OBJECTID, MSLINK) %&gt;%\n  summarize(\n    total_population = sum(total_popE, na.rm = TRUE),\n    median_income = weighted.mean(median_incomeE, total_popE, na.rm = TRUE),\n    pct_white = sum(white_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_black = sum(black_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_hispanic = sum(hispanic_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    n_tracts = n()\n  ) %&gt;%\n  arrange(desc(total_population))\n\n# Show results\nhead(district_demographics, 10)\n\n\n# A tibble: 10 × 8\n# Groups:   OBJECTID [10]\n   OBJECTID MSLINK total_population median_income pct_white pct_black\n      &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1      113     14          1229246       107602.      72.0     11.0 \n 2      108     11          1010001        76612.      76.6     13.4 \n 3      120      7          1006212        88796.      83.3      8.15\n 4      107      8           993966        70241.      88.7      2.24\n 5      117      3           986114        69017.      91.0      2.25\n 6      109     12           979419        63952.      92.2      1.81\n 7      118      4           974715       111250.      73.0      5.00\n 8      114     17           972225        71139.      91.8      2.75\n 9      110     19           932212       113798.      80.7      3.79\n10      123      6           927718        80080.      75.6      8.65\n# ℹ 2 more variables: pct_hispanic &lt;dbl&gt;, n_tracts &lt;int&gt;\n\n\n\n\n\nYour Task: Create a choropleth map of median income by congressional district.\n\n\nCode\n# Join demographics back to district boundaries\ndistricts_with_demographics &lt;- districts %&gt;%\n  left_join(district_demographics, by = \"OBJECTID\")\n\n# Create the map\nggplot(districts_with_demographics) +\n  geom_sf(aes(fill = median_income), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Median\\nIncome\",\n    labels = dollar,\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"Median Household Income by Congressional District\",\n    subtitle = \"Pennsylvania\",\n    caption = \"Source: ACS 2018-2022\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: Which districts are the most racially diverse?\n\n\nCode\n# Calculate diversity index (simple version: higher = more diverse)\n# A perfectly even distribution would be ~33% each for 3 groups\ndistrict_demographics &lt;- district_demographics %&gt;%\n  mutate(\n    diversity_score = 100 - abs(pct_white - 33.3) - abs(pct_black - 33.3) - abs(pct_hispanic - 33.3)\n  ) %&gt;%\n  arrange(desc(diversity_score))\n\n# Most diverse districts\nhead(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      115     20      37.9     25.4         23.7             77.8\n2      122     21      33.5     49.7          5.73            55.9\n3      121     15      58.8     24.4          5.77            38.1\n4      111      2      71.2      4.88        18.3             18.7\n5      113     14      72.0     11.0          7.09            12.8\n\n\nCode\n# Least diverse districts\ntail(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      107      8      88.7      2.24         5.73           -14.1\n2      116      1      89.3      3.66         2.53           -16.4\n3      117      3      91.0      2.25         3.24           -18.8\n4      114     17      91.8      2.75         1.49           -20.9\n5      109     12      92.2      1.81         2.09           -21.6\n\n\n\n\n\n\n\nGoal: Understand how CRS affects calculations\n\n\nYour Task: Calculate county areas using different coordinate systems and compare.\n\n\nCode\n# Calculate areas in different CRS\narea_comparison &lt;- pa_counties %&gt;%\n  # Geographic (WGS84) - WRONG for areas!\n  st_transform(4326) %&gt;%\n  mutate(area_geographic = as.numeric(st_area(.))) %&gt;%\n  # PA State Plane South - Good for PA\n  st_transform(3365) %&gt;%\n  mutate(area_state_plane = as.numeric(st_area(.))) %&gt;%\n  # Albers Equal Area - Good for areas\n  st_transform(5070) %&gt;%\n  mutate(area_albers = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(COUNTY_NAM, starts_with(\"area_\")) %&gt;%\n  mutate(\n    # Calculate errors compared to Albers (most accurate for area)\n    error_geographic_pct = abs(area_geographic - area_albers) / area_albers * 100,\n    error_state_plane_pct = abs(area_state_plane - area_albers) / area_state_plane * 100\n  )\n\n# Show counties with biggest errors\narea_comparison %&gt;%\n  arrange(desc(error_geographic_pct)) %&gt;%\n  select(COUNTY_NAM, error_geographic_pct, error_state_plane_pct) %&gt;%\n  head(10)\n\n\n    COUNTY_NAM error_geographic_pct error_state_plane_pct\n1         ERIE            0.1520567              90.71567\n2  SUSQUEHANNA            0.1480129              90.71426\n3       MCKEAN            0.1479719              90.71414\n4       WARREN            0.1478826              90.71421\n5     BRADFORD            0.1473177              90.71402\n6        TIOGA            0.1469541              90.71390\n7       POTTER            0.1463317              90.71371\n8     CRAWFORD            0.1447572              90.71326\n9        WAYNE            0.1440222              90.71306\n10     WYOMING            0.1409741              90.71215\n\n\n\n\n\nYour Task: Map which counties have the biggest area calculation errors.\n\n\nCode\n# Join error data back to counties\ncounties_with_errors &lt;- pa_counties %&gt;%\n  left_join(\n    area_comparison %&gt;% select(COUNTY_NAM, error_geographic_pct),\n    by = \"COUNTY_NAM\"\n  )\n\n# Map the error\nggplot(counties_with_errors) +\n  geom_sf(aes(fill = error_geographic_pct), color = \"white\") +\n  scale_fill_viridis_c(\n    name = \"Area\\nError %\",\n    option = \"magma\"\n  ) +\n  labs(\n    title = \"Area Calculation Errors by County\",\n    subtitle = \"Using geographic coordinates (WGS84)\\ninstead of projected CRS\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nQuestion: Which counties have the largest errors? Why might this be?\nCounties toward the north of Pennsylvania have the largest errors, likely because the CRS was a 3-dimensional GCS (WGS84) when the area calculations were done. Geographic coordinate systems have more warping at the poles when displayed on a 2-dimensional surface when it comes to area as a cost to preserve other geometric factors.\n\n\n\n\n\nGoal: Combine multiple operations for a complex policy question\n\n\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour Task: Combine what you’ve learned to identify vulnerable, underserved communities.\nSteps: 1. Get demographic (elderly and income) data for census tracts 2. Identify vulnerable tracts (low income AND high elderly population) 3. Calculate distance to nearest hospital 4. Check which ones are more than 15 miles from a hospital 5. Aggregate to county level 6. Create comprehensive map 7. Create a summary table\n\n\n\n\n\nAfter completing these exercises, reflect on:\n\nWhen did you need to transform CRS? Why was this necessary?\nWhat’s the difference between st_filter() and st_intersection()? When would you use each?\nHow does the choice of predicate (st_touches, st_intersects, st_within) change your results?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample Use\n\n\n\n\nst_filter()\nSelect features by spatial relationship\nFind neighboring counties\n\n\nst_buffer()\nCreate zones around features\nHospital service areas\n\n\nst_intersects()\nTest spatial overlap\nCheck access to services\n\n\nst_disjoint()\nTest spatial separation\nFind rural areas\n\n\nst_join()\nJoin by location\nAdd county info to tracts\n\n\nst_union()\nCombine geometries\nMerge overlapping buffers\n\n\nst_intersection()\nClip geometries\nCalculate overlap areas\n\n\nst_transform()\nChange CRS\nAccurate distance/area calculations\n\n\nst_area()\nCalculate areas\nCounty sizes, coverage\n\n\nst_distance()\nCalculate distances\nDistance to facilities\n\n\n\nImportant Reminder: Always check and standardize CRS when working with spatial data from multiple sources!"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#setup",
    "href": "weekly-notes/week-04/week-04-class-lab.html#setup",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Code\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"3aaee31789e10b674a531e9f236c35d5394b19ed\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\ndistricts &lt;- st_read(\"data/districts.geojson\")\n\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\n\nCode\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\n\nReading layer `hospitals' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\n\nCode\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "href": "weekly-notes/week-04/week-04-class-lab.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Goal: Practice spatial filtering with different predicates\n\n\nYour Task: Choose any PA county and find all counties that border it.\n\n\nCode\n# Step 1: Look at available county names\nunique(pa_counties$COUNTY_NAM)\n\n\n [1] \"MONTGOMERY\"     \"BRADFORD\"       \"BUCKS\"          \"TIOGA\"         \n [5] \"UNION\"          \"VENANGO\"        \"WASHINGTON\"     \"WAYNE\"         \n [9] \"MCKEAN\"         \"MERCER\"         \"MIFFLIN\"        \"MONTOUR\"       \n[13] \"NORTHAMPTON\"    \"NORTHUMBERLAND\" \"PERRY\"          \"PIKE\"          \n[17] \"POTTER\"         \"SCHUYLKILL\"     \"SNYDER\"         \"SOMERSET\"      \n[21] \"SULLIVAN\"       \"LEBANON\"        \"BUTLER\"         \"CAMBRIA\"       \n[25] \"CAMERON\"        \"CARBON\"         \"CENTRE\"         \"CLARION\"       \n[29] \"CLEARFIELD\"     \"CLINTON\"        \"COLUMBIA\"       \"CRAWFORD\"      \n[33] \"CUMBERLAND\"     \"DAUPHIN\"        \"INDIANA\"        \"JEFFERSON\"     \n[37] \"JUNIATA\"        \"LANCASTER\"      \"WESTMORELAND\"   \"WYOMING\"       \n[41] \"YORK\"           \"PHILADELPHIA\"   \"LEHIGH\"         \"LUZERNE\"       \n[45] \"LYCOMING\"       \"LAWRENCE\"       \"DELAWARE\"       \"ELK\"           \n[49] \"ERIE\"           \"FAYETTE\"        \"FOREST\"         \"FRANKLIN\"      \n[53] \"FULTON\"         \"GREENE\"         \"HUNTINGDON\"     \"ADAMS\"         \n[57] \"ALLEGHENY\"      \"ARMSTRONG\"      \"BEAVER\"         \"BEDFORD\"       \n[61] \"BLAIR\"          \"SUSQUEHANNA\"    \"WARREN\"         \"BERKS\"         \n[65] \"CHESTER\"        \"MONROE\"         \"LACKAWANNA\"    \n\n\nCode\n# Step 2: Pick one county (change this to your choice!)\nmy_county &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"BLAIR\") # Change \"CENTRE\" to your county\n\n# Step 3: Find neighbors using st_touches\nmy_neighbors &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_touches)\n\n# Step 4: How many neighbors does your county have?\ncat(\"Number of neighboring counties:\", nrow(my_neighbors), \"\\n\")\n\n\nNumber of neighboring counties: 5 \n\n\nCode\nprint(\"Neighbor names:\")\n\n\n[1] \"Neighbor names:\"\n\n\nCode\nprint(my_neighbors$COUNTY_NAM)\n\n\n[1] \"CAMBRIA\"    \"CENTRE\"     \"CLEARFIELD\" \"HUNTINGDON\" \"BEDFORD\"   \n\n\n\n\n\nYour Task: Create a map showing your county and its neighbors in different colors.\n\n\nCode\n# Create the map\nggplot() +\n  geom_sf(data = pa_counties, fill = \"lightgray\", color = \"white\") +\n  geom_sf(data = my_neighbors, fill = \"lightblue\", alpha = 0.7) +\n  geom_sf(data = my_county, fill = \"darkblue\") +\n  labs(\n    title = paste(\"Neighbors of\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = paste(nrow(my_neighbors), \"neighboring counties\")\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: What happens if you use st_intersects instead of st_touches? Why is the count different?\n\n\nCode\n# Use st_intersects\nintersecting_counties &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_intersects)\n\ncat(\"With st_touches:\", nrow(my_neighbors), \"counties\\n\")\n\n\nWith st_touches: 5 counties\n\n\nCode\ncat(\"With st_intersects:\", nrow(intersecting_counties), \"counties\\n\")\n\n\nWith st_intersects: 6 counties\n\n\nCode\ncat(\"Difference:\", nrow(intersecting_counties) - nrow(my_neighbors), \"\\n\")\n\n\nDifference: 1 \n\n\nQuestion: Why is there a difference of 1? What does this tell you about the difference between st_touches and st_intersects?\nThe difference of 1 is likely because st_intersects includes the county chosen as opposed to st_touches, which excludes the chosen county Blair."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#exercise-2-hospital-service-areas-15-minutes",
    "href": "weekly-notes/week-04/week-04-class-lab.html#exercise-2-hospital-service-areas-15-minutes",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Goal: Practice buffering and measuring accessibility\n\n\nYour Task: Create 15-mile (24140 meter) service areas around all hospitals in your county.\n\n\nCode\n# Step 1: Filter hospitals in your county\n# First do a spatial join to assign counties to hospitals\nhospitals_with_county &lt;- hospitals %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Filter for your county's hospitals\nmy_county_hospitals &lt;- hospitals_with_county %&gt;%\n  filter(COUNTY_NAM == \"BLAIR\") # Change to match your county\n\ncat(\"Number of hospitals in county:\", nrow(my_county_hospitals), \"\\n\")\n\n\nNumber of hospitals in county: 4 \n\n\nCode\n# Step 2: Project to accurate CRS for buffering\nmy_county_hospitals_proj &lt;- my_county_hospitals %&gt;%\n  st_transform(3365)  # Pennsylvania State Plane South\n\n# Step 3: Create 15-mile buffers (24140 meters = 15 miles)\nhospital_service_areas &lt;- my_county_hospitals_proj %&gt;%\n  st_buffer(dist = 79200)  # 15 miles in feet for PA State Plane\n\n# Step 4: Transform back for mapping\nhospital_service_areas &lt;- st_transform(hospital_service_areas, st_crs(pa_counties))\n\n\n\n\n\nYour Task: Create a map showing hospitals and their service areas.\n\n\nCode\nggplot() +\n  geom_sf(data = my_county, fill = \"white\", color = \"black\") +\n  geom_sf(data = hospital_service_areas, fill = \"lightblue\", alpha = 0.3) +\n  geom_sf(data = my_county_hospitals, color = \"red\", size = 2) +\n  labs(\n    title = paste(\"Hospital Service Areas in\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = \"Red points = Hospitals, Blue areas = 15-mile service zones\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: What percentage of your county is within 15 miles of a hospital?\n\n\nCode\n# Union all service areas into one polygon\ncombined_service_area &lt;- hospital_service_areas %&gt;%\n  st_union()\n\n# Calculate areas (need to be in projected CRS)\nmy_county_proj &lt;- st_transform(my_county, 3365)\ncombined_service_proj &lt;- st_transform(combined_service_area, 3365)\n\n# Find intersection\ncoverage_area &lt;- st_intersection(my_county_proj, combined_service_proj)\n\n# Calculate percentages\ncounty_area &lt;- as.numeric(st_area(my_county_proj))\ncovered_area &lt;- as.numeric(st_area(coverage_area))\ncoverage_pct &lt;- (covered_area / county_area) * 100\n\ncat(\"County area:\", round(county_area / 1e6, 1), \"sq km\\n\")\n\n\nCounty area: 14690.2 sq km\n\n\nCode\ncat(\"Covered area:\", round(covered_area / 1e6, 1), \"sq km\\n\")\n\n\nCovered area: 14690.2 sq km\n\n\nCode\ncat(\"Coverage:\", round(coverage_pct, 1), \"%\\n\")\n\n\nCoverage: 100 %\n\n\nQuestion: Is your county well-served by hospitals? What areas might be underserved?\nThe county seems to be well-served, but looking at the coverage of Blair’s surrounding counties could be a good further step because this doesn’t account for boundary spillover; people generally don’t confine the services they seek strictly within county or state boundaries."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#exercise-3-congressional-district-analysis-15-minutes",
    "href": "weekly-notes/week-04/week-04-class-lab.html#exercise-3-congressional-district-analysis-15-minutes",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Goal: Practice spatial joins and aggregation\n\n\nYour Task: Figure out which congressional districts overlap with each county.\n\n\nCode\n# Spatial join: districts to counties\ndistricts_by_county &lt;- districts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_districts = n_distinct(OBJECTID),\n    district_ids = paste(unique(MSLINK), collapse = \", \")\n  ) %&gt;%\n  arrange(desc(n_districts))\n\n# Which counties have the most districts?\nhead(districts_by_county, 10)\n\n\n# A tibble: 10 × 3\n   COUNTY_NAM   n_districts district_ids            \n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                   \n 1 MONTGOMERY             7 19, 2, 14, 20, 4, 15, 21\n 2 BERKS                  6 8, 19, 2, 14, 4, 5      \n 3 ALLEGHENY              5 11, 12, 17, 1, 7        \n 4 PHILADELPHIA           5 19, 14, 20, 15, 21      \n 5 WESTMORELAND           5 11, 12, 17, 3, 7        \n 6 BUCKS                  4 19, 2, 14, 20           \n 7 CHESTER                4 14, 4, 5, 15            \n 8 DAUPHIN                4 8, 3, 5, 6              \n 9 JUNIATA                4 8, 12, 3, 6             \n10 LANCASTER              4 8, 4, 5, 6              \n\n\n\n\n\nYour Task: Get demographic data for census tracts and aggregate to districts.\n\n\nCode\n# Get tract-level demographics\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    white_pop = \"B03002_003\",\n    black_pop = \"B03002_004\",\n    hispanic_pop = \"B03002_012\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(tract_demographics, by = \"GEOID\")\n\n# Spatial join to districts and aggregate\ndistrict_demographics &lt;- tracts_with_data %&gt;%\n  st_join(districts) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(OBJECTID, MSLINK) %&gt;%\n  summarize(\n    total_population = sum(total_popE, na.rm = TRUE),\n    median_income = weighted.mean(median_incomeE, total_popE, na.rm = TRUE),\n    pct_white = sum(white_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_black = sum(black_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_hispanic = sum(hispanic_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    n_tracts = n()\n  ) %&gt;%\n  arrange(desc(total_population))\n\n# Show results\nhead(district_demographics, 10)\n\n\n# A tibble: 10 × 8\n# Groups:   OBJECTID [10]\n   OBJECTID MSLINK total_population median_income pct_white pct_black\n      &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1      113     14          1229246       107602.      72.0     11.0 \n 2      108     11          1010001        76612.      76.6     13.4 \n 3      120      7          1006212        88796.      83.3      8.15\n 4      107      8           993966        70241.      88.7      2.24\n 5      117      3           986114        69017.      91.0      2.25\n 6      109     12           979419        63952.      92.2      1.81\n 7      118      4           974715       111250.      73.0      5.00\n 8      114     17           972225        71139.      91.8      2.75\n 9      110     19           932212       113798.      80.7      3.79\n10      123      6           927718        80080.      75.6      8.65\n# ℹ 2 more variables: pct_hispanic &lt;dbl&gt;, n_tracts &lt;int&gt;\n\n\n\n\n\nYour Task: Create a choropleth map of median income by congressional district.\n\n\nCode\n# Join demographics back to district boundaries\ndistricts_with_demographics &lt;- districts %&gt;%\n  left_join(district_demographics, by = \"OBJECTID\")\n\n# Create the map\nggplot(districts_with_demographics) +\n  geom_sf(aes(fill = median_income), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Median\\nIncome\",\n    labels = dollar,\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"Median Household Income by Congressional District\",\n    subtitle = \"Pennsylvania\",\n    caption = \"Source: ACS 2018-2022\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nYour Task: Which districts are the most racially diverse?\n\n\nCode\n# Calculate diversity index (simple version: higher = more diverse)\n# A perfectly even distribution would be ~33% each for 3 groups\ndistrict_demographics &lt;- district_demographics %&gt;%\n  mutate(\n    diversity_score = 100 - abs(pct_white - 33.3) - abs(pct_black - 33.3) - abs(pct_hispanic - 33.3)\n  ) %&gt;%\n  arrange(desc(diversity_score))\n\n# Most diverse districts\nhead(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      115     20      37.9     25.4         23.7             77.8\n2      122     21      33.5     49.7          5.73            55.9\n3      121     15      58.8     24.4          5.77            38.1\n4      111      2      71.2      4.88        18.3             18.7\n5      113     14      72.0     11.0          7.09            12.8\n\n\nCode\n# Least diverse districts\ntail(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      107      8      88.7      2.24         5.73           -14.1\n2      116      1      89.3      3.66         2.53           -16.4\n3      117      3      91.0      2.25         3.24           -18.8\n4      114     17      91.8      2.75         1.49           -20.9\n5      109     12      92.2      1.81         2.09           -21.6"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#exercise-5-projection-effects-10-minutes",
    "href": "weekly-notes/week-04/week-04-class-lab.html#exercise-5-projection-effects-10-minutes",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Goal: Understand how CRS affects calculations\n\n\nYour Task: Calculate county areas using different coordinate systems and compare.\n\n\nCode\n# Calculate areas in different CRS\narea_comparison &lt;- pa_counties %&gt;%\n  # Geographic (WGS84) - WRONG for areas!\n  st_transform(4326) %&gt;%\n  mutate(area_geographic = as.numeric(st_area(.))) %&gt;%\n  # PA State Plane South - Good for PA\n  st_transform(3365) %&gt;%\n  mutate(area_state_plane = as.numeric(st_area(.))) %&gt;%\n  # Albers Equal Area - Good for areas\n  st_transform(5070) %&gt;%\n  mutate(area_albers = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(COUNTY_NAM, starts_with(\"area_\")) %&gt;%\n  mutate(\n    # Calculate errors compared to Albers (most accurate for area)\n    error_geographic_pct = abs(area_geographic - area_albers) / area_albers * 100,\n    error_state_plane_pct = abs(area_state_plane - area_albers) / area_state_plane * 100\n  )\n\n# Show counties with biggest errors\narea_comparison %&gt;%\n  arrange(desc(error_geographic_pct)) %&gt;%\n  select(COUNTY_NAM, error_geographic_pct, error_state_plane_pct) %&gt;%\n  head(10)\n\n\n    COUNTY_NAM error_geographic_pct error_state_plane_pct\n1         ERIE            0.1520567              90.71567\n2  SUSQUEHANNA            0.1480129              90.71426\n3       MCKEAN            0.1479719              90.71414\n4       WARREN            0.1478826              90.71421\n5     BRADFORD            0.1473177              90.71402\n6        TIOGA            0.1469541              90.71390\n7       POTTER            0.1463317              90.71371\n8     CRAWFORD            0.1447572              90.71326\n9        WAYNE            0.1440222              90.71306\n10     WYOMING            0.1409741              90.71215\n\n\n\n\n\nYour Task: Map which counties have the biggest area calculation errors.\n\n\nCode\n# Join error data back to counties\ncounties_with_errors &lt;- pa_counties %&gt;%\n  left_join(\n    area_comparison %&gt;% select(COUNTY_NAM, error_geographic_pct),\n    by = \"COUNTY_NAM\"\n  )\n\n# Map the error\nggplot(counties_with_errors) +\n  geom_sf(aes(fill = error_geographic_pct), color = \"white\") +\n  scale_fill_viridis_c(\n    name = \"Area\\nError %\",\n    option = \"magma\"\n  ) +\n  labs(\n    title = \"Area Calculation Errors by County\",\n    subtitle = \"Using geographic coordinates (WGS84)\\ninstead of projected CRS\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nQuestion: Which counties have the largest errors? Why might this be?\nCounties toward the north of Pennsylvania have the largest errors, likely because the CRS was a 3-dimensional GCS (WGS84) when the area calculations were done. Geographic coordinate systems have more warping at the poles when displayed on a 2-dimensional surface when it comes to area as a cost to preserve other geometric factors."
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#bonus-challenge-combined-analysis-if-time-permits",
    "href": "weekly-notes/week-04/week-04-class-lab.html#bonus-challenge-combined-analysis-if-time-permits",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Goal: Combine multiple operations for a complex policy question\n\n\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour Task: Combine what you’ve learned to identify vulnerable, underserved communities.\nSteps: 1. Get demographic (elderly and income) data for census tracts 2. Identify vulnerable tracts (low income AND high elderly population) 3. Calculate distance to nearest hospital 4. Check which ones are more than 15 miles from a hospital 5. Aggregate to county level 6. Create comprehensive map 7. Create a summary table"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#reflection-questions",
    "href": "weekly-notes/week-04/week-04-class-lab.html#reflection-questions",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "After completing these exercises, reflect on:\n\nWhen did you need to transform CRS? Why was this necessary?\nWhat’s the difference between st_filter() and st_intersection()? When would you use each?\nHow does the choice of predicate (st_touches, st_intersects, st_within) change your results?"
  },
  {
    "objectID": "weekly-notes/week-04/week-04-class-lab.html#summary-of-key-functions-used",
    "href": "weekly-notes/week-04/week-04-class-lab.html#summary-of-key-functions-used",
    "title": "SPATIAL DATA & GIS OPERATIONS IN R",
    "section": "",
    "text": "Function\nPurpose\nExample Use\n\n\n\n\nst_filter()\nSelect features by spatial relationship\nFind neighboring counties\n\n\nst_buffer()\nCreate zones around features\nHospital service areas\n\n\nst_intersects()\nTest spatial overlap\nCheck access to services\n\n\nst_disjoint()\nTest spatial separation\nFind rural areas\n\n\nst_join()\nJoin by location\nAdd county info to tracts\n\n\nst_union()\nCombine geometries\nMerge overlapping buffers\n\n\nst_intersection()\nClip geometries\nCalculate overlap areas\n\n\nst_transform()\nChange CRS\nAccurate distance/area calculations\n\n\nst_area()\nCalculate areas\nCounty sizes, coverage\n\n\nst_distance()\nCalculate distances\nDistance to facilities\n\n\n\nImportant Reminder: Always check and standardize CRS when working with spatial data from multiple sources!"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html",
    "href": "weekly-notes/week-05/week-05-class-lab.html",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "",
    "text": "Scenario: You’re advising a state agency on resource allocation.\nSome counties have sparse data. Can you predict their median income using data from counties with better measurements?\nBroader question: How do we make informed predictions when we don’t have complete information?"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#the-general-problem",
    "href": "weekly-notes/week-05/week-05-class-lab.html#the-general-problem",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "The General Problem",
    "text": "The General Problem\nWe observe data: counties, income, population, education, etc.\nWe believe there’s some relationship between these variables.\nStatistical learning = a set of approaches for estimating that relationship"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#formalizing-the-relationship",
    "href": "weekly-notes/week-05/week-05-class-lab.html#formalizing-the-relationship",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Formalizing the Relationship",
    "text": "Formalizing the Relationship\nFor any quantitative response Y and predictors X₁, X₂, … Xₚ:\n\\[Y = f(X) + \\epsilon\\]\nWhere:\n\nf = the systematic information X provides about Y\nε = random error (irreducible)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#what-is-f",
    "href": "weekly-notes/week-05/week-05-class-lab.html#what-is-f",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "What is f?",
    "text": "What is f?\nf represents the true relationship between predictors and outcome\n\nIt’s fixed but unknown\nIt’s what we’re trying to estimate\nDifferent X values produce different Y values through f\n\nExample:\n\nY = median income\nX = population, education, poverty rate\nf = the way these factors systematically relate to income"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#why-estimate-f",
    "href": "weekly-notes/week-05/week-05-class-lab.html#why-estimate-f",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Why Estimate f?",
    "text": "Why Estimate f?\nTwo main reasons:\n1. Prediction\n\nEstimate Y for new observations\nDon’t necessarily care about the exact form of f\nFocus: accuracy of predictions\n\n2. Inference\n\nUnderstand how X affects Y\nWhich predictors matter?\nWhat’s the nature of the relationship?\nFocus: interpreting the model"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#how-do-we-estimate-f",
    "href": "weekly-notes/week-05/week-05-class-lab.html#how-do-we-estimate-f",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "How Do We Estimate f?",
    "text": "How Do We Estimate f?\nTwo broad approaches:\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we’ll focus on\n\nNon-Parametric Methods\n\nDon’t assume a specific form\nMore flexible\nRequire more data\nHarder to interpret"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#parametric-vs.-non-parametric",
    "href": "weekly-notes/week-05/week-05-class-lab.html#parametric-vs.-non-parametric",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Parametric vs. Non-Parametric",
    "text": "Parametric vs. Non-Parametric\n\nKey difference:\n\nParametric (blue): We assume f is linear, then estimate β₀ and β₁\nNon-parametric (green): We let the data determine the shape of f\n\n\n\n\n\n\n\nNoteWhat about deep learning?\n\n\n\nNeural networks are technically parametric (millions of parameters!), but achieve flexibility through parameter quantity rather than assuming a rigid form. We won’t cover them in this course, but they follow the same Y = f(X) + ε framework."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#parametric-approach-linear-regression",
    "href": "weekly-notes/week-05/week-05-class-lab.html#parametric-approach-linear-regression",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Parametric Approach: Linear Regression",
    "text": "Parametric Approach: Linear Regression\nThe assumption: Relationship between X and Y is linear\n\\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p\\]\nThe task: Estimate the β coefficients using our sample data\nThe method: Ordinary Least Squares (OLS)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#why-linear-regression",
    "href": "weekly-notes/week-05/week-05-class-lab.html#why-linear-regression",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Why Linear Regression?",
    "text": "Why Linear Regression?\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity (we’ll test this)\nSensitive to outliers\nMakes several assumptions (we’ll check these)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#prediction-vs-inference",
    "href": "weekly-notes/week-05/week-05-class-lab.html#prediction-vs-inference",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\nThe same model serves different purposes:\n\n\nInference\n\n“Does education affect income?”\nFocus on coefficients\nStatistical significance matters\nUnderstand mechanisms\n\n\nPrediction\n\n“What’s County Y’s income?”\nFocus on accuracy\nPrediction intervals matter\nDon’t need to understand why\n\n\n\nToday: We’ll do both, but emphasize prediction"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#example-prediction",
    "href": "weekly-notes/week-05/week-05-class-lab.html#example-prediction",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Example: Prediction",
    "text": "Example: Prediction\nGovernment use case:\nCensus misses people in hard-to-count areas. Can we predict:\n\nIncome for areas with poor survey response?\nPopulation for planning purposes?\nResource needs based on demographics?\n\nThe model doesn’t explain WHY these relationships exist, but if predictions are accurate, they’re useful for policy"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#example-inference",
    "href": "weekly-notes/week-05/week-05-class-lab.html#example-inference",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Example: Inference",
    "text": "Example: Inference\nResearch use case:\nUnderstanding gentrification:\n\nWhich neighborhood characteristics explain income change?\nHow much does education matter vs. proximity to downtown?\nAre policy interventions associated with outcomes?\n\nHere we care about the coefficients and what they tell us about mechanisms"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#connection-to-week-2-algorithmic-bias",
    "href": "weekly-notes/week-05/week-05-class-lab.html#connection-to-week-2-algorithmic-bias",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Connection to Week 2: Algorithmic Bias",
    "text": "Connection to Week 2: Algorithmic Bias\nRemember the healthcare algorithm that discriminated?\nThe model: Predicted healthcare needs using costs as proxy\nTechnically: Probably had good R², low prediction error (good “fit”)\nEthically: Learned and amplified existing discrimination\n\n\n\n\n\n\nImportantCritical Point\n\n\n\nA model can be statistically “good” while being ethically terrible for decision-making."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#start-with-data-and-visualization",
    "href": "weekly-notes/week-05/week-05-class-lab.html#start-with-data-and-visualization",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Start with Data and Visualization",
    "text": "Start with Data and Visualization\nLet’s apply these concepts to PA counties:"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#what-do-we-see",
    "href": "weekly-notes/week-05/week-05-class-lab.html#what-do-we-see",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "What Do We See?",
    "text": "What Do We See?\nBefore fitting any model, discuss the visualization:\n\nGenerally positive relationship\nConsiderable scatter (not deterministic)\nMost counties are small (clustered left)\nOne large county with surprisingly low income\nWider confidence band at higher populations\n\nQuestion: What does this tell us about f(X)?"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#fit-the-model",
    "href": "weekly-notes/week-05/week-05-class-lab.html#fit-the-model",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Fit the Model",
    "text": "Fit the Model\n\n\nCode\nmodel1 &lt;- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary(model1)\n\n\n\nCall:\nlm(formula = median_incomeE ~ total_popE, data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-39536  -6013  -1868   5075  44196 \n\nCoefficients:\n                Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 62855.819808  1760.477709  35.704 &lt; 0.0000000000000002 ***\ntotal_popE      0.021477     0.005192   4.137             0.000103 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11820 on 65 degrees of freedom\nMultiple R-squared:  0.2084,    Adjusted R-squared:  0.1962 \nF-statistic: 17.11 on 1 and 65 DF,  p-value: 0.0001032"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#interpreting-coefficients",
    "href": "weekly-notes/week-05/week-05-class-lab.html#interpreting-coefficients",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\nIntercept (β₀) = $62,855\n\nExpected income when population = 0\nNot usually meaningful in practice\n\nSlope (β₁) = $0.02\n\nFor each additional person, income increases by $0.02\nMore useful: For every 1,000 people, income increases by ~$20\n\nIs this relationship real?\n\np-value &lt; 0.001 → Very unlikely to see this if true β₁ = 0\nWe can reject the null hypothesis"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#the-holy-grail-concept",
    "href": "weekly-notes/week-05/week-05-class-lab.html#the-holy-grail-concept",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "The “Holy Grail” Concept",
    "text": "The “Holy Grail” Concept\n\n\nOur estimates are just that: estimates of the true (unknown) parameters\nKey insight:\n\nRed line = true relationship (unknowable)\nBlue line = our estimate from this sample\nDifferent samples → slightly different blue lines\nStandard errors quantify this uncertainty"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#statistical-significance",
    "href": "weekly-notes/week-05/week-05-class-lab.html#statistical-significance",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nThe logic:\n\nNull hypothesis (H₀): β₁ = 0 (no relationship)\nOur estimate: β₁ = 0.02\nQuestion: Could we get 0.02 just by chance if H₀ is true?\n\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H₀ is true\n\nSmall p → reject H₀, conclude relationship exists"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#how-good-is-this-model",
    "href": "weekly-notes/week-05/week-05-class-lab.html#how-good-is-this-model",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "How Good is This Model?",
    "text": "How Good is This Model?\nTwo key questions:\n\nHow well does it fit the data we used? (in-sample fit)\nHow well would it predict new data? (out-of-sample performance)\n\nThese are NOT the same thing!"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#in-sample-fit-r²",
    "href": "weekly-notes/week-05/week-05-class-lab.html#in-sample-fit-r²",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "In-Sample Fit: R²",
    "text": "In-Sample Fit: R²\nR² = 0.208\n“21% of variation in income is explained by population”\nIs this good?\n\nDepends on your goal!\nFor prediction: Moderate\nFor inference: Shows population matters, but other factors exist\n\nR² alone doesn’t tell us if the model is trustworthy"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#the-problem-overfitting",
    "href": "weekly-notes/week-05/week-05-class-lab.html#the-problem-overfitting",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "The Problem: Overfitting",
    "text": "The Problem: Overfitting\nThree scenarios:\n\nUnderfitting: Model too simple (high bias)\nGood fit: Captures pattern without noise\nOverfitting: Memorizes training data (high variance)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#overfitting-in-regression",
    "href": "weekly-notes/week-05/week-05-class-lab.html#overfitting-in-regression",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Overfitting in Regression",
    "text": "Overfitting in Regression\n\n\n\n\n\n\n\n\n\nThe danger: High R² doesn’t mean good predictions!"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#traintest-split",
    "href": "weekly-notes/week-05/week-05-class-lab.html#traintest-split",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nSolution: Hold out some data to test predictions\n\n\nCode\nset.seed(123)\nn &lt;- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices &lt;- sample(1:n, size = 0.7 * n)\ntrain_data &lt;- pa_data[train_indices, ]\ntest_data &lt;- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train &lt;- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions &lt;- predict(model_train, newdata = test_data)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#evaluate-predictions",
    "href": "weekly-notes/week-05/week-05-class-lab.html#evaluate-predictions",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Evaluate Predictions",
    "text": "Evaluate Predictions\n\n\nCode\n# Calculate prediction error (RMSE)\nrmse_test &lt;- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train &lt;- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\n\n\nTraining RMSE: 12893 \n\n\nCode\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n\n\nTest RMSE: 9536 \n\n\n\n\n\n\n\n\nNoteInterpreting RMSE\n\n\n\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#cross-validation",
    "href": "weekly-notes/week-05/week-05-class-lab.html#cross-validation",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nBetter approach: Multiple train/test splits\n\nGives more stable estimate of true prediction performance"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#cross-validation-in-action",
    "href": "weekly-notes/week-05/week-05-class-lab.html#cross-validation-in-action",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Cross-Validation in Action",
    "text": "Cross-Validation in Action\n\n\nCode\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\ncv_model &lt;- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n\n\n  intercept     RMSE  Rsquared      MAE   RMSESD RsquaredSD    MAESD\n1      TRUE 12577.76 0.5643826 8859.865 5609.002  0.2997098 2238.042\n\n\nKey Metrics (Averaged Across 10 Folds)\n\nRMSE: Typical prediction error (~$12,578)\nR²: % of variation explained (0.564)\nMAE: Average absolute error (~$8,860) - easier to interpret"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#when-can-we-trust-this-model",
    "href": "weekly-notes/week-05/week-05-class-lab.html#when-can-we-trust-this-model",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "When Can We Trust This Model?",
    "text": "When Can We Trust This Model?\nLinear regression makes assumptions. If violated:\n\nCoefficients may be biased\nStandard errors wrong\nPredictions unreliable\n\nWe must check diagnostics before trusting any model"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#assumption-1-linearity",
    "href": "weekly-notes/week-05/week-05-class-lab.html#assumption-1-linearity",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Assumption 1: Linearity",
    "text": "Assumption 1: Linearity\nWhat we assume: Relationship is actually linear\nHow to check: Residual plot\n\n\nCode\npa_data$residuals &lt;- residuals(model1)\npa_data$fitted &lt;- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#reading-residual-plots",
    "href": "weekly-notes/week-05/week-05-class-lab.html#reading-residual-plots",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Reading Residual Plots",
    "text": "Reading Residual Plots\n\n\nGood\n\nRandom scatter\nPoints around 0\nConstant spread\n\n\nBad\n\nCurved pattern\nModel missing something\nPredictions biased\n\n\n\n\n\n\n\n\n\nImportantWhy This Matters for Prediction\n\n\n\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and you fit a straight line, you’ll systematically underpredict in some regions and overpredict in others\nBiased predictions in predictable ways (not random errors!)\nResidual plots should show random scatter - any pattern means your model is missing something systematic"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#assumption-2-constant-variance",
    "href": "weekly-notes/week-05/week-05-class-lab.html#assumption-2-constant-variance",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Assumption 2: Constant Variance",
    "text": "Assumption 2: Constant Variance\nHeteroscedasticity: Variance changes across X\nImpact: Standard errors are wrong → p-values misleading\n\n\n\n\n\n\nWarningWhat Heteroskedasticity Tells You\n\n\n\nOften a symptom of model misspecification:\n\nModel fits well for some values (e.g., small counties) but poorly for others (large counties)\nMay indicate missing variables that matter more at certain X values\nAsk: “What’s different about observations with large residuals?”\n\nExample: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately."
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#heteroskedasticity-visualized",
    "href": "weekly-notes/week-05/week-05-class-lab.html#heteroskedasticity-visualized",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Heteroskedasticity Visualized",
    "text": "Heteroskedasticity Visualized\n\nKey Insight: Adding the right predictor can fix heteroscedasticity"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#formal-test-breusch-pagan",
    "href": "weekly-notes/week-05/week-05-class-lab.html#formal-test-breusch-pagan",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Formal Test: Breusch-Pagan",
    "text": "Formal Test: Breusch-Pagan\n\n\nCode\nlibrary(lmtest)\nbptest(model1)\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 27.481, df = 1, p-value = 0.0000001587\n\n\nInterpretation:\n\np &gt; 0.05: Constant variance assumption OK\np &lt; 0.05: Evidence of heteroscedasticity\n\nIf detected, solutions:\n\nTransform Y (try log(income))\nRobust standard errors\nAdd missing variables\nAccept it (point predictions still OK for prediction goals)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#assumption-normality-of-residuals",
    "href": "weekly-notes/week-05/week-05-class-lab.html#assumption-normality-of-residuals",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Assumption: Normality of Residuals",
    "text": "Assumption: Normality of Residuals\nWhat we assume: Residuals are normally distributed\nWhy it matters:\n\nLess critical for point predictions (unbiased regardless)\nImportant for confidence intervals and prediction intervals\nNeeded for valid hypothesis tests (t-tests, F-tests)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#assumption-3-no-multicollinearity",
    "href": "weekly-notes/week-05/week-05-class-lab.html#assumption-3-no-multicollinearity",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Assumption 3: No Multicollinearity",
    "text": "Assumption 3: No Multicollinearity\nFor multiple regression: Predictors shouldn’t be too correlated\n\n\nCode\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF &gt; 10 suggests problems\n# Not relevant with only 1 predictor!\n\n\nWhy it matters: Coefficients become unstable, hard to interpret"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#assumption-4-no-influential-outliers",
    "href": "weekly-notes/week-05/week-05-class-lab.html#assumption-4-no-influential-outliers",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Assumption 4: No Influential Outliers",
    "text": "Assumption 4: No Influential Outliers\nNot all outliers are problems - only those with high leverage AND large residuals\n\n\n\nVisual Diagnostic\n\n\n\n\n\n\n\n\n\n\n\n\nIdentify Influential Points\n\n\n# A tibble: 2 × 4\n  NAME                              total_popE median_incomeE cooks_d\n  &lt;chr&gt;                                  &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 Philadelphia County, Pennsylvania    1593208          57537   5.95 \n2 Allegheny County, Pennsylvania       1245310          72537   0.399\n\n\nInterpretation:\n\nCook’s D &gt; 4/n = potentially influential\nHigh leverage + large residual = pulls regression line"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#what-to-do-with-influential-points",
    "href": "weekly-notes/week-05/week-05-class-lab.html#what-to-do-with-influential-points",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "What To Do With Influential Points",
    "text": "What To Do With Influential Points\n\n\n\n\n\n\nTipInvestigation Strategy\n\n\n\n\nInvestigate: Why is this observation unusual? (data error? truly unique?)\nReport: Always note influential observations in your analysis\nSensitivity check: Refit model without them - do conclusions change?\nDon’t automatically remove: They might represent real, important cases\n\nFor policy: An influential county might need special attention, not exclusion!\n\n\n\n\n\n\n\n\nImportantConnection to Algorithmic Bias\n\n\n\nHigh-influence observations in demographic could represent marginalized communities or unique populations. Automatically removing them can erase important populations from analysis and lead to biased policy decisions.\nAlways investigate before removing!"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#adding-more-predictors",
    "href": "weekly-notes/week-05/week-05-class-lab.html#adding-more-predictors",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Adding More Predictors",
    "text": "Adding More Predictors\nMaybe population alone isn’t enough:\n\n\n\nCall:\nlm(formula = median_incomeE ~ total_popE + percent_collegeE + \n    poverty_rateE, data = pa_data_full)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32939  -4473  -1098   3861  27672 \n\nCoefficients:\n                    Estimate  Std. Error t value             Pr(&gt;|t|)    \n(Intercept)      60893.86796  1589.24122  38.316 &lt; 0.0000000000000002 ***\ntotal_popE           0.06657     0.03356   1.984               0.0517 .  \npercent_collegeE     0.04710     0.14916   0.316               0.7532    \npoverty_rateE       -0.36503     0.07749  -4.711             0.000014 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8610 on 63 degrees of freedom\nMultiple R-squared:  0.5931,    Adjusted R-squared:  0.5738 \nF-statistic: 30.61 on 3 and 63 DF,  p-value: 0.000000000002485"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#log-transformations",
    "href": "weekly-notes/week-05/week-05-class-lab.html#log-transformations",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Log Transformations",
    "text": "Log Transformations\nIf relationship is curved, try transforming:\n\n\nCode\n# Compare linear vs log\nmodel_linear &lt;- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log &lt;- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n\n\n\nCall:\nlm(formula = median_incomeE ~ log(total_popE), data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-27231  -5780  -2118   3953  40638 \n\nCoefficients:\n                Estimate Std. Error t value   Pr(&gt;|t|)    \n(Intercept)        -4867      12374  -0.393      0.695    \nlog(total_popE)     6276       1074   5.842 0.00000018 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10760 on 65 degrees of freedom\nMultiple R-squared:  0.3443,    Adjusted R-squared:  0.3342 \nF-statistic: 34.13 on 1 and 65 DF,  p-value: 0.0000001805\n\n\nCode\n# Check which residual plot looks better\n\n\nInterpretation changes: Log models show percentage relationships"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#categorical-variables",
    "href": "weekly-notes/week-05/week-05-class-lab.html#categorical-variables",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\n\nCode\n# Create metro/non-metro indicator\npa_data &lt;- pa_data %&gt;%\n  mutate(metro = ifelse(total_popE &gt; 500000, 1, 0))\n\nmodel3 &lt;- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n\n\n\nCall:\nlm(formula = median_incomeE ~ total_popE + metro, data = pa_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28825  -6732  -2885   7672  26622 \n\nCoefficients:\n                Estimate   Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 64924.857892  1659.821786  39.116 &lt; 0.0000000000000002 ***\ntotal_popE     -0.005290     0.008047  -0.657             0.513257    \nmetro       29865.529225  7319.309190   4.080             0.000127 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10610 on 64 degrees of freedom\nMultiple R-squared:  0.3718,    Adjusted R-squared:  0.3522 \nF-statistic: 18.94 on 2 and 64 DF,  p-value: 0.0000003456\n\n\nR creates dummy variables automatically"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#the-task",
    "href": "weekly-notes/week-05/week-05-class-lab.html#the-task",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "The Task",
    "text": "The Task\nPredict median home value (B25077_001) for PA counties using any combination of predictors\nBuild the model with lowest 10-fold cross-validated RMSE"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#available-predictors",
    "href": "weekly-notes/week-05/week-05-class-lab.html#available-predictors",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Available Predictors",
    "text": "Available Predictors\n\n\nCode\nchallenge_data &lt;- get_acs(\n  geography = \"county\",\n  state = \"PA\",\n  variables = c(\n    home_value = \"B25077_001\",      # YOUR TARGET\n    total_pop = \"B01003_001\",       # Total population*\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\",      # Median age*\n    percent_college = \"B15003_022\", # Bachelor's degree or higher*\n    median_rent = \"B25058_001\",     # Median rent\n    poverty_rate = \"B17001_002\"     # Population in poverty*\n  ),\n  year = 2022,\n  output = \"wide\"\n)\n\nchallenge_data\n\n\n\n\nCode\n#hist(challenge_data$total_popE)\n#hist(challenge_data$median_ageE)\n#hist(challenge_data$percent_collegeE)\n#hist(challenge_data$poverty_rateE)\n\n#hist(log(challenge_data$total_popE))\n#hist(log(challenge_data$median_ageE))\n#hist(log(challenge_data$percent_collegeE))\n#hist(log(challenge_data$poverty_rateE))\n\n\n\n\nCode\n# Set random seed for reproducibility\n#set.seed(123)\n\n# Split into 70% training and 30% testing\n#n &lt;- nrow(challenge_data)\n#train_indices &lt;- sample(1:n, size = 0.7 * n)\n#train_data &lt;- challenge_data[train_indices, ]\n#test_data  &lt;- challenge_data[-train_indices, ]\n\n# Fit the regression model on training data\n#model_train &lt;- lm(\n#  home_valueE ~ log(total_popE) + median_ageE + log(percent_collegeE) + log(poverty_rateE),\n#  data = train_data\n#)\n\n# Predict on both training and test data\n#train_predictions &lt;- predict(model_train, newdata = train_data)\ntest_predictions  &lt;- predict(model_train, newdata = test_data)\n\n# Compute RMSE for training and testing\n#rmse_train &lt;- sqrt(mean((train_data$home_valueE - train_predictions)^2))\n#rmse_test  &lt;- sqrt(mean((test_data$home_valueE - test_predictions)^2))\n\n# Display results\n#cat(\"Training RMSE:\", round(rmse_train, 2), \"\\n\")\n#cat(\"Test RMSE:\", round(rmse_test, 2), \"\\n\")\n\n# Show summary of model fit\n#summary(model_train)"
  },
  {
    "objectID": "weekly-notes/week-05/week-05-class-lab.html#rules-strategy",
    "href": "weekly-notes/week-05/week-05-class-lab.html#rules-strategy",
    "title": "INTRODUCTION TO LINEAR REGRESSION",
    "section": "Rules & Strategy",
    "text": "Rules & Strategy\nYou can:\n\nUse any combination of predictors (time permitting, you can fetch more)\nTry log transformations: log(total_popE)\nEngineer new categorical features\nRemove influential outliers (but document which!)\n\nYou must:\n\nUse 10-fold cross-validation to report final RMSE\nDo a full diagnostic check (residual plot, Cook’s D, or Breusch-Pagan)\nBe ready to explain your model in 2 minutes\n\n\n\n\n\n\n\nTipHints\n\n\n\n\nStart simple (one predictor), check diagnostics\nIncome and rent are probably highly correlated (multicollinearity!)\nTry log transformation if relationship looks curved\nDon’t forget to remove NAs: na.omit(challenge_data)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html",
    "href": "weekly-notes/week-06/week-06-class-lab.html",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "",
    "text": "Warm-Up: Build a Baseline Model\n\nQuick review of Week 5 regression\nCreate simple structural model\nIdentify its limitations\n\nPart 1: Expanding Your Toolkit\n\nCategorical variables\nInteractions\nPolynomial terms\n\n\nPart 2: Why Space Matters\n\nHedonic model framework\nTobler’s First Law\nSpatial autocorrelation\n\nPart 3: Creating Spatial Features\n\nBuffer aggregation\nk-Nearest Neighbors\nDistance to amenities\n\n\n\nPart 4: Fixed Effects"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#what-well-cover",
    "href": "weekly-notes/week-06/week-06-class-lab.html#what-well-cover",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "",
    "text": "Warm-Up: Build a Baseline Model\n\nQuick review of Week 5 regression\nCreate simple structural model\nIdentify its limitations\n\nPart 1: Expanding Your Toolkit\n\nCategorical variables\nInteractions\nPolynomial terms\n\n\nPart 2: Why Space Matters\n\nHedonic model framework\nTobler’s First Law\nSpatial autocorrelation\n\nPart 3: Creating Spatial Features\n\nBuffer aggregation\nk-Nearest Neighbors\nDistance to amenities\n\n\n\nPart 4: Fixed Effects"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#lets-build-something-simple-together",
    "href": "weekly-notes/week-06/week-06-class-lab.html#lets-build-something-simple-together",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Let’s Build Something Simple Together",
    "text": "Let’s Build Something Simple Together\nWe’ll start by creating a basic model using only structural features - this will be our baseline to improve upon today.\n\n\nCode\n# Load packages and data\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\n\n# Load Boston housing data\nboston &lt;- read_csv(here(\"data/boston.csv\"))\n\n# Quick look at the data\nglimpse(boston)\n\n\nRows: 1,485\nColumns: 24\n$ ...1       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ Parcel_No  &lt;dbl&gt; 100032000, 100058000, 100073000, 100112000, 100137000, 1001…\n$ SalePrice  &lt;dbl&gt; 450000, 600000, 450000, 670000, 260000, 355000, 665000, 355…\n$ PricePerSq &lt;dbl&gt; 228.89, 164.34, 105.98, 291.94, 217.21, 190.96, 227.35, 120…\n$ LivingArea &lt;dbl&gt; 1966, 3840, 4246, 2295, 1197, 1859, 2925, 2904, 892, 1916, …\n$ Style      &lt;chr&gt; \"Conventional\", \"Semi?Det\", \"Decker\", \"Row\\xa0End\", \"Coloni…\n$ GROSS_AREA &lt;dbl&gt; 3111, 5603, 6010, 3482, 1785, 2198, 4341, 3892, 1658, 3318,…\n$ NUM_FLOORS &lt;dbl&gt; 2.0, 3.0, 3.0, 3.0, 2.0, 1.5, 3.0, 3.0, 2.0, 2.0, 1.5, 2.0,…\n$ R_BDRMS    &lt;dbl&gt; 4, 8, 9, 6, 2, 3, 8, 6, 2, 2, 4, 3, 3, 3, 6, 8, 4, 4, 3, 2,…\n$ R_FULL_BTH &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 1, 2,…\n$ R_HALF_BTH &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ R_KITCH    &lt;dbl&gt; 2, 3, 3, 3, 1, 2, 3, 3, 1, 2, 2, 3, 1, 1, 3, 3, 2, 3, 2, 2,…\n$ R_AC       &lt;chr&gt; \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\",…\n$ R_FPLACE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ LU         &lt;chr&gt; \"R2\", \"R3\", \"R3\", \"R3\", \"R1\", \"R2\", \"R3\", \"E\", \"R1\", \"R2\", …\n$ OWN_OCC    &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\",…\n$ R_BLDG_STY &lt;chr&gt; \"CV\", \"SD\", \"DK\", \"RE\", \"CL\", \"CV\", \"DK\", \"DK\", \"RE\", \"TF\",…\n$ R_ROOF_TYP &lt;chr&gt; \"H\", \"F\", \"F\", \"F\", \"F\", \"G\", \"F\", \"F\", \"G\", \"F\", \"G\", \"F\",…\n$ R_EXT_FIN  &lt;chr&gt; \"M\", \"B\", \"M\", \"M\", \"P\", \"M\", \"M\", \"A\", \"A\", \"M\", \"W\", \"W\",…\n$ R_TOTAL_RM &lt;dbl&gt; 10, 17, 20, 14, 5, 8, 14, 14, 4, 9, 7, 7, 5, 5, 15, 14, 11,…\n$ R_HEAT_TYP &lt;chr&gt; \"W\", \"W\", \"W\", \"W\", \"E\", \"E\", \"W\", \"W\", \"W\", \"W\", \"W\", \"W\",…\n$ YR_BUILT   &lt;dbl&gt; 1900, 1910, 1910, 1905, 1860, 1905, 1900, 1890, 1900, 1900,…\n$ Latitude   &lt;dbl&gt; 42.37963, 42.37877, 42.37940, 42.38014, 42.37967, 42.37953,…\n$ Longitude  &lt;dbl&gt; -71.03076, -71.02943, -71.02846, -71.02859, -71.02903, -71.…\n\n\nCode\n# Simple model: Predict price from living area\nbaseline_model &lt;- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,    Adjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#what-does-this-model-tell-us",
    "href": "weekly-notes/week-06/week-06-class-lab.html#what-does-this-model-tell-us",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "What Does This Model Tell Us?",
    "text": "What Does This Model Tell Us?\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,    Adjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: &lt; 2.2e-16\n\n\n[1] 0.1312636"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#interpreting-our-baseline",
    "href": "weekly-notes/week-06/week-06-class-lab.html#interpreting-our-baseline",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Interpreting Our Baseline",
    "text": "Interpreting Our Baseline\nExpected Output:\n\n\n\nVariable\nCoefficient\nStd. Error\nt-value\np-value\n\n\n\n\nIntercept\n157968.32\n35855.59\n4.406\n&lt;0.001\n\n\nLivingArea\n216.54\n14.47\n14.969\n&lt;0.001\n\n\n\nWhat this means:\n\nBase price (0 sq ft) ≈ 157968.32\nEach additional square foot adds ~216 to price\nRelationship is statistically significant (p &lt; 0.001)\nBut R² is only 0.13 (13% of variation explained)\n\n\n\n\n\n\n\nImportantThe Problem\n\n\n\nMOST of the variation in house prices is still unexplained!\nWhat are we missing? 🤔"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#limitations-of-this-model",
    "href": "weekly-notes/week-06/week-06-class-lab.html#limitations-of-this-model",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Limitations of This Model",
    "text": "Limitations of This Model\n\n\n\n\n\n\nWarningWhat’s Missing?\n\n\n\n\nWhat does this model ignore?\n\nLocation! (North End vs. Roxbury vs. Back Bay)\nProximity to downtown, waterfront, parks\nNearby crime levels\nSchool quality\nNeighborhood characteristics\n\nWhy might it fail?\n\n1,000 sq ft in Back Bay ≠ 1,000 sq ft in Roxbury\nSame house, different locations → vastly different prices\n“Location, location, location!”\n\nHow could we improve it?\n\nAdd spatial features (crime nearby, distance to amenities)\nControl for neighborhood (fixed effects)\nInclude interactions (does size matter more in wealthy areas?)\n\n\n\n\nThis is exactly where spatial features come in!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#lets-add-one-more-feature",
    "href": "weekly-notes/week-06/week-06-class-lab.html#lets-add-one-more-feature",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Let’s Add One More Feature",
    "text": "Let’s Add One More Feature\n\n\nCode\n# Add number of bathrooms\nbetter_model &lt;- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\nsummary(better_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea + R_FULL_BTH, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-783019 -230756  -65737   75367 9193133 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 107683.96   37340.88   2.884  0.00399 ** \nLivingArea     145.68      21.33   6.828 1.25e-11 ***\nR_FULL_BTH  106978.13   23800.30   4.495 7.50e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 560200 on 1482 degrees of freedom\nMultiple R-squared:  0.1429,    Adjusted R-squared:  0.1418 \nF-statistic: 123.6 on 2 and 1482 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# Compare models\ncat(\"Baseline R²:\", summary(baseline_model)$r.squared, \"\\n\")\n\n\nBaseline R²: 0.1312636 \n\n\nCode\ncat(\"With bathrooms R²:\", summary(better_model)$r.squared, \"\\n\")\n\n\nWith bathrooms R²: 0.1429474 \n\n\nR² improves a smidge… but still missing location!\n\n\n\n\n\n\nNoteToday’s Goal\n\n\n\nBy the end of class, you’ll build models that: - Incorporate spatial relationships - Account for neighborhood effects\n- Achieve much better prediction accuracy - Help you understand what drives housing prices"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#converting-to-spatial-data",
    "href": "weekly-notes/week-06/week-06-class-lab.html#converting-to-spatial-data",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Converting to Spatial Data",
    "text": "Converting to Spatial Data\n\nStep 1: Make your data spatial\n\n\nCode\nlibrary(sf)\n\n# Convert boston data to sf object\nboston.sf &lt;- boston %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n# Check it worked\nhead(boston.sf)\n\n\nSimple feature collection with 6 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 238643.6 ymin: 903246 xmax: 238833.2 ymax: 903399.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 23\n   ...1 Parcel_No SalePrice PricePerSq LivingArea Style    GROSS_AREA NUM_FLOORS\n  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1     1 100032000    450000       229.       1966 \"Conven…       3111        2  \n2     2 100058000    600000       164.       3840 \"Semi?D…       5603        3  \n3     3 100073000    450000       106.       4246 \"Decker\"       6010        3  \n4     4 100112000    670000       292.       2295 \"Row\\xa…       3482        3  \n5     5 100137000    260000       217.       1197 \"Coloni…       1785        2  \n6     6 100138000    355000       191.       1859 \"Conven…       2198        1.5\n# ℹ 15 more variables: R_BDRMS &lt;dbl&gt;, R_FULL_BTH &lt;dbl&gt;, R_HALF_BTH &lt;dbl&gt;,\n#   R_KITCH &lt;dbl&gt;, R_AC &lt;chr&gt;, R_FPLACE &lt;dbl&gt;, LU &lt;chr&gt;, OWN_OCC &lt;chr&gt;,\n#   R_BLDG_STY &lt;chr&gt;, R_ROOF_TYP &lt;chr&gt;, R_EXT_FIN &lt;chr&gt;, R_TOTAL_RM &lt;dbl&gt;,\n#   R_HEAT_TYP &lt;chr&gt;, YR_BUILT &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nCode\nclass(boston.sf)  # Should show \"sf\" and \"data.frame\"\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n\nTipWhy transform CRS?\n\n\n\n\n4326 = WGS84 (lat/lon in degrees) - fine for display\nESRI:102286 = MA State Plane (feet) - good for distance calculations"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#step-2-spatial-join-with-neighborhoods",
    "href": "weekly-notes/week-06/week-06-class-lab.html#step-2-spatial-join-with-neighborhoods",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Step 2: Spatial Join with Neighborhoods",
    "text": "Step 2: Spatial Join with Neighborhoods\n\n\nCode\n# Load neighborhood boundaries\nnhoods &lt;- read_sf(here(\"data/BPDA_Neighborhood_Boundaries.geojson\")) %&gt;%\n  st_transform('ESRI:102286')  # Match CRS!\n\n# Check the neighborhoods\nhead(nhoods)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 229049.1 ymin: 890856 xmax: 236590.9 ymax: 900302.5\nProjected CRS: NAD_1983_HARN_StatePlane_Massachusetts_Mainland_FIPS_2001\n# A tibble: 6 × 8\n  sqmiles name         neighborhood_id  acres SHAPE__Length objectid SHAPE__Area\n    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt;         &lt;dbl&gt;    &lt;int&gt;       &lt;dbl&gt;\n1    2.51 Roslindale   15              1606.         53564.       53   69938273.\n2    3.94 Jamaica Pla… 11              2519.         56350.       54  109737890.\n3    0.55 Mission Hill 13               351.         17919.       55   15283120.\n4    0.29 Longwood     28               189.         11909.       56    8215904.\n5    0.04 Bay Village  33                26.5         4651.       57    1156071.\n6    0.02 Leather Dis… 27                15.6         3237.       58     681272.\n# ℹ 1 more variable: geometry &lt;MULTIPOLYGON [m]&gt;\n\n\nCode\nnrow(nhoods)  # How many neighborhoods?\n\n\n[1] 26\n\n\nCode\n# Spatial join: Assign each house to its neighborhood\nboston.sf &lt;- boston.sf %&gt;%\n  st_join(nhoods, join = st_intersects)\n\n# Check results\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name) %&gt;%\n  arrange(desc(n))\n\n\n# A tibble: 19 × 2\n   name              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 Dorchester      344\n 2 West Roxbury    242\n 3 Hyde Park       152\n 4 East Boston     149\n 5 Roslindale      142\n 6 Jamaica Plain   113\n 7 South Boston     80\n 8 Charlestown      65\n 9 Mattapan         65\n10 Roxbury          63\n11 South End        20\n12 Beacon Hill      17\n13 Mission Hill     14\n14 Allston           6\n15 Brighton          6\n16 Back Bay          3\n17 Fenway            2\n18 Bay Village       1\n19 Downtown          1\n\n\n\n\n\n\n\n\nImportantWhat just happened?\n\n\n\nst_join() found which neighborhood polygon contains each house point!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#visualize-prices-by-neighborhood",
    "href": "weekly-notes/week-06/week-06-class-lab.html#visualize-prices-by-neighborhood",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Visualize: Prices by Neighborhood",
    "text": "Visualize: Prices by Neighborhood"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#the-spatial-pattern-is-clear",
    "href": "weekly-notes/week-06/week-06-class-lab.html#the-spatial-pattern-is-clear",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "The Spatial Pattern is Clear!",
    "text": "The Spatial Pattern is Clear!\n\n\nCode\n# Which neighborhoods are most expensive?\nprice_by_nhood %&gt;%\n  arrange(desc(median_price)) %&gt;%\n  head(5)\n\n\n# A tibble: 5 × 3\n  name        median_price n_sales\n  &lt;chr&gt;              &lt;dbl&gt;   &lt;int&gt;\n1 Back Bay         9500000       3\n2 Beacon Hill      2892750      17\n3 South End        2797500      20\n4 Bay Village      2300000       1\n5 Fenway           2112500       2\n\n\nCode\n# Which have most sales?\nprice_by_nhood %&gt;%\n  arrange(desc(n_sales)) %&gt;%\n  head(5)\n\n\n# A tibble: 5 × 3\n  name         median_price n_sales\n  &lt;chr&gt;               &lt;dbl&gt;   &lt;int&gt;\n1 Dorchester         511250     344\n2 West Roxbury       503750     242\n3 Hyde Park          390000     152\n4 East Boston        463000     149\n5 Roslindale         489500     142\n\n\n\n\n\n\n\n\nNoteDiscussion Question\n\n\n\nWhy do you think certain neighborhoods command higher prices? - Proximity to downtown? - Historical character? - School quality? - Safety? - All of the above?\nThis is why we need spatial features and neighborhood controls!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#beyond-continuous-variables",
    "href": "weekly-notes/week-06/week-06-class-lab.html#beyond-continuous-variables",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Beyond Continuous Variables",
    "text": "Beyond Continuous Variables\n\n\n\n✅ Continuous Variables\n\nSquare footage\nAge of house\nIncome levels\nDistance to downtown\n\n\n\n\n🏷️ Categorical Variables\n\nNeighborhood\nSchool district\nBuilding type\nHas garage? (Yes/No)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#dummy-variables",
    "href": "weekly-notes/week-06/week-06-class-lab.html#dummy-variables",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Dummy Variables",
    "text": "Dummy Variables\n\nOur Boston Data: name variable from spatial join\nNeighborhoods in our dataset (showing just a few):\n\n\n# A tibble: 10 × 2\n   name              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 Dorchester      344\n 2 West Roxbury    242\n 3 Hyde Park       152\n 4 East Boston     149\n 5 Roslindale      142\n 6 Jamaica Plain   113\n 7 South Boston     80\n 8 Charlestown      65\n 9 Mattapan         65\n10 Roxbury          63\n\n\nHow R Handles This\nWhen you include name in a model, R automatically creates binary indicators:\n\nBack_Bay: 1 if Back Bay, 0 otherwise\nBeacon_Hill: 1 if Beacon Hill, 0 otherwise\nCharlestown: 1 if Charlestown, 0 otherwise\n…and so on for all neighborhoods\n\n\n\n\n\n\n\nWarning⚠️ The (n-1) Rule\n\n\n\nOne neighborhood is automatically chosen as the reference category (omitted)!\nR picks the first alphabetically unless you specify otherwise."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#add-dummy-categorical-variables-to-the-model",
    "href": "weekly-notes/week-06/week-06-class-lab.html#add-dummy-categorical-variables-to-the-model",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Add Dummy (Categorical) Variables to the Model",
    "text": "Add Dummy (Categorical) Variables to the Model\n\n\nCode\n# Ensure name is a factor\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(name = as.factor(name))\n\n# Check which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\n\n\n[1] \"Allston\"\n\n\nCode\n# Fit model with neighborhood fixed effects\nmodel_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, \n                          data = boston.sf)\n\n# Show just first 10 coefficients\nsummary(model_neighborhoods)$coef[1:10, ]\n\n\n                    Estimate   Std. Error    t value      Pr(&gt;|t|)\n(Intercept)      704306.0751 9.022210e+04  7.8063584  1.112892e-14\nLivingArea          138.3206 6.468186e+00 21.3847638  1.605175e-88\nnameBack Bay    7525585.0560 1.533714e+05 49.0677253 1.522921e-311\nnameBay Village 1284057.5326 2.320256e+05  5.5341206  3.700442e-08\nnameBeacon Hill 1767805.2297 1.020211e+05 17.3278366  2.465874e-61\nnameBrighton    -168941.2113 1.239972e+05 -1.3624597  1.732623e-01\nnameCharlestown   -2556.8667 9.208989e+04 -0.0277649  9.778534e-01\nnameDorchester  -576819.4858 8.846990e+04 -6.5199517  9.653393e-11\nnameDowntown      58553.1237 2.322150e+05  0.2521505  8.009601e-01\nnameEast Boston -534060.4110 8.962950e+04 -5.9585340  3.182615e-09"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#interpreting-neighborhood-dummy-variables",
    "href": "weekly-notes/week-06/week-06-class-lab.html#interpreting-neighborhood-dummy-variables",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Interpreting Neighborhood Dummy Variables",
    "text": "Interpreting Neighborhood Dummy Variables\n\n\n\n\n\nVariable\nCoefficient\np-value\n\n\n\n\nIntercept (Allston)\n$649,635\n&lt; 0.001***\n\n\nLiving Area (per sq ft)\n$114\n&lt; 0.001***\n\n\nBack Bay\n$7,561,273\n&lt; 0.001***\n\n\nBeacon Hill\n$1,780,861\n&lt; 0.001***\n\n\nCharlestown\n$22,575\n0.806\n\n\nDorchester\n-$540,267\n&lt; 0.001***\n\n\nEast Boston\n-$515,990\n&lt; 0.001***\n\n\nRoxbury\n-$566,534\n&lt; 0.001***\n\n\n\n\n\n\nHow to Read This Table\nReference Category: Allston (automatically chosen - alphabetically first)\nStructural Variables:\n\nLiving Area: Each additional sq ft adds this amount (same for all neighborhoods)\nBedrooms: Effect of one more full bathroom (same for all neighborhoods)\n\nNeighborhood Dummies:\n\nPositive coefficient = This neighborhood is MORE expensive than Allston\nNegative coefficient = This neighborhood is LESS expensive than Allston\nAll else equal (same size, same bathrooms)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#concrete-example-comparing-two-houses",
    "href": "weekly-notes/week-06/week-06-class-lab.html#concrete-example-comparing-two-houses",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Concrete Example: Comparing Two Houses",
    "text": "Concrete Example: Comparing Two Houses\nUsing our model, let’s compare identical houses in different neighborhoods:\n\n\n\nHouse A: Back Bay\n\nLiving Area: 1,500 sq ft\nBaths: 2\nNeighborhood: Back Bay\n\nPredicted Price:\n\n\n           1 \n\"$8,458,873\" \n\n\n\n\n\nHouse B: Roxbury\n\nLiving Area: 1,500 sq ft\nBaths: 2\nNeighborhood: Roxbury\n\nPredicted Price:\n\n\n         1 \n\"$331,066\" \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Neighborhood Effect Price Difference\n\n\n           1 \n\"$8,127,807\" \n\n\nSame house, different location = huge price difference! This is what the neighborhood dummies capture."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#interaction-effects-when-relationships-depend",
    "href": "weekly-notes/week-06/week-06-class-lab.html#interaction-effects-when-relationships-depend",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Interaction Effects: When Relationships Depend",
    "text": "Interaction Effects: When Relationships Depend\n\nThe Question\nDoes the effect of one variable depend on the level of another variable?\n\n\nExample Scenarios\n\nHousing: Does square footage matter more in wealthy neighborhoods?\nEducation: Do tutoring effects vary by initial skill level?\nPublic Health: Do pollution effects differ by age?\n\n\n\n\n\n\n\nImportantMathematical Form\n\n\n\nSalePrice = β₀ + β₁(LivingArea) + β₂(WealthyNeighborhood) + β₃(LivingArea × WealthyNeighborhood) + ε\n\n\nToday’s example: Is the value of square footage the same across all Boston neighborhoods?"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#theory-luxury-premium-hypothesis",
    "href": "weekly-notes/week-06/week-06-class-lab.html#theory-luxury-premium-hypothesis",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Theory: Luxury Premium Hypothesis",
    "text": "Theory: Luxury Premium Hypothesis\n\n\n\n🏛️ In Wealthy Neighborhoods\n(Back Bay, Beacon Hill, South End)\n\nHigh-end buyers pay premium for space\nLuxury finishes, location prestige\nEach sq ft adds substantial value\nSteep slope\n\nHypothesis: $300+ per sq ft\n\n\n\n🏘️ In Working-Class Neighborhoods\n(Dorchester, Mattapan, East Boston)\n\nBuyers value function over luxury\nMore price-sensitive market\nSpace matters, but less premium\nFlatter slope\n\nHypothesis: $100-150 per sq ft\n\n\n\n\n\n\n\n\n\nNoteThe Key Question\n\n\n\nIf we assume one slope for all neighborhoods, are we misunderstanding the market?"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#create-the-neighborhood-categories",
    "href": "weekly-notes/week-06/week-06-class-lab.html#create-the-neighborhood-categories",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Create the Neighborhood Categories",
    "text": "Create the Neighborhood Categories\n\n\nCode\n# Define wealthy neighborhoods based on median prices\nwealthy_hoods &lt;- c(\"Back Bay\", \"Beacon Hill\", \"South End\", \"Bay Village\")\n\n# Create binary indicator\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, \"Wealthy\", \"Not Wealthy\"),\n    wealthy_neighborhood = as.factor(wealthy_neighborhood)\n  )\n\n# Check the split\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(wealthy_neighborhood)\n\n\n# A tibble: 2 × 2\n  wealthy_neighborhood     n\n  &lt;fct&gt;                &lt;int&gt;\n1 Not Wealthy           1444\n2 Wealthy                 41"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#model-1-no-interaction-parallel-slopes",
    "href": "weekly-notes/week-06/week-06-class-lab.html#model-1-no-interaction-parallel-slopes",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Model 1: No Interaction (Parallel Slopes)",
    "text": "Model 1: No Interaction (Parallel Slopes)\n\n\nCode\n# Model assumes same slope everywhere\nmodel_no_interact &lt;- lm(SalePrice ~ LivingArea + wealthy_neighborhood, \n                        data = boston.sf)\n\nsummary(model_no_interact)$coef\n\n\n                                Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)                  213842.7642 23893.619394  8.949785  1.039392e-18\nLivingArea                      160.2357     9.713346 16.496442  2.936087e-56\nwealthy_neighborhoodWealthy 2591012.0471 59958.794614 43.213211 1.103579e-264\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhat This Assumes\nLiving area has the same effect in all neighborhoods Only the intercept differs (wealthy areas start higher) Parallel lines on a plot"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#model-2-with-interaction-different-slopes",
    "href": "weekly-notes/week-06/week-06-class-lab.html#model-2-with-interaction-different-slopes",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Model 2: With Interaction (Different Slopes)",
    "text": "Model 2: With Interaction (Different Slopes)\n\n\nCode\n# Model allows different slopes\nmodel_interact &lt;- lm(SalePrice ~ LivingArea * wealthy_neighborhood, \n                     data = boston.sf)\n\nsummary(model_interact)$coef\n\n\n                                            Estimate   Std. Error   t value\n(Intercept)                             358542.41696 1.863997e+04 19.235144\nLivingArea                                  95.63947 7.620382e+00 12.550483\nwealthy_neighborhoodWealthy            -377937.38372 1.005554e+05 -3.758498\nLivingArea:wealthy_neighborhoodWealthy     985.12500 2.975904e+01 33.103383\n                                            Pr(&gt;|t|)\n(Intercept)                             8.875710e-74\nLivingArea                              2.079700e-34\nwealthy_neighborhoodWealthy             1.775774e-04\nLivingArea:wealthy_neighborhoodWealthy 2.445570e-180\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat This Allows\nLiving area can have different effects in different neighborhoods Both intercept AND slope differ Non-parallel lines on a plot"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#interpreting-the-interaction-coefficients",
    "href": "weekly-notes/week-06/week-06-class-lab.html#interpreting-the-interaction-coefficients",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Interpreting the Interaction Coefficients",
    "text": "Interpreting the Interaction Coefficients\n\n\n\n\n\nTerm\nCoefficient\nt-value\np-value\nSig\n\n\n\n\nIntercept (Not Wealthy)\n$358,542\n19.235\n0\n***\n\n\nLiving Area (Not Wealthy)\n$96\n12.550\n0\n***\n\n\nWealthy Neighborhood Premium\n-$377,937\n-3.758\n0\n***\n\n\nExtra $/sq ft in Wealthy Areas\n$985\n33.103\n0\n***\n\n\n\n\n\nWe get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive (at what sq ft) = 384."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#breaking-down-the-coefficients",
    "href": "weekly-notes/week-06/week-06-class-lab.html#breaking-down-the-coefficients",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Breaking Down the Coefficients",
    "text": "Breaking Down the Coefficients\n\n\n🏘️ Not Wealthy Areas Equation: Price = $358,542 + $96 × LivingArea Interpretation:\nBase price: $358,542 Each sq ft adds: $96\n\n🏛️ Wealthy Areas Equation: Price = -$19,395 + $1,081 × LivingArea Interpretation:\nBase price: -$19,395 Each sq ft adds: $1,081\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe Interaction Effect Wealthy areas value each sq ft $985 more than non-wealthy areas!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#visualizing-the-interaction-effect",
    "href": "weekly-notes/week-06/week-06-class-lab.html#visualizing-the-interaction-effect",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Visualizing the Interaction Effect",
    "text": "Visualizing the Interaction Effect\n\n\n\n\n\n\n\n\n\nKey Observation: The lines are NOT parallel - that’s the interaction!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#compare-model-performance",
    "href": "weekly-notes/week-06/week-06-class-lab.html#compare-model-performance",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Compare Model Performance",
    "text": "Compare Model Performance\n\n\nCode\n# Compare R-squared\ncat(\"Model WITHOUT interaction R²:\", round(summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITHOUT interaction R²: 0.6156 \n\n\nCode\ncat(\"Model WITH interaction R²:\", round(summary(model_interact)$r.squared, 4), \"\\n\")\n\n\nModel WITH interaction R²: 0.7791 \n\n\nCode\ncat(\"Improvement:\", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.1635 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nModel Improvement Adding the interaction improves R² by 0.1635 (a 26.6% relative improvement)\nInterpretation: We explain 16.35% more variation in prices by allowing different slopes!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#policy-implications",
    "href": "weekly-notes/week-06/week-06-class-lab.html#policy-implications",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Policy Implications",
    "text": "Policy Implications\n\n\n\n\n\n\nWarningWhat This Tells Us About Boston’s Housing Market\n\n\n\n\nMarket Segmentation: Boston operates as TWO distinct housing markets\n\n\nLuxury market: Every sq ft is premium ($1081/sq ft)\nStandard market: Space valued, but lower premium ($96/sq ft)\n\n\nAffordability Crisis: The interaction amplifies inequality\n\n\nLarge homes in wealthy areas become exponentially more expensive\nCreates barriers to mobility between neighborhoods\n\n\nPolicy Design: One-size-fits-all policies may fail\n\n\nProperty tax assessments should account for neighborhood-specific valuation\nHousing assistance needs vary dramatically by area"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#when-not-to-use-interactions",
    "href": "weekly-notes/week-06/week-06-class-lab.html#when-not-to-use-interactions",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "When Not To Use Interactions",
    "text": "When Not To Use Interactions\n\n\n\n\n\n\nWarning\n\n\n\nWhen NOT to Use Interactions:\n\nSmall samples: Need sufficient data in each group\nOverfitting: Too many interactions make models unstable"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#polynomial-terms-non-linear-relationships",
    "href": "weekly-notes/week-06/week-06-class-lab.html#polynomial-terms-non-linear-relationships",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Polynomial Terms: Non-Linear Relationships",
    "text": "Polynomial Terms: Non-Linear Relationships\n\nWhen Straight Lines Don’t Fit\n\n\nSigns of Non-Linearity:\n\nCurved residual plots\nDiminishing returns\nAccelerating effects\nU-shaped or inverted-U patterns\nTheoretical reasons\n\n\nExamples:\n\nHouse age: depreciation then vintage premium\nTest scores: plateau after studying\nAdvertising: diminishing returns\nCrime prevention: early gains, then plateaus\n\n\n\n\n\n\n\n\n\nImportantPolynomial Regression\n\n\n\nSalePrice = β₀ + β₁(Age) + β₂(Age²) + ε\nThis allows for a curved relationship"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#theory-the-u-shaped-age-effect",
    "href": "weekly-notes/week-06/week-06-class-lab.html#theory-the-u-shaped-age-effect",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Theory: The U-Shaped Age Effect",
    "text": "Theory: The U-Shaped Age Effect\n\nWhy Would Age Have a Non-Linear Effect?\n\n\n\n\n🏗️ New Houses\n(0-20 years)\n\nModern amenities\nMove-in ready\nNo repairs needed\nHigh value\nSteep depreciation initially\n\n\n\n\n🏠 Middle-Aged\n(20-80 years)\n\nNeeds updates\nWear and tear\nNot yet “historic”\nLowest value\nTrough of the curve\n\n\n\n\n🏛️ Historic/Vintage\n(80+ years)\n\nArchitectural character\nHistoric districts\nPrestige value\nRising value\n“Vintage premium”\n\n\n\n\n\n\n\n\n\n\nNoteBoston Context\n\n\n\nBoston has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#create-age-variable",
    "href": "weekly-notes/week-06/week-06-class-lab.html#create-age-variable",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Create Age Variable",
    "text": "Create Age Variable\n\n\nCode\n# Calculate age from year built\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2025 - YR_BUILT)%&gt;% filter(Age &lt;2000)\n\n\n# Check the distribution of age\nsummary(boston.sf$Age)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   12.0    95.0   115.0   108.1   125.0   223.0 \n\n\nCode\n# Visualize age distribution\nggplot(boston.sf, aes(x = Age)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of House Age in Boston\",\n       x = \"Age (years)\",\n       y = \"Count\") +\n  theme_minimal()"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#first-linear-model-baseline",
    "href": "weekly-notes/week-06/week-06-class-lab.html#first-linear-model-baseline",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "First: Linear Model (Baseline)",
    "text": "First: Linear Model (Baseline)\n\n\nCode\n# Simple linear relationship\nmodel_age_linear &lt;- lm(SalePrice ~ Age + LivingArea, data = boston.sf)\n\nsummary(model_age_linear)$coef\n\n\n                Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -120808.8738 57836.98918 -2.088782 3.689911e-02\nAge            2834.0198   497.72013  5.694003 1.496246e-08\nLivingArea      202.8312    15.10373 13.429214 7.228187e-39\n\n\nInterpretation: Each additional year of age changes price by $2834.01 (assumed constant rate)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#visualize-is-the-relationship-linear",
    "href": "weekly-notes/week-06/week-06-class-lab.html#visualize-is-the-relationship-linear",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Visualize: Is the relationship Linear?",
    "text": "Visualize: Is the relationship Linear?"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#add-polynomial-term-age-squared",
    "href": "weekly-notes/week-06/week-06-class-lab.html#add-polynomial-term-age-squared",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Add Polynomial Term: Age Squared",
    "text": "Add Polynomial Term: Age Squared\n\n\nCode\n# Quadratic model (Age²)\nmodel_age_quad &lt;- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)\n\nsummary(model_age_quad)$coef\n\n\n                Estimate   Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 570397.14406 97894.237962  5.826667 6.938345e-09\nAge         -13007.51918  1896.446156 -6.858892 1.019524e-11\nI(Age^2)        80.88988     9.360652  8.641479 1.425208e-17\nLivingArea     203.75007    14.739041 13.823835 5.975020e-41\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe I() Function Why I(Age^2) instead of just Age^2? In R formulas, ^ has special meaning. I() tells R: “interpret this literally, compute Age²” Without I(): R would interpret it differently in the formula"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#interpreting-polynomial-coefficients",
    "href": "weekly-notes/week-06/week-06-class-lab.html#interpreting-polynomial-coefficients",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Interpreting Polynomial Coefficients",
    "text": "Interpreting Polynomial Coefficients\nModel equation: Price = $570,397 + -$13,008×Age + $80×Age² + $204×LivingArea\n\n\n\n\n\n\nWarning\n\n\n\n⚠️ Can’t Interpret Coefficients Directly!\nWith Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of Age = β₁ + 2×β₂×Age This means the effect changes at every age!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#compare-model-performance-1",
    "href": "weekly-notes/week-06/week-06-class-lab.html#compare-model-performance-1",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Compare Model Performance",
    "text": "Compare Model Performance\n\n\nCode\n# R-squared comparison\nr2_linear &lt;- summary(model_age_linear)$r.squared\nr2_quad &lt;- summary(model_age_quad)$r.squared\n\ncat(\"Linear model R²:\", round(r2_linear, 4), \"\\n\")\n\n\nLinear model R²: 0.1549 \n\n\nCode\ncat(\"Quadratic model R²:\", round(r2_quad, 4), \"\\n\")\n\n\nQuadratic model R²: 0.1959 \n\n\nCode\ncat(\"Improvement:\", round(r2_quad - r2_linear, 4), \"\\n\\n\")\n\n\nImprovement: 0.0409 \n\n\nCode\n# F-test: Is the Age² term significant?\nanova(model_age_linear, model_age_quad)\n\n\nAnalysis of Variance Table\n\nModel 1: SalePrice ~ Age + LivingArea\nModel 2: SalePrice ~ Age + I(Age^2) + LivingArea\n  Res.Df        RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1   1469 4.5786e+14                                   \n2   1468 4.3569e+14  1 2.2163e+13 74.675 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#check-residual-plot",
    "href": "weekly-notes/week-06/week-06-class-lab.html#check-residual-plot",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Check Residual Plot",
    "text": "Check Residual Plot\n\n\nCode\n# Compare residual plots\npar(mfrow = c(1, 2))\n\n# Linear model residuals\nplot(fitted(model_age_linear), residuals(model_age_linear),\n     main = \"Linear Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Quadratic model residuals  \nplot(fitted(model_age_quad), residuals(model_age_quad),\n     main = \"Quadratic Model Residuals\",\n     xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lty = 2)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#why-space-matters-for-housing-prices",
    "href": "weekly-notes/week-06/week-06-class-lab.html#why-space-matters-for-housing-prices",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Why Space Matters for Housing Prices",
    "text": "Why Space Matters for Housing Prices\n\nTobler’s First Law of Geography\n\n\n\n\n\n\nNote“Everything is related to everything else, but near things are more related than distant things”\n\n\n\n- Waldo Tobler, 1970\n\n\n\n\nWhat This Means for House Prices\n\nCrime nearby matters more than crime across the city\nParks within walking distance affect value\nYour immediate neighborhood defines your market\n\n\n\n\n\n\n\nImportantThe Challenge\n\n\n\nHow do we quantify “nearbyness” in a way our regression model can use?\nAnswer: Create spatial features that measure proximity to amenities/disamenities"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#three-approaches-to-spatial-features",
    "href": "weekly-notes/week-06/week-06-class-lab.html#three-approaches-to-spatial-features",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Three Approaches to Spatial Features",
    "text": "Three Approaches to Spatial Features\n\n1️⃣ Buffer Aggregation\nCount or sum events within a defined distance\nExample: Number of crimes within 500 feet\n\n\n2️⃣ k-Nearest Neighbors (kNN)\nAverage distance to k closest events\nExample: Average distance to 3 nearest violent crimes\n\n\n3️⃣ Distance to Specific Points\nStraight-line distance to important locations\nExample: Distance to downtown, nearest T station\n\n\n\n\n\n\nTip\n\n\n\nToday: We’ll create all three types using Boston crime data!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#load-and-prepare-crime-data",
    "href": "weekly-notes/week-06/week-06-class-lab.html#load-and-prepare-crime-data",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Load and Prepare Crime Data",
    "text": "Load and Prepare Crime Data\n\n\nRows: 200,977\nColumns: 21\n$ ...1                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ STREET              &lt;chr&gt; \"ALBERT ST\", \"ALBERT ST\", \"L ST\", \"HAROLD ST\", \"BL…\n$ OFFENSE_DESCRIPTION &lt;chr&gt; \"DRUGS - POSS CLASS B - INTENT TO MFR DIST DISP\", …\n$ SHOOTING            &lt;chr&gt; NA, NA, NA, \"Y\", NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ OFFENSE_CODE        &lt;dbl&gt; 1843, 1841, 2900, 413, 3111, 2900, 3402, 1841, 311…\n$ DISTRICT            &lt;chr&gt; \"B2\", \"B2\", \"C6\", \"B2\", \"B3\", \"B3\", \"A1\", \"D4\", \"E…\n$ REPORTING_AREA      &lt;dbl&gt; NA, NA, 230, 314, 465, 406, 76, 270, 499, 356, 170…\n$ OCCURRED_ON_DATE    &lt;dttm&gt; 2015-08-01 23:07:00, 2015-08-01 23:07:00, 2015-08…\n$ DAY_OF_WEEK         &lt;chr&gt; \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"S…\n$ MONTH               &lt;dbl&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,…\n$ HOUR                &lt;dbl&gt; 23, 23, 8, 18, 1, 1, 1, 17, 15, 13, 19, 12, 1, 1, …\n$ X_full_count        &lt;dbl&gt; 273, 273, 273, 273, 273, 273, 273, 273, 273, 273, …\n$ Long                &lt;dbl&gt; NA, NA, -71.03529, -71.09072, -71.09104, -71.07442…\n$ YEAR                &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 20…\n$ Lat                 &lt;dbl&gt; NA, NA, 42.33271, 42.31532, 42.28582, 42.27130, 42…\n$ INCIDENT_NUMBER     &lt;chr&gt; \"I152063654\", \"I152063654\", \"I152063493\", \"I152063…\n$ rank                &lt;dbl&gt; 0.314183, 0.314366, 0.314974, 0.315240, 0.315467, …\n$ X_id                &lt;dbl&gt; 243806, 243805, 243949, 243858, 244025, 244015, 24…\n$ OFFENSE_CODE_GROUP  &lt;chr&gt; \"Drug Violation\", \"Drug Violation\", \"Other\", \"Aggr…\n$ UCR_PART            &lt;chr&gt; \"Part Two\", \"Part Two\", \"Part Two\", \"Part One\", \"P…\n$ Location            &lt;chr&gt; \"(0.00000000, 0.00000000)\", \"(0.00000000, 0.000000…\n\n\nReading layer `BPDA_Neighborhood_Boundaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-06\\data\\BPDA_Neighborhood_Boundaries.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 26 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -71.19125 ymin: 42.22792 xmax: -70.92278 ymax: 42.39699\nGeodetic CRS:  WGS 84\n\n\n    xmin     ymin     xmax     ymax \n227125.8 886966.0 241483.7 904797.2 \n\n\n    xmin     ymin     xmax     ymax \n226505.0 887148.3 241435.5 904894.0 \n\n\n    xmin     ymin     xmax     ymax \n225465.6 886449.8 247575.8 905284.0 \n\n\nRecords with missing coordinates: 10103 \n\n\nViolent crime records: 39854 \n\n\n# A tibble: 9 × 2\n  OFFENSE_CODE_GROUP             n\n  &lt;chr&gt;                      &lt;int&gt;\n1 Larceny                    16485\n2 Larceny From Motor Vehicle  7899\n3 Aggravated Assault          4618\n4 Residential Burglary        4263\n5 Robbery                     3083\n6 Auto Theft                  2380\n7 Commercial Burglary          754\n8 Other Burglary               286\n9 Homicide                      86"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#visaulize-crime-locations",
    "href": "weekly-notes/week-06/week-06-class-lab.html#visaulize-crime-locations",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Visaulize: Crime Locations",
    "text": "Visaulize: Crime Locations"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#approach-1-buffer-aggregation",
    "href": "weekly-notes/week-06/week-06-class-lab.html#approach-1-buffer-aggregation",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Approach 1: Buffer Aggregation",
    "text": "Approach 1: Buffer Aggregation\n\n\nCode\n# Create buffer features - these will work now that CRS is correct\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crimes.Buffer = lengths(st_intersects(\n      st_buffer(geometry, 660),\n      crimes.sf\n    )),\n    crimes_500ft = lengths(st_intersects(\n      st_buffer(geometry, 500),\n      crimes.sf\n    ))\n  )\n\n# Check it worked\n#summary(boston.sf$crimes.Buffer)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#approach-2-k-nearest-neighborhoods-method",
    "href": "weekly-notes/week-06/week-06-class-lab.html#approach-2-k-nearest-neighborhoods-method",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Approach 2: k-Nearest Neighborhoods Method",
    "text": "Approach 2: k-Nearest Neighborhoods Method\n\n\nCode\n# Calculate distance matrix (houses to crimes)\ndist_matrix &lt;- st_distance(boston.sf, crimes.sf)\n\n# Function to get mean distance to k nearest neighbors\nget_knn_distance &lt;- function(dist_matrix, k) {\n  apply(dist_matrix, 1, function(distances) {\n    # Sort and take first k, then average\n    mean(as.numeric(sort(distances)[1:k]))\n  })\n}\n\n# Create multiple kNN features\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    crime_nn1 = get_knn_distance(dist_matrix, k = 1),\n    crime_nn3 = get_knn_distance(dist_matrix, k = 3),\n    crime_nn5 = get_knn_distance(dist_matrix, k = 5)\n  )\n\n# Check results\nsummary(boston.sf %&gt;% st_drop_geometry() %&gt;% select(starts_with(\"crime_nn\")))\n\n\n   crime_nn1         crime_nn3         crime_nn5     \n Min.   :  9.372   Min.   :  9.372   Min.   : 10.67  \n 1st Qu.: 29.944   1st Qu.: 37.615   1st Qu.: 44.30  \n Median : 54.036   Median : 62.835   Median : 70.27  \n Mean   : 70.750   Mean   : 83.298   Mean   : 94.66  \n 3rd Qu.: 90.145   3rd Qu.:107.735   3rd Qu.:127.14  \n Max.   :596.529   Max.   :637.559   Max.   :683.53  \n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: crime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#which-k-value-correlates-most-with-price",
    "href": "weekly-notes/week-06/week-06-class-lab.html#which-k-value-correlates-most-with-price",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Which k value correlates most with price?",
    "text": "Which k value correlates most with price?\n\n\nCode\n# Which k value correlates most with price?\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %&gt;%\n  cor(use = \"complete.obs\") %&gt;%\n  as.data.frame() %&gt;%\n  select(SalePrice)\n\n\n            SalePrice\nSalePrice  1.00000000\ncrime_nn1 -0.07317564\ncrime_nn3 -0.08726806\ncrime_nn5 -0.09599580\n\n\n\n\n\n\n\n\nTip\n\n\n\nFinding: The kNN feature with the strongest correlation tells us the relevant “zone of influence” for crime perception!\n\n\nApproach 3: Distance to Downtown\n\n\nCode\n# Define downtown Boston (Boston Common: 42.3551° N, 71.0656° W)\ndowntown &lt;- st_sfc(st_point(c(-71.0656, 42.3551)), crs = \"EPSG:4326\") %&gt;%\n  st_transform('ESRI:102286')\n\n# Calculate distance from each house to downtown\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),\n    dist_downtown_mi = dist_downtown_ft / 5280\n  )\n\n# Summary\nsummary(boston.sf$dist_downtown_mi)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.05467 0.80699 1.37601 1.39335 1.94227 2.77678 \n\n\n\nAll spatial features together\n\n\nCode\n# Summary of all spatial features created\nspatial_summary &lt;- boston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %&gt;%\n  summary()\n\nspatial_summary\n\n\n crimes.Buffer     crimes_500ft      crime_nn3       dist_downtown_mi \n Min.   :   4.0   Min.   :   0.0   Min.   :  9.372   Min.   :0.05467  \n 1st Qu.: 105.0   1st Qu.:  63.0   1st Qu.: 37.615   1st Qu.:0.80699  \n Median : 323.0   Median : 188.0   Median : 62.835   Median :1.37601  \n Mean   : 440.2   Mean   : 261.9   Mean   : 83.298   Mean   :1.39335  \n 3rd Qu.: 737.0   3rd Qu.: 427.0   3rd Qu.:107.735   3rd Qu.:1.94227  \n Max.   :3108.0   Max.   :2088.0   Max.   :637.559   Max.   :2.77678  \n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nType\nWhat it Measures\n\n\n\n\ncrimes.Buffer (660ft)\nBuffer count\nNumber of crimes near house\n\n\ncrimes_500ft\nBuffer count\nCrimes within 500ft\n\n\ncrime_nn3\nkNN distance\nAvg distance to 3 nearest crimes (ft)\n\n\ndist_downtown_mi\nPoint distance\nMiles from downtown Boston\n\n\n\n\n\n\n\nModel Comparison: Adding Spatial Features\n\n\nCode\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(Age = 2015 - YR_BUILT)  \n\n# Model 1: Structural only\nmodel_structural &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age, \n                       data = boston.sf)\n\n# Model 2: Add spatial features\nmodel_spatial &lt;- lm(SalePrice ~ LivingArea + R_BDRMS + Age +\n                    crimes_500ft + crime_nn3 + dist_downtown_mi,\n                    data = boston.sf)\n\n# Compare\ncat(\"Structural R²:\", round(summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nStructural R²: 0.2169 \n\n\nCode\ncat(\"With spatial R²:\", round(summary(model_spatial)$r.squared, 4), \"\\n\")\n\n\nWith spatial R²: 0.3529 \n\n\nCode\ncat(\"Improvement:\", round(summary(model_spatial)$r.squared - \n                          summary(model_structural)$r.squared, 4), \"\\n\")\n\n\nImprovement: 0.136"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#what-are-fixed-effects",
    "href": "weekly-notes/week-06/week-06-class-lab.html#what-are-fixed-effects",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "What Are Fixed Effects?",
    "text": "What Are Fixed Effects?\nFixed Effects = Categorical variables that capture all unmeasured characteristics of a group\nIn hedonic models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn’t explicitly measure\n\nWe technically already did this when I went over categorical data!\n\n\nWhat They Capture:\n\nSchool quality\n“Prestige” or reputation\nWalkability\nAccess to jobs\nCultural amenities\nThings we can’t easily measure\n\n\nThe Code:\n\n\nCode\n# Add neighborhood fixed effects\nreg5 &lt;- lm(\n  SalePrice ~ LivingArea + Age + \n              crimes_500ft + \n              parks_nn3 + \n              as.factor(name),  # FE\n  data = boston.sf\n)\n\n\nR creates a dummy for each neighborhood automatically!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#how-fixed-effects-work",
    "href": "weekly-notes/week-06/week-06-class-lab.html#how-fixed-effects-work",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "How Fixed Effects Work",
    "text": "How Fixed Effects Work\n\n\nCode\n# Behind the scenes, R creates dummies:\n# is_BackBay = 1 if Back Bay, 0 otherwise\n# is_Beacon = 1 if Beacon Hill, 0 otherwise\n# is_Allston = 1 if Allston, 0 otherwise\n# ... (R drops one as reference category)\n\n\nInterpretation Example:\nCoefficients:\n(Intercept)           50000\nLivingArea              150\nnameBack_Bay          85000   ← $85k premium vs. reference\nnameBeacon_Hill      125000   ← $125k premium  \nnameAllston          -15000   ← $15k discount\nEach coefficient = price premium/discount for that neighborhood (holding all else constant)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#why-use-fixed-effects",
    "href": "weekly-notes/week-06/week-06-class-lab.html#why-use-fixed-effects",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Why Use Fixed Effects?",
    "text": "Why Use Fixed Effects?\n\n\n\nDramatically Improve Prediction\nModel Comparison (R²):\n- Structural only:     0.58\n- + Spatial features:  0.67\n- + Fixed Effects:     0.81 ✓\nWhy such a big jump?\n\nNeighborhoods bundle many unmeasured factors\nSchool districts\nJob access\nAmenities\n“Cool factor”\n\n\n\n\nCoefficients Change\nCrime coefficient:\n\nWithout FE: -$125/crime\nWith FE: -$85/crime\n\nWhy?\n\nWithout FE: captured neighborhood confounders too\nWith FE: neighborhoods “absorb” other differences\nNow just the crime effect\n\n\n\n\nTrade-off: FE are powerful but they’re a black box - we don’t know WHY Back Bay commands a premium"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#lets-compare-all-our-models",
    "href": "weekly-notes/week-06/week-06-class-lab.html#lets-compare-all-our-models",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Let’s Compare All Our Models",
    "text": "Let’s Compare All Our Models\n\n\nCode\n# Model 3: Structural Only\nreg3 &lt;- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH, \n           data = boston.sf)\n\n# Model 4: Add Spatial Features  \nreg4 &lt;- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi,\n           data = boston.sf)\n\nboston.sf &lt;- boston.sf %&gt;%\n  st_join(nhoods, join = st_intersects)\n\n# Model 5: Add Fixed Effects\nreg5 &lt;- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +\n                       crimes_500ft + crime_nn3+ dist_downtown_mi +\n                       as.factor(name),\n           data = boston.sf)\nlibrary(stargazer)\n# Compare in-sample fit\nstargazer(reg3, reg4, reg5, type = \"text\")\n\n\n\n=========================================================================================================\n                                                         Dependent variable:                             \n                             ----------------------------------------------------------------------------\n                                                              SalePrice                                  \n                                       (1)                      (2)                       (3)            \n---------------------------------------------------------------------------------------------------------\nLivingArea                          148.860***               111.501***                118.107***        \n                                     (21.242)                 (20.077)                  (8.586)          \n                                                                                                         \nAge                                 322.029***                123.069                  212.973***        \n                                     (80.730)                 (75.779)                  (30.508)         \n                                                                                                         \nR_FULL_BTH                        113,372.800***            40,173.330*              50,895.880***       \n                                   (23,735.650)             (22,987.460)              (9,458.101)        \n                                                                                                         \ncrimes_500ft                                                 762.817***               -211.220***        \n                                                              (85.812)                  (42.796)         \n                                                                                                         \ncrime_nn3                                                   2,219.712***               466.580***        \n                                                             (242.880)                 (105.506)         \n                                                                                                         \ndist_downtown_mi                                          -202,215.700***           -103,165.100***      \n                                                            (29,626.430)              (30,260.730)       \n                                                                                                         \nas.factor(name)Back Bay                                                             7,668,782.000***     \n                                                                                     (152,445.600)       \n                                                                                                         \nas.factor(name)Bay Village                                                          1,374,060.000***     \n                                                                                     (225,127.300)       \n                                                                                                         \nas.factor(name)Beacon Hill                                                          1,720,668.000***     \n                                                                                     (101,788.600)       \n                                                                                                         \nas.factor(name)Brighton                                                               -186,898.400       \n                                                                                     (120,211.300)       \n                                                                                                         \nas.factor(name)Charlestown                                                            -99,353.820        \n                                                                                      (92,604.600)       \n                                                                                                         \nas.factor(name)Dorchester                                                           -555,954.300***      \n                                                                                      (85,853.130)       \n                                                                                                         \nas.factor(name)Downtown                                                               216,317.000        \n                                                                                     (226,782.100)       \n                                                                                                         \nas.factor(name)East Boston                                                          -574,986.800***      \n                                                                                      (87,455.850)       \n                                                                                                         \nas.factor(name)Fenway                                                               1,034,562.000***     \n                                                                                     (169,543.800)       \n                                                                                                         \nas.factor(name)Hyde Park                                                            -480,342.800***      \n                                                                                      (93,199.370)       \n                                                                                                         \nas.factor(name)Jamaica Plain                                                         -198,209.800**      \n                                                                                      (87,603.690)       \n                                                                                                         \nas.factor(name)Mattapan                                                             -561,786.900***      \n                                                                                      (90,587.160)       \n                                                                                                         \nas.factor(name)Mission Hill                                                          214,567.100**       \n                                                                                     (101,389.300)       \n                                                                                                         \nas.factor(name)Roslindale                                                           -442,450.500***      \n                                                                                      (89,746.210)       \n                                                                                                         \nas.factor(name)Roxbury                                                              -592,878.500***      \n                                                                                      (88,945.650)       \n                                                                                                         \nas.factor(name)South Boston                                                         -290,863.400***      \n                                                                                      (90,664.580)       \n                                                                                                         \nas.factor(name)South End                                                            1,750,702.000***     \n                                                                                      (98,722.040)       \n                                                                                                         \nas.factor(name)West Roxbury                                                         -380,868.400***      \n                                                                                      (91,694.890)       \n                                                                                                         \nConstant                            50,899.030             199,477.900***            778,604.600***      \n                                   (39,788.170)             (70,002.400)             (100,039.000)       \n                                                                                                         \n---------------------------------------------------------------------------------------------------------\nObservations                          1,485                    1,485                     1,485           \nR2                                    0.152                    0.276                     0.885           \nAdjusted R2                           0.150                    0.273                     0.883           \nResidual Std. Error          557,399.300 (df = 1481)  515,703.200 (df = 1478)   206,473.300 (df = 1460)  \nF Statistic                  88.527*** (df = 3; 1481) 93.739*** (df = 6; 1478) 469.541*** (df = 24; 1460)\n=========================================================================================================\nNote:                                                                         *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nBut in-sample R² can be misleading! We need cross-validation…"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#cv-recap-from-last-week",
    "href": "weekly-notes/week-06/week-06-class-lab.html#cv-recap-from-last-week",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "CV Recap (From Last Week)",
    "text": "CV Recap (From Last Week)\nThree common validation approaches:\n\nTrain/Test Split - 80/20 split, simple but unstable\nk-Fold Cross-Validation - Split into k folds, train on k-1, test on 1, repeat\nLOOCV - Leave one observation out at a time (special case of k-fold)\n\nToday we’ll use k-fold CV to compare our hedonic models\n\n\n\n\nCode\nlibrary(caret)\n\nctrl &lt;- trainControl(\n  method = \"cv\",\n  number = 10  # 10-fold CV\n)\n\nmodel_cv &lt;- train(\n  SalePrice ~ LivingArea + Age,\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n\n\n\nWhy CV?\n\nTells us how well model predicts NEW data\nMore honest than in-sample R²\nHelps detect overfitting"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#comparing-models-with-cv",
    "href": "weekly-notes/week-06/week-06-class-lab.html#comparing-models-with-cv",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Comparing Models with CV",
    "text": "Comparing Models with CV\n\n\nCode\nlibrary(caret)\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\n# Model 1: Structural\ncv_m1 &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Fixed Effects (BUT WAIT - there's a (potential) problem!)\ncv_m3 &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Compare\ndata.frame(\n  Model = c(\"Structural\", \"Spatial\", \"Fixed Effects\"),\n  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE)\n)"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#the-problem-sparse-categories",
    "href": "weekly-notes/week-06/week-06-class-lab.html#the-problem-sparse-categories",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "⚠️ The Problem: Sparse Categories",
    "text": "⚠️ The Problem: Sparse Categories\n\nWhen CV Fails with Categorical Variables\n\n\nCode\n# You might see this error:\n#Error in model.frame.default: \n#  factor 'name' has new level 'West End'\n\n\nWhat happened?\n\nRandom split created 10 folds\nAll “West End” sales ended up in ONE fold (the test fold)\nTraining folds never saw “West End”\nModel can’t predict for a category it never learned\n\n\n\n\n\n\n\nImportantThe Issue\n\n\n\nWhen neighborhoods have very few sales (&lt;10), random CV splits can put all instances in the same fold, breaking the model."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#check-your-data-first",
    "href": "weekly-notes/week-06/week-06-class-lab.html#check-your-data-first",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Check Your Data First!",
    "text": "Check Your Data First!\n\n\nCode\n# ALWAYS run this before CV with categorical variables\ncategory_check %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name) %&gt;%\n  arrange(n)\n\nprint(category_check)\n\n\nTypical output:\nname                n\nWest End            3  ⚠️ Problem!\nBay Village         5  ⚠️ Risky\nLeather District    8  ⚠️ Borderline\nBack Bay           89  ✓ Safe\nSouth Boston      112  ✓ Safe\n\n\n\n\n\n\nTip\n\n\n\nRule of Thumb: Categories with n &lt; 10 will likely cause CV problems"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#solution-group-small-neighborhoods",
    "href": "weekly-notes/week-06/week-06-class-lab.html#solution-group-small-neighborhoods",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Solution: Group Small Neighborhoods",
    "text": "Solution: Group Small Neighborhoods\nMost practical approach:\n\n\nCode\n# Step 1: Add count column\nboston.sf &lt;- boston.sf %&gt;%\n  add_count(name)\n\n# Step 2: Group small neighborhoods\nboston.sf &lt;- boston.sf %&gt;%\n  mutate(\n    name_cv = if_else(\n      n &lt; 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(name)            # Keep original\n    ),\n    name_cv = as.factor(name_cv)\n  )\n\n# Step 3: Use grouped version in CV\ncv_model_fe &lt;- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + \n              as.factor(name_cv),   # Use name_cv, not name!\n  data = boston.sf,\n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n\n\nTrade-off: Lose granularity for small neighborhoods, but avoid CV crashes"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#alternative-drop-sparse-categories",
    "href": "weekly-notes/week-06/week-06-class-lab.html#alternative-drop-sparse-categories",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Alternative: Drop Sparse Categories",
    "text": "Alternative: Drop Sparse Categories\nIf grouping doesn’t make sense:\n\n\nCode\n# Remove neighborhoods with &lt; 10 sales\nneighborhood_counts %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name)\n\nkeep_neighborhoods %&gt;%\n  filter(n &gt;= 10) %&gt;%\n  pull(name)\n\nboston_filtered %&gt;%\n  filter(name %in% keep_neighborhoods)\n\ncat(\"Removed\", nrow(boston.sf) - nrow(boston_filtered), \"observations\")\n\n\n\n\n\n\n\n\nWarning\n\n\n\nConsider carefully: Which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why."
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#my-recommended-workflow",
    "href": "weekly-notes/week-06/week-06-class-lab.html#my-recommended-workflow",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "My Recommended Workflow",
    "text": "My Recommended Workflow\n\n\nCode\n# 1. Check category sizes\nboston.sf %&gt;%\n  st_drop_geometry() %&gt;%\n  count(name) %&gt;%\n  arrange(n) %&gt;%\n  print()\n\n\n# A tibble: 19 × 2\n   name              n\n   &lt;chr&gt;         &lt;int&gt;\n 1 Bay Village       1\n 2 Downtown          1\n 3 Fenway            2\n 4 Back Bay          3\n 5 Allston           6\n 6 Brighton          6\n 7 Mission Hill     14\n 8 Beacon Hill      17\n 9 South End        20\n10 Roxbury          63\n11 Charlestown      65\n12 Mattapan         65\n13 South Boston     80\n14 Jamaica Plain   113\n15 Roslindale      142\n16 East Boston     149\n17 Hyde Park       152\n18 West Roxbury    242\n19 Dorchester      344\n\n\nCode\n# 2. Group if needed\nboston.sf &lt;- boston.sf %&gt;%\n  add_count(name) %&gt;%\n  mutate(\n    name_cv = if_else(n &lt; 10, \"Small_Neighborhoods\", as.character(name)),\n    name_cv = as.factor(name_cv)\n  )\n\n# 3. Set up CV\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\n# 4. Use grouped neighborhoods in ALL models with FE\nmodel &lt;- train(\n  SalePrice ~ LivingArea + Age + crimes_500ft + as.factor(name_cv),\n  data = boston.sf,\n  method = \"lm\",\n  trControl = ctrl\n)\n\n# 5. Report\ncat(\"10-fold CV RMSE:\", round(model$results$RMSE, 0), \"\\n\")\n\n\n10-fold CV RMSE: 339127"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#full-model-comparison-with-cv",
    "href": "weekly-notes/week-06/week-06-class-lab.html#full-model-comparison-with-cv",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Full Model Comparison with CV",
    "text": "Full Model Comparison with CV\n\n\nCode\nlibrary(caret)\n\n# Prep data\nboston.sf &lt;- boston.sf %&gt;%\n  add_count(name) %&gt;%\n  mutate(name_cv = if_else(n &lt; 10, \"Small_Neighborhoods\", as.character(name)),\n         name_cv = as.factor(name_cv))\n\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\n# Compare models\ncv_structural &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_spatial &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\ncv_fixedeffects &lt;- train(\n  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + \n              as.factor(name_cv),\n  data = boston.sf, method = \"lm\", trControl = ctrl\n)\n\n# Results\ndata.frame(\n  Model = c(\"Structural\", \"+ Spatial\", \"+ Fixed Effects\"),\n  RMSE = c(cv_structural$results$RMSE, \n           cv_spatial$results$RMSE, \n           cv_fixedeffects$results$RMSE)\n)\n\n\n            Model     RMSE\n1      Structural 539175.1\n2       + Spatial 512337.2\n3 + Fixed Effects 345039.6"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#expected-results",
    "href": "weekly-notes/week-06/week-06-class-lab.html#expected-results",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Expected Results",
    "text": "Expected Results\nTypical pattern:\nModel              RMSE      Interpretation\nStructural        $533,330.60   Baseline - just house characteristics\n+ Spatial         $500,421.5    Adding location features helps!\n+ Fixed Effects   $347,261.30   Neighborhoods capture a LOT\nNote: these values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact\nKey Insight: Each layer improves out-of-sample prediction, with fixed effects providing the biggest boost\nWhy? Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can’t easily quantify individually"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#investigating-those-errors",
    "href": "weekly-notes/week-06/week-06-class-lab.html#investigating-those-errors",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Investigating those errors…",
    "text": "Investigating those errors…\n\n\n\n\n\n\n\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n       0   417500   519000   647981   650000 11600000 \n\n\n# A tibble: 10 × 3\n   SalePrice LivingArea name       \n       &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n 1  11600000       9908 Back Bay   \n 2   9500000       5283 Back Bay   \n 3   6350000       4765 Back Bay   \n 4   4600000       3018 Beacon Hill\n 5   4300000       3981 South End  \n 6   4274500       5439 Beacon Hill\n 7   4200000       4146 South End  \n 8   3900000       4396 Beacon Hill\n 9   3877500       3849 Beacon Hill\n10   3810000       4449 Beacon Hill\n\n\n\n\n\n\n\n\n\nLook for: - Prices over $2-3 million (could be luxury condos or errors) - Prices near $0 (data errors) - Long right tail in histogram"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#team-exercise-practice-what-weve-done-and-build-your-best-model",
    "href": "weekly-notes/week-06/week-06-class-lab.html#team-exercise-practice-what-weve-done-and-build-your-best-model",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Team Exercise: Practice What We’ve Done and Build Your Best Model",
    "text": "Team Exercise: Practice What We’ve Done and Build Your Best Model\nGoal: Create a comprehensive hedonic model using ALL concepts from today (and last week)\nRequirements:\n\nStructural variables (including categorical)\nSpatial features (create your own - nhttps://data.boston.gov/group/geospatial\nAt least one interaction term\nOne non-linear (polynomial term)\nNeighborhood fixed effects (handle sparse categories!)\n10-fold cross-validation\nReport final RMSE"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#use-diagnostics-from-last-week-as-you-build",
    "href": "weekly-notes/week-06/week-06-class-lab.html#use-diagnostics-from-last-week-as-you-build",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Use diagnostics from last week as you build!",
    "text": "Use diagnostics from last week as you build!"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#report-out-on-board",
    "href": "weekly-notes/week-06/week-06-class-lab.html#report-out-on-board",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Report Out on Board",
    "text": "Report Out on Board\nEach team will share (3 minutes):\n\nVariables used:\n\nStructural: ____________\nSpatial: ____________\nNon-linear: __________\nInteractions: ____________\nFixed Effects: Yes/No, how handled sparse categories?\n\nFinal cross-validated RMSE: $____________ and MAE $______________\nOne insight:\n\nWhat made the biggest difference?\nDid anything surprise you?\nWhich variables mattered most?"
  },
  {
    "objectID": "weekly-notes/week-06/week-06-class-lab.html#tips-for-success",
    "href": "weekly-notes/week-06/week-06-class-lab.html#tips-for-success",
    "title": "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION",
    "section": "Tips for Success",
    "text": "Tips for Success\n\n\n\n✅ Do:\n\nStart simple, add complexity\nCheck for NAs: sum(is.na())\nTest on small subset first\nComment your code\nCheck coefficient signs\nUse glimpse(), summary()\n\n\n\n❌ Don’t:\n\nAdd 50 variables at once\nIgnore errors\nForget st_drop_geometry() for non-spatial operations\nSkip sparse category check\n\n\n\n\nCommon Errors\n“Factor has new levels” → Group sparse categories\n“Computationally singular” → Remove collinear variables\nVery high RMSE → Check outliers, scale\nCV takes forever → Simplify model or reduce folds\nNegative R² → Model worse than mean, rethink variables"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html",
    "href": "weekly-notes/week-07/week-07-class-lab.html",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "",
    "text": "We noticed something in your homework submissions…\nMany of you have messy output in your rendered HTML files from tigris and tidycensus functions.\nExample of what we’re seeing:\nRetrieving data for the year 2022\n  |======================================================================| 100%\n  |======================================================================| 100%\nDownloading: 4.3 MB     \nDownloading: 3.7 MB\nThis clutters your professional report!\n\n\n\n\nWhat’s happening:\nWhen you use tigris or tidycensus functions, they show download progress by default.\n\n\n\n\n\n\nCode\ntracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE\n)\n\n\nShows:\nGetting data from the 2018-2022 5-year ACS\n  |======| 100%\nThis is helpful when coding!\n\n\n\n\nAll those progress messages appear as ugly text in your final document.\nThis looks unprofessional and makes your work harder to read.\nSolution: Suppress progress messages in your code chunks\n\n\n\n\n\n\n\nTwo ways to fix this:\n\n\n\n\nCode\n# Add progress = FALSE to EVERY tigris/tidycensus call\ntracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE,\n  progress = FALSE  # &lt;-- Add this!\n)\n\nroads &lt;- roads(state = \"PA\", \n               county = \"Philadelphia\",\n               progress = FALSE)  # &lt;-- Add this!\n\n\n\n\n\n\n\nCode\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant📝 To-Do Before We Grade\n\n\n\nPlease go back to your homework and:\n\nOpen your .qmd file\nAdd progress = FALSE to all get_acs(), get_decennial(), and tigris function calls\n\nOR add the global options at the top of your document\n\nRe-render your document (Click “Render” button)\nCheck that the HTML output is clean\nRe-submit on Canvas if needed (but it should all update on your website once you re-render!)\n\nDeadline: Before our next class meeting\nWhy this matters: We gotta look good!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#before-we-start-a-quick-note-on-your-submissions",
    "href": "weekly-notes/week-07/week-07-class-lab.html#before-we-start-a-quick-note-on-your-submissions",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "",
    "text": "We noticed something in your homework submissions…\nMany of you have messy output in your rendered HTML files from tigris and tidycensus functions.\nExample of what we’re seeing:\nRetrieving data for the year 2022\n  |======================================================================| 100%\n  |======================================================================| 100%\nDownloading: 4.3 MB     \nDownloading: 3.7 MB\nThis clutters your professional report!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-problem-progress-bars-in-rendered-output",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-problem-progress-bars-in-rendered-output",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "",
    "text": "What’s happening:\nWhen you use tigris or tidycensus functions, they show download progress by default.\n\n\n\n\n\n\nCode\ntracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE\n)\n\n\nShows:\nGetting data from the 2018-2022 5-year ACS\n  |======| 100%\nThis is helpful when coding!\n\n\n\n\nAll those progress messages appear as ugly text in your final document.\nThis looks unprofessional and makes your work harder to read.\nSolution: Suppress progress messages in your code chunks"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-solution-add-progress-false",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-solution-add-progress-false",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "",
    "text": "Two ways to fix this:\n\n\n\n\nCode\n# Add progress = FALSE to EVERY tigris/tidycensus call\ntracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"B01003_001\",\n  state = \"PA\",\n  geometry = TRUE,\n  progress = FALSE  # &lt;-- Add this!\n)\n\nroads &lt;- roads(state = \"PA\", \n               county = \"Philadelphia\",\n               progress = FALSE)  # &lt;-- Add this!\n\n\n\n\n\n\n\nCode\n# Add this near the top of your .qmd after loading libraries\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)  # Suppress tigris progress bars"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#action-required-re-render-before-final-grading",
    "href": "weekly-notes/week-07/week-07-class-lab.html#action-required-re-render-before-final-grading",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "",
    "text": "Important📝 To-Do Before We Grade\n\n\n\nPlease go back to your homework and:\n\nOpen your .qmd file\nAdd progress = FALSE to all get_acs(), get_decennial(), and tigris function calls\n\nOR add the global options at the top of your document\n\nRe-render your document (Click “Render” button)\nCheck that the HTML output is clean\nRe-submit on Canvas if needed (but it should all update on your website once you re-render!)\n\nDeadline: Before our next class meeting\nWhy this matters: We gotta look good!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#agenda-overview",
    "href": "weekly-notes/week-07/week-07-class-lab.html#agenda-overview",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Agenda Overview",
    "text": "Agenda Overview\nPart 1: Review & Connect\n\nWhere we’ve been and where we’re going\nThe regression workflow so far\n\nPart 2: Evaluating Model Quality\n\nTrain/test splits vs. cross-validation review\nSpatial patterns in errors\nIntroduction to spatial autocorrelation\n\nPart 3: Moran’s I as a Diagnostic\n\nUnderstanding spatial clustering\nCalculating and interpreting Moran’s I\nLocal vs. global measures\n\nBREAK (10 min)\nPart 4: Midterm Work Session (90+ min)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-journey-so-far",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-journey-so-far",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "The Journey So Far",
    "text": "The Journey So Far\nWeeks 1-3: Data foundations\n\nCensus data, tidycensus, spatial data basics\nVisualization and exploratory analysis\n\nWeek 5: Linear regression fundamentals\n\nY = f(X) + ε framework\nTrain/test splits, cross-validation\nChecking assumptions\n\nWeek 6: Expanding the toolkit\n\nCategorical variables and interactions\nSpatial features (buffers, kNN, distance)\nNeighborhood fixed effects"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#last-weeks-key-innovation",
    "href": "weekly-notes/week-07/week-07-class-lab.html#last-weeks-key-innovation",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Last Week’s Key Innovation",
    "text": "Last Week’s Key Innovation\nYou learned to create spatial features:\n\n\nCode\n# Buffer aggregation\ncrimes_500ft &lt;- count_features_in_buffer(houses, crimes, 500)\n\n# k-Nearest Neighbors\ncrime_nn3 &lt;- average_distance_to_knn(houses, crimes, k=3)\n\n# Fixed effects\nlm(SalePrice ~ ... + as.factor(neighborhood))\n\n\nToday’s Question:\n\nHow do we know if our model still has spatial structure in its errors?\n\nIf errors are spatially clustered, we’re missing something important!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-regression-workflow-updated",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-regression-workflow-updated",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "The Regression Workflow (Updated)",
    "text": "The Regression Workflow (Updated)\n\n\nBuilding the model:\n\nVisualize relationships\nEngineer features\nFit the model\nEvaluate performance (RMSE, R²)\nCheck assumptions\n\n\nNEW: Spatial diagnostics:\n\nAre errors random or clustered?\nDo we predict better in some areas?\nIs there remaining spatial structure?\n\n\n\n\n\n\n\n\n\nImportantWhy This Matters\n\n\n\nIf errors cluster spatially, it suggests:\n\nMissing spatial variables\nMisspecified relationships\nNon-stationarity (relationships vary across space)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#what-are-model-errors",
    "href": "weekly-notes/week-07/week-07-class-lab.html#what-are-model-errors",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "What Are Model Errors?",
    "text": "What Are Model Errors?\nPrediction error for observation i:\n\\[e_i = \\hat{y}_i - y_i\\]\nWhere:\n\n\\(\\hat{y}_i\\) = predicted value\n\\(y_i\\) = actual value\n\nIn our house price context:\n\n\nCode\n# Load packages and data\nlibrary(sf)\nlibrary(here)\nlibrary(tidyverse)\n\n# Load Boston housing data\nboston &lt;- read_csv(here(\"data/boston.csv\"))\n\n# Simple model: Predict price from living area\nbaseline_model &lt;- lm(SalePrice ~ LivingArea, data = boston)\nsummary(baseline_model)\n\n\n\nCall:\nlm(formula = SalePrice ~ LivingArea, data = boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-855962 -219491  -68291   55248 9296561 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 157968.32   35855.59   4.406 1.13e-05 ***\nLivingArea     216.54      14.47  14.969  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 563800 on 1483 degrees of freedom\nMultiple R-squared:  0.1313,    Adjusted R-squared:  0.1307 \nF-statistic: 224.1 on 1 and 1483 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nboston_test &lt;- boston %&gt;%\n  mutate(\n    predicted = predict(baseline_model, boston),\n    error = predicted - SalePrice,\n    abs_error = abs(error),\n    pct_error = abs(error) / SalePrice\n  )"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#good-errors-vs.-bad-errors",
    "href": "weekly-notes/week-07/week-07-class-lab.html#good-errors-vs.-bad-errors",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Good Errors vs. Bad Errors",
    "text": "Good Errors vs. Bad Errors\n\n\n** Random errors (good)**\n\nNo systematic pattern\nScattered across space\nPrediction equally good everywhere\nModel captures key relationships\n\n\n** Clustered errors (bad)**\n\nSpatial pattern visible\nUnder/over-predict in areas\nModel misses something about location\nNeed more spatial features!\n\n\n\nHow do we test this?\nLook for spatial autocorrelation in the errors"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#toblers-first-law-revisited",
    "href": "weekly-notes/week-07/week-07-class-lab.html#toblers-first-law-revisited",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Tobler’s First Law (Revisited)",
    "text": "Tobler’s First Law (Revisited)\n\n\n\n\n\n\nNoteThe First Law of Geography\n\n\n\n“Everything is related to everything else, but near things are more related than distant things.”\n— Waldo Tobler (1970)\n\n\nApplied to house prices:\n\nNearby houses have similar prices\nNearby neighborhoods have similar characteristics\nCrime in one block affects adjacent blocks\n\nApplied to model errors:\n\nIf nearby houses have similar errors…\n…our model is missing a spatial pattern!\nNeed to add more spatial features or fixed effects"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#visualizing-error-patterns",
    "href": "weekly-notes/week-07/week-07-class-lab.html#visualizing-error-patterns",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Visualizing Error Patterns",
    "text": "Visualizing Error Patterns\nMap your errors to see patterns:\n\n\nCode\nlibrary(sf)\noptions(scipen = 999)\n# Convert boston data to sf object\nboston_test &lt;- boston_test %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102286')  # MA State Plane (feet)\n\n\n# Install if needed: install.packages(\"hexbin\")\nlibrary(hexbin)\n\nggplot(boston_test) +\n  geom_sf(aes(fill = error),\n          shape = 21,\n          size = 1,\n          alpha = 0.6,\n          stroke = 0.2) +\n  scale_fill_gradient2(\n    low = \"blue\",\n    mid = \"white\",\n    high = \"red\",\n    midpoint = 0,\n    limits = c(-300000, 300000),\n    oob = scales::squish\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nWhat to look for:\n\nBlue clusters (we under-predict)\nRed clusters (we over-predict)\nRandom scatter (good!)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#scatter-plot-spatial-lag-of-errors",
    "href": "weekly-notes/week-07/week-07-class-lab.html#scatter-plot-spatial-lag-of-errors",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Scatter Plot: Spatial Lag of Errors",
    "text": "Scatter Plot: Spatial Lag of Errors\nCreate the spatial lag:\n\n\nCode\nlibrary(spdep)\n\n# Define neighbors (5 nearest)\ncoords &lt;- st_coordinates(boston_test)\nneighbors &lt;- knn2nb(knearneigh(coords, k=5))\nweights &lt;- nb2listw(neighbors, style=\"W\")\n\n# Calculate spatial lag of errors\nboston_test$error_lag &lt;- lag.listw(weights, boston_test$error)\n\n\nThen plot:\n\n\nCode\nggplot(boston_test, aes(x=error_lag, y=error)) +\n  geom_point(alpha=0.5) +\n  geom_smooth(method=\"lm\", color=\"red\") +\n  labs(title=\"Is my error correlated with neighbors' errors?\",\n       x=\"Avg error of 5 nearest neighbors\",\n       y=\"My error\")"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#what-is-morans-i",
    "href": "weekly-notes/week-07/week-07-class-lab.html#what-is-morans-i",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "What is Moran’s I?",
    "text": "What is Moran’s I?\nMoran’s I measures spatial autocorrelation\nRange: -1 to +1\n\n+1 = Perfect positive correlation (clustering)\n0 = Random spatial pattern\n-1 = Perfect negative correlation (dispersion)\n\nFormula (look’s scary, but its so intuitive!):\n\\[I = \\frac{n \\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i \\sum_j w_{ij} \\sum_i (x_i - \\bar{x})^2}\\]\nWhere \\(w_{ij}\\) = spatial weight between locations i and j"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#worked-example-understanding-the-formula",
    "href": "weekly-notes/week-07/week-07-class-lab.html#worked-example-understanding-the-formula",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Worked Example: Understanding the Formula",
    "text": "Worked Example: Understanding the Formula\n5 houses in a row, predicting sale prices:\n\n\n\nHouse\nActual Price\nPredicted Price\nError\n\n\n\n\nA\n$500k\n$400k\n+$100k\n\n\nB\n$480k\n$400k\n+$80k\n\n\nC\n$420k\n$400k\n+$20k\n\n\nD\n$350k\n$400k\n-$50k\n\n\nE\n$330k\n$400k\n-$70k\n\n\n\nMean error = +$16k\nThe question: Are errors for nearby houses similar to each other?"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#step-1-calculate-deviations-from-mean",
    "href": "weekly-notes/week-07/week-07-class-lab.html#step-1-calculate-deviations-from-mean",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Step 1: Calculate Deviations from Mean",
    "text": "Step 1: Calculate Deviations from Mean\nSubtract the mean error from each house’s error:\n\n\n\nHouse\nError\nMean Error\nDeviation from Mean\n\n\n\n\nA\n+$100k\n+$16k\n+$84k\n\n\nB\n+$80k\n+$16k\n+$64k\n\n\nC\n+$20k\n+$16k\n+$4k\n\n\nD\n-$50k\n+$16k\n-$66k\n\n\nE\n-$70k\n+$16k\n-$86k\n\n\n\nPositive deviation = we over-predicted (actual &gt; predicted)\nNegative deviation = we under-predicted (actual &lt; predicted)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#step-2-multiply-neighbor-deviations",
    "href": "weekly-notes/week-07/week-07-class-lab.html#step-2-multiply-neighbor-deviations",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Step 2: Multiply Neighbor Deviations",
    "text": "Step 2: Multiply Neighbor Deviations\nFor each neighbor pair, multiply their deviations:\n\n\nNeighbor Pairs:\n\nA-B: \\((+84k) \\times (+64k) = +5,376\\)\nB-C: \\((+64k) \\times (+4k) = +256\\)\nC-D: \\((+4k) \\times (-66k) = -264\\)\nD-E: \\((-66k) \\times (-86k) = +5,676\\)\n\nSum of products = 11,044\n\nWhat does this mean?\nPositive products = similar neighbors - A-B: both over-predicted (both positive) - D-E: both under-predicted (both negative)\nNegative product = dissimilar neighbors\n- C-D: one over, one under\nThe pattern: High-error houses cluster together, low-error houses cluster together"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-intuition-behind-morans-i",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-intuition-behind-morans-i",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "The Intuition Behind Moran’s I",
    "text": "The Intuition Behind Moran’s I\nThe formula is really just asking:\n\n“When I’m above/below average, are my neighbors also above/below average?”\n\nBreaking it down:\n\n\\((x_i - \\bar{x})\\) = How far is my house’s error from the mean?\n\\((x_j - \\bar{x})\\) = How far is my neighbor’s error from the mean?\nMultiply them:\n\nIf both positive or both negative → positive product (similar)\nIf opposite signs → negative product (dissimilar)\n\nSum across all neighbor pairs and normalize\n\nResult:\n\nLots of positive products → High Moran’s I (clustering)\nProducts near zero → Low Moran’s I (random)\nNegative products → Negative Moran’s I (rare with errors)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#the-intuition-behind-morans-i-1",
    "href": "weekly-notes/week-07/week-07-class-lab.html#the-intuition-behind-morans-i-1",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "The Intuition Behind Moran’s I",
    "text": "The Intuition Behind Moran’s I\nThe formula is really just asking:\n\n“When I’m above/below average, are my neighbors also above/below average?”\n\nBreaking it down:\n\n\\((x_i - \\bar{x})\\) = How far am I from the mean?\n\\((x_j - \\bar{x})\\) = How far is my neighbor from the mean?\nMultiply them:\n\nIf both positive or both negative → positive product (similar)\nIf opposite signs → negative product (dissimilar)\n\nSum across all neighbor pairs and normalize\n\nResult:\n\nLots of positive products → High Moran’s I (clustering)\nProducts near zero → Low Moran’s I (random)\nNegative products → Negative Moran’s I (rare with errors)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#defining-neighbors",
    "href": "weekly-notes/week-07/week-07-class-lab.html#defining-neighbors",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Defining “Neighbors”",
    "text": "Defining “Neighbors”\nDifferent ways to define spatial relationships:\n\n\nContiguity\n\nPolygons that share a border\nQueen vs. Rook\n\n\nDistance\n\nAll within X meters\nFixed threshold\n\n\nk-Nearest\n\nClosest k points\nAdaptive distance\n\n\n\nFor point data (houses), use k-nearest neighbors\n\n\nCode\n# Create 5-nearest neighbors\ncoords &lt;- st_coordinates(boston_test)\nnb &lt;- knn2nb(knearneigh(coords, k=5))\nweights &lt;- nb2listw(nb, style=\"W\")"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#calculating-spatial-lag",
    "href": "weekly-notes/week-07/week-07-class-lab.html#calculating-spatial-lag",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Calculating Spatial Lag",
    "text": "Calculating Spatial Lag\nSpatial lag = average value of neighbors\n\n\n\n\n\n\nTipExample: 5 houses\n\n\n\n\n\n\nHouse\nSale Price\n2 Nearest\nSpatial Lag\n\n\n\n\nA\n$200k\nB, C\n$275k\n\n\nB\n$250k\nA, C\n$250k\n\n\nC\n$300k\nB, D\n$275k\n\n\nD\n$350k\nC, E\n$350k\n\n\nE\n$400k\nD\n$350k\n\n\n\n\n\nIn R:\n\n\nCode\nboston$price_lag &lt;- lag.listw(weights, boston$SalePrice)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#computing-morans-i",
    "href": "weekly-notes/week-07/week-07-class-lab.html#computing-morans-i",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Computing Moran’s I",
    "text": "Computing Moran’s I\nCalculate Moran’s I for your errors:\n\n\nCode\n# Test for spatial autocorrelation in errors\nmoran_test &lt;- moran.mc(\n  boston_test$error,        # Your errors\n  weights,                  # Spatial weights matrix\n  nsim = 999                # Number of permutations\n)\n\n# View results\nmoran_test$statistic         # Moran's I value\n\n\nstatistic \n0.7186593 \n\n\nCode\nmoran_test$p.value          # Is it significant?\n\n\n[1] 0.001\n\n\nInterpretation:\n\nI &gt; 0 and p &lt; 0.05 → Significant clustering\nI ≈ 0 → Random pattern (good!)\nI &lt; 0 → Dispersion (rare with errors)"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#visualizing-significance",
    "href": "weekly-notes/week-07/week-07-class-lab.html#visualizing-significance",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Visualizing Significance",
    "text": "Visualizing Significance\nCompare observed I to random permutations:"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#what-morans-i-tells-you",
    "href": "weekly-notes/week-07/week-07-class-lab.html#what-morans-i-tells-you",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "What Moran’s I Tells You",
    "text": "What Moran’s I Tells You\n\n\n\n\n\n\nImportantDecision Framework\n\n\n\nIf Moran’s I is high (errors clustered):\n\nAdd more spatial features\n\nTry different buffer sizes\nInclude more amenities/disamenities\nCreate neighborhood-specific variables\n\nTry spatial fixed effects\n\nNeighborhood dummies\nGrid cell dummies\n\nConsider spatial regression models\n\nSpatial lag model\nSpatial error model\n(Advanced topic, not covered today)\n\n\n\n\nIf Moran’s I ≈ 0 (random errors):\n✅ Your model adequately captures spatial relationships!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#what-about-spatial-lagerror-models",
    "href": "weekly-notes/week-07/week-07-class-lab.html#what-about-spatial-lagerror-models",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "“What About Spatial Lag/Error Models?”",
    "text": "“What About Spatial Lag/Error Models?”\n“In my spatial statistics class, I learned about spatial lag and spatial error models for dealing with spatial autocorrelation. Why aren’t we using those here?”\n\n\n\nSpatial Econometrics Models\n(Spatial Statistics Class)\nSpatial Lag Model: \\(Y_i = \\rho WY + \\beta X_i + \\varepsilon\\)\nSpatial Error Model: \\(Y_i = \\beta X_i + \\lambda W\\varepsilon + \\xi\\)\nPurpose:\n\nCausal inference with spatial spillovers\nUnderstanding neighbor effects\nCorrect standard errors for hypothesis testing\nCross-sectional analysis\n\nWhen to use: Academic research on spillover effects, peer influence, regional economics\n\n\n\nPredictive Spatial Features\n(This Class)\nOur Approach: \\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2(\\text{crimes}_{500ft}) + \\beta_3(\\text{dist}_{downtown}) + \\varepsilon_i\\]\nPurpose:\n\nOut-of-sample prediction\nForecasting new observations\n\nApplied machine learning\nGeneralization to new areas\n\nWhen to use: Real estate prediction, housing market forecasting, policy planning"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#why-not-spatial-lag-models-for-prediction",
    "href": "weekly-notes/week-07/week-07-class-lab.html#why-not-spatial-lag-models-for-prediction",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Why Not Spatial Lag Models for Prediction?",
    "text": "Why Not Spatial Lag Models for Prediction?\n\n\n\nThe Problems with Spatial Lag for Prediction:\n1. Simultaneity Problem\n\nIncluding \\(WY\\) (neighbor prices) creates circular logic\nMy price affects neighbors → neighbors affect me\nOLS estimates are biased and inconsistent\n\n2. Prediction Paradox\n\nNeed neighbors’ prices to predict my price\nBut for new developments or future periods, those prices don’t exist yet\nCan’t generalize to truly new areas\n\n3. Data Leakage in CV\n\nGeographic CV holds out spatial regions\nSpatial lag “leaks” information from test set\nArtificially good performance that won’t hold\n\n\n\n\nOur Solution: Spatial Features of X (Not Y)\nInstead of modeling dependence in Y (prices), model proximity in X (predictors)\n\n\n\n❌ Spatial Lag\n✅ Our Approach\n\n\n\n\n“Near expensive houses”\n“Near low crime areas”\n\n\nUses neighbor prices\nUses neighbor characteristics\n\n\nCircular logic\nCausal mechanism\n\n\nCan’t predict new areas\nGeneralizes well\n\n\n\nIf Moran’s I shows clustered errors:\n✅ Add more spatial features (different buffers, more amenities)\n✅ Try neighborhood fixed effects\n✅ Use spatial cross-validation\n❌ Don’t add spatial lag of Y for prediction purposes\n\n\n\n\n\n\nTipThe Bottom Line\n\n\n\nBoth approaches are valid for different goals! Match method to purpose: inference → spatial lag/error models; prediction → spatial features."
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#quick-clarification-biased-vs.-inconsistent",
    "href": "weekly-notes/week-07/week-07-class-lab.html#quick-clarification-biased-vs.-inconsistent",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Quick Clarification: Biased vs. Inconsistent",
    "text": "Quick Clarification: Biased vs. Inconsistent\nWhen we say OLS estimates are “biased and inconsistent” with spatial lag models, what does that mean?\n\n\n\nBiased Estimator\nDefinition: Expected value ≠ true parameter\n\\[E[\\hat{\\beta}] \\neq \\beta\\]\nWhat this means:\n\nOn average, across all possible samples, your estimate is systematically wrong\nDoesn’t get the right answer even in expectation\nMore data doesn’t fix it\n\nExample:\n\nTrue effect: β = 100\nYour estimates average to: 80\nYou’re systematically 20 units off\n\n\n\n\nInconsistent Estimator\nDefinition: Doesn’t converge to true value as n → ∞\n\\[\\hat{\\beta} \\not\\to \\beta \\text{ as } n \\to \\infty\\]\nWhat this means:\n\nEven with infinite data, you won’t get the right answer\nThe problem doesn’t go away with bigger samples\nViolates a fundamental property of good estimators\n\nExample: - n = 100 → estimate = 80 - n = 10,000 → estimate = 82 - n = 1,000,000 → estimate = 84 - Never reaches true value of 100"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#key-takeaways",
    "href": "weekly-notes/week-07/week-07-class-lab.html#key-takeaways",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nSpatial autocorrelation in errors indicates model misspecification\nMoran’s I is a diagnostic tool:\n\nGlobal I: overall clustering\nMaps of residuals give clues to what you might be missing\n\nIterative improvement:\n\nDiagnose → Engineer features → Re-test → Repeat\nDocument what you try!"
  },
  {
    "objectID": "weekly-notes/week-07/week-07-class-lab.html#resources",
    "href": "weekly-notes/week-07/week-07-class-lab.html#resources",
    "title": "MODEL DIAGNOSTICS & SPATIAL AUTOCORRELATION",
    "section": "Resources",
    "text": "Resources\nSpatial autocorrelation: - https://mgimond.github.io/Spatial/spatial-autocorrelation.html\nspdep package: - https://r-spatial.github.io/spdep/"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html",
    "href": "weekly-notes/week-09/week-09-class-lab.html",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#about-this-exercise",
    "href": "weekly-notes/week-09/week-09-class-lab.html#about-this-exercise",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "",
    "text": "In this exercise, you will build a spatial predictive model for burglaries using count regression and spatial features."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.1-load-chicago-spatial-data",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.1-load-chicago-spatial-data",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 1.1: Load Chicago Spatial Data",
    "text": "Exercise 1.1: Load Chicago Spatial Data\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"✓ Loaded spatial boundaries\\n\")\n\n\n✓ Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277 \n\n\n\n\n\n\n\n\nNoteCoordinate Reference System\n\n\n\nWe’re using ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is appropriate for Chicago because:\n\nIt minimizes distortion in this region\nUses feet (common in US planning)\nAllows accurate distance calculations"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.2-load-burglary-data",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.2-load-burglary-data",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 1.2: Load Burglary Data",
    "text": "Exercise 1.2: Load Burglary Data\n\n\nCode\n# Load from provided data file (downloaded from Chicago open data portal)\nburglaries &lt;- st_read(\"data/burglaries.shp\") %&gt;% \n  st_transform('ESRI:102271')\n\n\nReading layer `burglaries' from data source \n  `C:\\Users\\Tess\\Documents\\GitHub\\portfolio-setup-TessaVu\\weekly-notes\\week-09\\data\\burglaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7482 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 340492 ymin: 552959.6 xmax: 367153.5 ymax: 594815.1\nProjected CRS: NAD83(HARN) / Illinois East\n\n\nCode\n# Check the data\ncat(\"\\n✓ Loaded burglary data\\n\")\n\n\n\n✓ Loaded burglary data\n\n\nCode\ncat(\"  - Number of burglaries:\", nrow(burglaries), \"\\n\")\n\n\n  - Number of burglaries: 7482 \n\n\nCode\ncat(\"  - CRS:\", st_crs(burglaries)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nCode\ncat(\"  - Date range:\", min(burglaries$date, na.rm = TRUE), \"to\", \n    max(burglaries$date, na.rm = TRUE), \"\\n\")\n\n\n  - Date range: Inf to -Inf \n\n\nQuestion 1.1: How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis?\nYour answer here:\n\n\n\n\n\n\nWarningCritical Pause #1: Data Provenance\n\n\n\nBefore proceeding, consider where this data came from:\nWho recorded this data? Chicago Police Department officers and detectives\nWhat might be missing?\n\nUnreported burglaries (victims didn’t call police)\nIncidents police chose not to record\nDowngraded offenses (burglary recorded as trespassing)\nSpatial bias (more patrol = more recorded crime)\n\nThink About Was there a Department of Justice investigation of CPD during this period? What did they find about data practices?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.3-visualize-point-data",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-1.3-visualize-point-data",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 1.3: Visualize Point Data",
    "text": "Exercise 1.3: Visualize Point Data\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nQuestion 1.2: What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-2.1-understanding-the-fishnet",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-2.1-understanding-the-fishnet",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 2.1: Understanding the Fishnet",
    "text": "Exercise 2.1: Understanding the Fishnet\nA fishnet grid converts irregular point data into a regular grid of cells where we can:\n\nAggregate counts\nCalculate spatial features\nApply regression models\n\nThink of it as overlaying graph paper on a map.\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 meters per cell\n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters\n\n\nQuestion 2.1: Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-2.2-aggregate-burglaries-to-grid",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-2.2-aggregate-burglaries-to-grid",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 2.2: Aggregate Burglaries to Grid",
    "text": "Exercise 2.2: Aggregate Burglaries to Grid\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglaries_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Visualize aggregated counts\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",  # Square root for better visualization of skewed data\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\nQuestion 2.2: What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.1-load-311-abandoned-vehicle-calls",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 4.1: Load 311 Abandoned Vehicle Calls",
    "text": "Exercise 4.1: Load 311 Abandoned Vehicle Calls\n\n\nCode\nabandoned_cars &lt;- read_csv(\"data/abandoned_cars_2017.csv\")%&gt;%\n  filter(!is.na(Latitude), !is.na(Longitude)) %&gt;%\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\"), crs = 4326) %&gt;%\n  st_transform('ESRI:102271')\n\ncat(\"✓ Loaded abandoned vehicle calls\\n\")\n\n\n✓ Loaded abandoned vehicle calls\n\n\nCode\ncat(\"  - Number of calls:\", nrow(abandoned_cars), \"\\n\")\n\n\n  - Number of calls: 31390 \n\n\n\n\n\n\n\n\nNoteData Loading Note\n\n\n\nThe data was downloaded from Chicago’s Open Data Portal. You can now request an api from the Chicago portal and tap into the data there.\nConsider: How might the 311 reporting system itself be biased? Who calls 311? What neighborhoods have better 311 awareness?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.2-count-of-abandoned-cars-per-cell",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 4.2: Count of Abandoned Cars per Cell",
    "text": "Exercise 4.2: Count of Abandoned Cars per Cell\n\n\nCode\n# Aggregate abandoned car calls to fishnet\nabandoned_fishnet &lt;- st_join(abandoned_cars, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(abandoned_cars = n())\n\n# Join to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(abandoned_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(abandoned_cars = replace_na(abandoned_cars, 0))\n\ncat(\"Abandoned car distribution:\\n\")\n\n\nAbandoned car distribution:\n\n\nCode\nsummary(fishnet$abandoned_cars)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    2.00    9.00   12.74   19.00  123.00 \n\n\n\n\nCode\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abandoned_cars), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"magma\") +\n  labs(title = \"Abandoned Vehicle 311 Calls\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\") +\n  labs(title = \"Burglaries\") +\n  theme_crime()\n\np1 + p2 +\n  plot_annotation(title = \"Are abandoned cars and burglaries correlated?\")\n\n\n\n\n\n\n\n\n\nQuestion 4.1: Do you see a visual relationship between abandoned cars and burglaries? What does this suggest?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.3-nearest-neighbor-features",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.3-nearest-neighbor-features",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 4.3: Nearest Neighbor Features",
    "text": "Exercise 4.3: Nearest Neighbor Features\nCount in a cell is one measure. Distance to the nearest 3 abandoned cars captures local context.\n\n\nCode\n# Calculate mean distance to 3 nearest abandoned cars\n# (Do this OUTSIDE of mutate to avoid sf conflicts)\n\n# Get coordinates\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\nabandoned_coords &lt;- st_coordinates(abandoned_cars)\n\n# Calculate k nearest neighbors and distances\nnn_result &lt;- get.knnx(abandoned_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    abandoned_cars.nn = rowMeans(nn_result$nn.dist)\n  )\n\ncat(\"✓ Calculated nearest neighbor distances\\n\")\n\n\n✓ Calculated nearest neighbor distances\n\n\nCode\nsummary(fishnet$abandoned_cars.nn)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   4.386   88.247  143.293  246.946  271.283 2195.753 \n\n\nQuestion 4.2: What does a low value of abandoned_cars.nn mean? A high value? Why might this be informative?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.4-distance-to-hot-spots",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-4.4-distance-to-hot-spots",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 4.4: Distance to Hot Spots",
    "text": "Exercise 4.4: Distance to Hot Spots\nLet’s identify clusters of abandoned cars using Local Moran’s I, then calculate distance to these hot spots.\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n# Apply to abandoned cars\nfishnet &lt;- calculate_local_morans(fishnet, \"abandoned_cars\", k = 5)\n\n\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Abandoned Car Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_crime()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  \n  cat(\"✓ Calculated distance to abandoned car hot spots\\n\")\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No significant hot spots found\\n\")\n}\n\n\n✓ Calculated distance to abandoned car hot spots\n  - Number of hot spot cells: 275 \n\n\nQuestion 4.3: Why might distance to a cluster of abandoned cars be more informative than distance to a single abandoned car? What does Local Moran’s I tell us?\nYour answer here:\n\n\n\n\n\n\nNote\n\n\n\nLocal Moran’s I identifies:\n\nHigh-High: Hot spots (high values surrounded by high values)\nLow-Low: Cold spots (low values surrounded by low values)\nHigh-Low / Low-High: Spatial outliers\n\nThis helps us understand spatial clustering patterns."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.1-poisson-regression",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.1-poisson-regression",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 6.1: Poisson Regression",
    "text": "Exercise 6.1: Poisson Regression\nBurglary counts are count data (0, 1, 2, 3…). We’ll use Poisson regression.\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    abandoned_cars,\n    abandoned_cars.nn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\ncat(\"✓ Prepared modeling data\\n\")\n\n\n✓ Prepared modeling data\n\n\nCode\ncat(\"  - Observations:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Observations: 1708 \n\n\nCode\ncat(\"  - Variables:\", ncol(fishnet_model), \"\\n\")\n\n\n  - Variables: 6 \n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        1.976262369  0.042512701  46.486 &lt;0.0000000000000002 ***\nabandoned_cars    -0.001360741  0.001089805  -1.249               0.212    \nabandoned_cars.nn -0.004965200  0.000198914 -24.962 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000002874  0.000006206   0.463               0.643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 5070.6  on 1704  degrees of freedom\nAIC: 9138.9\n\nNumber of Fisher Scoring iterations: 6\n\n\nQuestion 6.1: Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.2-check-for-overdispersion",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.2-check-for-overdispersion",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 6.2: Check for Overdispersion",
    "text": "Exercise 6.2: Check for Overdispersion\nPoisson regression assumes mean = variance. Real count data often violates this (overdispersion).\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 3.38 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"⚠ Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"✓ Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n⚠ Overdispersion detected! Consider Negative Binomial model."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.3-negative-binomial-regression",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-6.3-negative-binomial-regression",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 6.3: Negative Binomial Regression",
    "text": "Exercise 6.3: Negative Binomial Regression\nIf overdispersed, use Negative Binomial regression (more flexible).\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.603099596, \n    link = log)\n\nCoefficients:\n                      Estimate   Std. Error z value            Pr(&gt;|z|)    \n(Intercept)        2.092907737  0.077469423  27.016 &lt;0.0000000000000002 ***\nabandoned_cars    -0.002006352  0.002091851  -0.959               0.337    \nabandoned_cars.nn -0.005844829  0.000321389 -18.186 &lt;0.0000000000000002 ***\ndist_to_hotspot    0.000006861  0.000011049   0.621               0.535    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.6031) family taken to be 1)\n\n    Null deviance: 2534.0  on 1707  degrees of freedom\nResidual deviance: 1796.6  on 1704  degrees of freedom\nAIC: 7522.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.6031 \n          Std. Err.:  0.0888 \n\n 2 x log-likelihood:  -7512.5850 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 9138.9 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7522.6 \n\n\nQuestion 6.2: Which model fits better (lower AIC)? What does this tell you about the data?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-8.1-generate-final-predictions",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-8.1-generate-final-predictions",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 8.1: Generate Final Predictions",
    "text": "Exercise 8.1: Generate Final Predictions\n\n\nCode\n# Fit final model on all data\nfinal_model &lt;- glm.nb(\n  countBurglaries ~ abandoned_cars + abandoned_cars.nn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Add predictions back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, fishnet_model, type = \"response\")[match(uniqueID, fishnet_model$uniqueID)]\n  )\n\n# Also add KDE predictions (normalize to same scale as counts)\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBurglaries, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-8.2-compare-model-vs.-kde-baseline",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 8.2: Compare Model vs. KDE Baseline",
    "text": "Exercise 8.2: Compare Model vs. KDE Baseline\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Actual Burglaries\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_crime()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 15)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_crime()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Burglaries\",\n    subtitle = \"Does our complex model outperform simple KDE?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBurglaries - prediction_nb)),\n    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),\n    kde_mae = mean(abs(countBurglaries - prediction_kde)),\n    kde_rmse = sqrt(mean((countBurglaries - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\napproach\nmae\nrmse\n\n\n\n\nmodel\n2.48\n3.59\n\n\nkde\n2.06\n2.95\n\n\n\n\n\nQuestion 8.1: Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-9.3-where-does-the-model-work-well",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-9.3-where-does-the-model-work-well",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 9.3: Where Does the Model Work Well?",
    "text": "Exercise 9.3: Where Does the Model Work Well?\n\n\nCode\n# Calculate errors\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBurglaries - prediction_nb,\n    error_kde = countBurglaries - prediction_kde,\n    abs_error_nb = abs(error_nb),\n    abs_error_kde = abs(error_kde)\n  )\n\n# Map errors\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-10, 10)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\") +\n  theme_crime()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_crime()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nQuestion 9.2: Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?\nYour answer here:"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-10.1-model-summary-table",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-10.1-model-summary-table",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 10.1: Model Summary Table",
    "text": "Exercise 10.1: Model Summary Table\n\n\nCode\n# Create nice summary table\nmodel_summary &lt;- broom::tidy(final_model, exponentiate = TRUE) %&gt;%\n  mutate(\n    across(where(is.numeric), ~round(., 3))\n  )\n\nmodel_summary %&gt;%\n  kable(\n    caption = \"Final Negative Binomial Model Coefficients (Exponentiated)\",\n    col.names = c(\"Variable\", \"Rate Ratio\", \"Std. Error\", \"Z\", \"P-Value\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = \"Rate ratios &gt; 1 indicate positive association with burglary counts.\"\n  )\n\n\n\nFinal Negative Binomial Model Coefficients (Exponentiated)\n\n\nVariable\nRate Ratio\nStd. Error\nZ\nP-Value\n\n\n\n\n(Intercept)\n8.108\n0.077\n27.016\n0.000\n\n\nabandoned_cars\n0.998\n0.002\n-0.959\n0.337\n\n\nabandoned_cars.nn\n0.994\n0.000\n-18.186\n0.000\n\n\ndist_to_hotspot\n1.000\n0.000\n0.621\n0.535\n\n\n\nNote: \n\n\n\n\n\n\n Rate ratios &gt; 1 indicate positive association with burglary counts."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-class-lab.html#exercise-10.2-key-findings-summary",
    "href": "weekly-notes/week-09/week-09-class-lab.html#exercise-10.2-key-findings-summary",
    "title": "PREDICTIVE POLICING - TECHNICAL IMPLEMENTATION",
    "section": "Exercise 10.2: Key Findings Summary",
    "text": "Exercise 10.2: Key Findings Summary\nBased on your analysis, complete this summary:\nTechnical Performance:\n\nCross-validation MAE: 2.7\nModel vs. KDE: [Which performed better?]\nMost predictive variable: [Which had largest effect?]\n\nSpatial Patterns:\n\nBurglaries are [evenly distributed / clustered]\nHot spots are located in [describe]\nModel errors show [random / systematic] patterns\n\nModel Limitations:\n\nOverdispersion: [Yes/No]\nSpatial autocorrelation in residuals: [Test this!]\nCells with zero counts: [What % of data?]"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html",
    "href": "weekly-notes/week-09/week-09-slides.html",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "",
    "text": "Before we discuss HOW to build predictive policing systems, we need to ask: SHOULD we?\n“A statistically ‘good’ model can still be socially harmful.”\n\nRichardson, Schultz & Crawford (2019)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#opening-thought",
    "href": "weekly-notes/week-09/week-09-slides.html#opening-thought",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "",
    "text": "Before we discuss HOW to build predictive policing systems, we need to ask: SHOULD we?\n“A statistically ‘good’ model can still be socially harmful.”\n\nRichardson, Schultz & Crawford (2019)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-sales-pitch",
    "href": "weekly-notes/week-09/week-09-slides.html#the-sales-pitch",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Sales Pitch",
    "text": "The Sales Pitch\nWhat vendors and police departments claim:\n\nEfficiency: “Deploy limited resources where they’re needed most”\nObjectivity: “Remove human bias from decision-making”\n\nProactivity: “Prevent crime before it happens”\nData-driven: “Let the data tell us where crime will occur”\n\nSounds great, right?\n. . .\nBut these claims rest on critical assumptions:\n\nThat crime data accurately reflects crime (it doesn’t)\nThat past patterns predict future crime (they might just predict policing)\nThat we can separate “good” from “bad” data (we often can’t)\nThat technical solutions can fix social problems (they can’t)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-technical-evolution",
    "href": "weekly-notes/week-09/week-09-slides.html#the-technical-evolution",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Technical Evolution",
    "text": "The Technical Evolution\nGeneration | Method | Data Used | Example |\n||–|–|| | 1st: Hotspots | Kernel Density | Past crime locations | KDE maps | | 2nd: Risk Terrain | Logistic Reg. | Crime + features | RTM software | | 3rd: ML | Random Forest, Neural Nets | Everything | PredPol, Palantir | | 4th: Person-Based | Network analysis | Social connections | Strategic Subject List |\nEach generation claims to be more “objective” and “accurate”\nBut what if they’re all built on the same flawed foundation?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#what-is-dirty-data",
    "href": "weekly-notes/week-09/week-09-slides.html#what-is-dirty-data",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "What Is “Dirty Data”?",
    "text": "What Is “Dirty Data”?\nTraditional definition (data mining):\n\nMissing data\nIncorrect data\n\nNon-standardized formats\n\nExtended definition (Richardson et al. 2019):\n\n“Data derived from or influenced by corrupt, biased, and unlawful practices, including data that has been intentionally manipulated or ‘juked,’ as well as data that is distorted by individual and societal biases.”"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-many-forms-of-dirty-data",
    "href": "weekly-notes/week-09/week-09-slides.html#the-many-forms-of-dirty-data",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Many Forms of Dirty Data",
    "text": "The Many Forms of Dirty Data\n1. Fabricated/Manipulated Data\n\nFalse arrests (planted evidence)\nDowngraded crime classifications to “juke the stats”\nPressuring victims not to file reports\n\n2. Systematically Biased Data\n\nOver-policing of certain communities → more recorded “crime”\nUnder-policing of white-collar crime → appears less common\nRacial profiling → disproportionate stops/arrests\n\n3. Missing/Incomplete Data\n\nUnreported crimes (especially in over-policed areas with low police trust)\nIgnored complaints\nIncomplete records\n\n4. Proxy Problems\n\nArrests ≠ crimes committed\nCalls for service ≠ actual need\nGang database ≠ actual gang membership"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#case-study-1-juking-the-stats---the-wire-and-baltimore",
    "href": "weekly-notes/week-09/week-09-slides.html#case-study-1-juking-the-stats---the-wire-and-baltimore",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Case Study 1: “Juking the Stats” - The Wire and Baltimore",
    "text": "Case Study 1: “Juking the Stats” - The Wire and Baltimore\nFrom TV to Reality:\nThe Wire (2004): “If the crime rate doesn’t fall, you most certainly will”\nBaltimore Reality (2008-2018):\n\n14,000+ serious assaults misrecorded as minor offenses\nExtensive Gun Trace Task Force corruption\nOfficers robbing residents, planting evidence\nFalse arrests, fabricated reports\nData manipulation to show “success”\n\nResult: 55+ potential lawsuits, thousands of convictions questioned\nQuestion: What happens when this data trains a predictive algorithm?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#case-study-2-compstat-and-nypd",
    "href": "weekly-notes/week-09/week-09-slides.html#case-study-2-compstat-and-nypd",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Case Study 2: CompStat and NYPD",
    "text": "Case Study 2: CompStat and NYPD\nThe Promise: Data-driven accountability, crime reduction\nThe Reality Revealed:\n\n100+ retired NYPD captains surveyed: Intense pressure led to stat manipulation\nSerious crimes downgraded to meet targets\nOfficers planting drugs to meet arrest quotas\nCommanders persuading victims not to file reports\n\nThe Dual Strategy:\n\nDowngrade serious crimes (reported to FBI) → claim success\nIncrease minor arrests (stops, summonses) → show “control”\n\n2013: Independent audit confirmed systematic data problems"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-feedback-loop-diagram",
    "href": "weekly-notes/week-09/week-09-slides.html#the-feedback-loop-diagram",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Feedback Loop Diagram",
    "text": "The Feedback Loop Diagram\nThe Confirmation Bias Loop:\n\nAlgorithm learns: “Crime happens in neighborhood X”\nPolice sent to neighborhood X\nMore arrests in neighborhood X (regardless of actual crime)\n\nAlgorithm “confirmed”: “We were right about neighborhood X!”\nCycle intensifies"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#vendor-claims-about-bias-mitigation",
    "href": "weekly-notes/week-09/week-09-slides.html#vendor-claims-about-bias-mitigation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Vendor Claims About Bias Mitigation",
    "text": "Vendor Claims About Bias Mitigation\nPredPol claims:\n\n“Uses ONLY 3 data points—crime type, crime location, and crime date/time”\n\nHunchLab claims:\n\n“We would not use data that relates people to predict places—no arrests, no social media, no gang status”\n\nBoth exclude: Arrest data, stop data, traffic stops\nBoth include: Crime reports, calls for service"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-cleaning-the-data-isnt-enough",
    "href": "weekly-notes/week-09/week-09-slides.html#why-cleaning-the-data-isnt-enough",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why “Cleaning” The Data Isn’t Enough",
    "text": "Why “Cleaning” The Data Isn’t Enough\nProblem 1: Crime reports reflect police decisions\n\nOfficer decides what to investigate\nOfficer decides what to classify as “crime”\n\nOfficer decides what to document\n\nProblem 2: Calls for service reflect community bias\n\nNeighbors calling police on Black people barbecuing\n“Suspicious activity” = person of color in “wrong” neighborhood\nGentrification → increased 311 calls on existing residents\n\nProblem 3: What counts as “clean” data?\n\nIf drug arrests are racially biased, exclude them ✓\nBut isn’t burglary enforcement also biased? What about assault?\nWhere do you draw the line?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-impossibility-of-neutral-crime-data",
    "href": "weekly-notes/week-09/week-09-slides.html#the-impossibility-of-neutral-crime-data",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Impossibility of Neutral Crime Data",
    "text": "The Impossibility of Neutral Crime Data\nCrime data is ALWAYS:\n\nSocially constructed - Societies define what counts as “crime”\nSelectively enforced - More resources to some neighborhoods\nOrganizationally filtered - Police priorities, department culture\nPolitically shaped - “Tough on crime” eras, moral panics\nTechnically mediated - 911 systems, CAD software, databases\n\nThere is no “view from nowhere”\nCrime data doesn’t reveal “crime” - it reveals policing patterns"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#comparison-what-gets-policed-vs.-what-gets-committed",
    "href": "weekly-notes/week-09/week-09-slides.html#comparison-what-gets-policed-vs.-what-gets-committed",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Comparison: What Gets Policed vs. What Gets Committed",
    "text": "Comparison: What Gets Policed vs. What Gets Committed\nType | Frequency | Policing Intensity | In Predictive Models? |\n||–|-|| | Violent Crime | Moderate | High | ✓ Yes | | Property Crime | High | High | ✓ Yes | | Drug Offenses | Very High | Racially Disparate | Some exclude | | White-Collar Crime | Very High | Very Low | ✗ Rarely | | Wage Theft | Exceeds all robbery | Almost None | ✗ Never | | Corporate Fraud | $300B+ annually | Minimal | ✗ Never |\nQuestion: What would predictive policing look like if we predicted white-collar crime with the same intensity?\nSee: White Collar Crime Risk Zones - A satirical map"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-missing-data-problem",
    "href": "weekly-notes/week-09/week-09-slides.html#the-missing-data-problem",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The “Missing” Data Problem",
    "text": "The “Missing” Data Problem\nWhat crime data DOESN’T capture:\n\nLess than half of violent crimes reported to police (DOJ estimates)\nEven fewer property crimes reported\nGroups with less favorable views of police less likely to report\nCrimes in areas with low police presence go unrecorded\n\nExample - LAPD 2005-2012:\n\n14,000 serious assaults misclassified as minor offenses\nError not discovered until 2015\nLAPD already working with PredPol by then\nWas this data used? No evidence either way.\n\nIf you can’t trust the labels, you can’t trust the model."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#who-bears-the-costs",
    "href": "weekly-notes/week-09/week-09-slides.html#who-bears-the-costs",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Who Bears the Costs?",
    "text": "Who Bears the Costs?\n\n\nFalse Positives (Predicted High Risk, No Crime Occurs)\nCost to individuals:\n\nIncreased police presence/harassment\nStops, searches without cause\nTrauma, stress\nMissed opportunities (late to work, etc.)\nNormalized surveillance\n\nCost to communities:\n\nBroken trust with police\nReduced reporting of actual crimes\nEconomic harm (businesses avoid area)\nQuality of life degradation\n\n\nFalse Negatives (Predicted Low Risk, Crime Occurs)\nCost to individuals:\n\nInadequate police response\nVictimization\nProperty loss\nSafety concerns\n\nCost to communities:\n\nUnequal protection\nPerception of being “written off”\nActual under-policing\n\nWho experiences each?\n\nFP: Predominantly Black/Brown communities\nFN: Also often Black/Brown communities (just different ones)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#what-consent-decrees-cover-and-dont-cover",
    "href": "weekly-notes/week-09/week-09-slides.html#what-consent-decrees-cover-and-dont-cover",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "What Consent Decrees Cover (And Don’t Cover)",
    "text": "What Consent Decrees Cover (And Don’t Cover)\nTypical Reforms Required:\n\nTraining on constitutional policing\nEarly intervention systems for problem officers\nRevised use-of-force policies\nCommunity oversight\nData collection improvements\n\nWhat They DON’T Typically Address:\n\nHistorical data generated during unconstitutional periods\nOngoing use of that data in algorithms\nData sharing with other jurisdictions\nVendor accountability for bias mitigation\nPublic transparency about predictive systems"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-chicago-consent-decree-2019",
    "href": "weekly-notes/week-09/week-09-slides.html#the-chicago-consent-decree-2019",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Chicago Consent Decree (2019)",
    "text": "The Chicago Consent Decree (2019)\nWhat it includes:\n\nCPD must collect better data\nCommunity oversight board\nConstitutional policing training\nUse of force reforms\n\nWhat community organizations noted is MISSING:\n\nNo restrictions on use of SSL\nNo prohibition on using data from 2011-2017 (period of documented violations)\nNo requirements for predictive policing oversight\nLimited transparency about algorithms\n\nIn 2019: CPD contracted with UChicago Crime Lab to “update” predictive approaches\nQuestion: Using which data?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#questions-to-ask-about-any-predictive-policing-system",
    "href": "weekly-notes/week-09/week-09-slides.html#questions-to-ask-about-any-predictive-policing-system",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Questions to Ask About Any Predictive Policing System",
    "text": "Questions to Ask About Any Predictive Policing System\n\n1. Data Provenance\n\nWhat time period does training data cover?\nWere there civil rights investigations during that period?\nWere there documented cases of data manipulation?\nWhat evidence exists that data is accurate?\n\n\n\n2. Variable Selection\n\nWhat specific variables are used?\nHow might each variable embed bias?\nWhat’s excluded and why?\nWho made these choices?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#questions-continued",
    "href": "weekly-notes/week-09/week-09-slides.html#questions-continued",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Questions (Continued)",
    "text": "Questions (Continued)\n\n3. Validation\n\nHow is accuracy measured?\nWhat counts as “success”?\nAre error rates reported by neighborhood?\nWho experiences false positives vs. false negatives?\n\n\n\n4. Deployment\n\nHow do predictions translate to action?\nWhat discretion do officers have?\nAre social services actually offered?\nWhat are the measurable outcomes?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#questions-continued-1",
    "href": "weekly-notes/week-09/week-09-slides.html#questions-continued-1",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Questions (Continued)",
    "text": "Questions (Continued)\n\n5. Transparency & Accountability\n\nIs the methodology public?\nCan community members access their own “risk scores”?\nIs there a process to challenge predictions?\nWho monitors for disparate impact?\n\n\n\n6. Alternatives\n\nWhat non-punitive interventions were considered?\nCould these resources address root causes instead?\nWhat would actual community safety investment look like?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-fundamental-question",
    "href": "weekly-notes/week-09/week-09-slides.html#the-fundamental-question",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Fundamental Question",
    "text": "The Fundamental Question\n\n\n\n\n\n\nImportantBefore Asking “Does It Work?”, Ask:\n\n\n\n“Should we be doing this at all?”\nConsider:\n\nA system that accurately predicts where over-policing will occur is “accurate”\nA system that reinforces existing inequalities can be “validated”\nA system that efficiently targets marginalized communities is “effective”\n\nBut:\n\nIs it just?\nIs it equitable?\nDoes it make communities safer?\nDoes it address root causes?\nOr does it just manage inequality more efficiently?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#modeling-workflow",
    "href": "weekly-notes/week-09/week-09-slides.html#modeling-workflow",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Modeling Workflow",
    "text": "Modeling Workflow\n\n\n1 Setup & Data Preparation\n\nLoad burglaries (point data)\nLoad abandoned cars (311 calls)\nCreate fishnet (500m × 500m grid)\nAggregate burglaries to cells\n\n2 Baseline Comparison\n\nKernel Density Estimation (KDE)\nSimple spatial smoothing\nWhat we need to beat!\n\n3 Feature Engineering\nUsing Abandoned Cars as “Disorder Indicator”:\n\nCount in each cell\nk-Nearest Neighbors (mean distance to 3 nearest)\nLISA (Local Moran’s I - identify hot spots)\nDistance to hot spots (significant clusters)\n\n\n4 Count Regression Models\n\nFit Poisson regression\nTest for overdispersion\nFit Negative Binomial (if needed)\nInterpret coefficients\n\n5 Spatial Cross-Validation\n\nLeave-One-Group-Out (LOGO)\nTrain on n-1 districts\nTest on held-out district\nCalculate MAE/RMSE\n\n6 Model Comparison\n\nCompare to KDE baseline\nMap predictions vs. actual\nAnalyze errors spatially"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-core-logic-broken-windows-theory",
    "href": "weekly-notes/week-09/week-09-slides.html#the-core-logic-broken-windows-theory",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Core Logic: “Broken Windows Theory”",
    "text": "The Core Logic: “Broken Windows Theory”\n\n\n\n\n\n\nNoteOur Modeling Approach\n\n\n\nHypothesis: Signs of disorder (abandoned cars) predict property crime (burglaries)\nWhy different spatial measures?\n\nCount → How much disorder is HERE?\nk-NN Distance → How CLOSE are we to disorder?\nHot Spots (LISA) → Where does disorder CLUSTER?\nDistance to Hot Spots → How close to concentrated disorder?\n\nEach captures a different aspect of spatial proximity to our indicator variable.\n\n\n. . .\n\n\n\n\n\n\nWarning\n\n\n\nDoes this actually predict crime? Or does it predict where certain communities call 311?\nWhat assumptions are embedded in using “disorder” as a predictor?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#global-vs.-local-spatial-autocorrelation",
    "href": "weekly-notes/week-09/week-09-slides.html#global-vs.-local-spatial-autocorrelation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Global vs. Local Spatial Autocorrelation",
    "text": "Global vs. Local Spatial Autocorrelation\n\n\n\nGlobal Measures\nMoran’s I (one value for entire study area)\nQuestion answered: “Is there spatial clustering overall?”\nRange:\n\n-1 to +1\n+1: Perfect positive clustering\n0: Random spatial pattern\n-1: Perfect dispersion\n\nLimitation: Doesn’t tell us WHERE clusters are\nExample: “Chicago burglaries show positive spatial autocorrelation (I = 0.65)”\n\n\n\nLocal Measures\nLocal Moran’s I (one value per location)\nQuestion answered: “Where are the clusters? Which specific locations?”\nFor each location:\n\nIs it part of a cluster?\nWhat type of cluster?\nIs it statistically significant?\n\nAdvantage: Maps showing WHERE patterns exist\nExample: “South Side shows significant High-High clustering”"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#spatial-weights-matrix-the-foundation",
    "href": "weekly-notes/week-09/week-09-slides.html#spatial-weights-matrix-the-foundation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Spatial Weights Matrix: The Foundation",
    "text": "Spatial Weights Matrix: The Foundation\nBefore calculating spatial autocorrelation, we need to define “neighbors”\nSpatial weights matrix (W):\n\n\\(w_{ij}\\) = weight between locations \\(i\\) and \\(j\\)\nIf \\(i\\) and \\(j\\) are neighbors: \\(w_{ij} &gt; 0\\)\nIf not neighbors: \\(w_{ij} = 0\\)\n\nCommon approaches:\n\nContiguity: Share a border? (Queen vs. Rook)\nDistance: Within threshold distance?\nK-nearest neighbors: Closest k locations"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#spatial-weights-contiguity",
    "href": "weekly-notes/week-09/week-09-slides.html#spatial-weights-contiguity",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Spatial Weights: Contiguity",
    "text": "Spatial Weights: Contiguity\nQueen Contiguity: Neighbors share edge OR vertex (corner)\n[A][B][C]\n[D][E][F]\n[G][H][I]\nCell E’s Queen neighbors: B, D, F, H (edges) + A, C, G, I (corners) = 8 neighbors\nRook Contiguity: Neighbors share only edges\nCell E’s Rook neighbors: B, D, F, H = 4 neighbors\nOur fishnet grid uses Queen contiguity (most common for regular grids)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#spatial-weights-row-standardization",
    "href": "weekly-notes/week-09/week-09-slides.html#spatial-weights-row-standardization",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Spatial Weights: Row Standardization",
    "text": "Spatial Weights: Row Standardization\nRaw weights: Binary (0 or 1)\nProblem: Corner cells have fewer neighbors than interior cells\nSolution: Row standardization (“W” style)\n\\[w^*_{ij} = \\frac{w_{ij}}{\\sum_j w_{ij}}\\]\nEffect: Each row sums to 1\nExample:\n\nCell with 4 neighbors: each gets weight 0.25\nCell with 8 neighbors: each gets weight 0.125\n\nWhy? Makes cells comparable regardless of number of neighbors"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#global-morans-i-formula",
    "href": "weekly-notes/week-09/week-09-slides.html#global-morans-i-formula",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Global Moran’s I Formula",
    "text": "Global Moran’s I Formula\n\\[I = \\frac{n}{\\sum_i \\sum_j w_{ij}} \\cdot \\frac{\\sum_i \\sum_j w_{ij}(Y_i - \\bar{Y})(Y_j - \\bar{Y})}{\\sum_i (Y_i - \\bar{Y})^2}\\]\nWhere:\n\n\\(n\\) = number of locations\n\\(Y_i\\) = value at location \\(i\\) (e.g., burglary count)\n\\(\\bar{Y}\\) = mean value\n\\(w_{ij}\\) = spatial weight between \\(i\\) and \\(j\\)\n\nInterpretation:\n\nPositive I: Similar values cluster (high near high, low near low)\nNegative I: Dissimilar values adjacent (high near low)\nZero: Random spatial pattern"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#local-morans-i-formula",
    "href": "weekly-notes/week-09/week-09-slides.html#local-morans-i-formula",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Local Moran’s I Formula",
    "text": "Local Moran’s I Formula\n\\[I_i = \\frac{(Y_i - \\bar{Y})}{\\sum_j (Y_j - \\bar{Y})^2 / n} \\sum_j w_{ij}(Y_j - \\bar{Y})\\]\nFor each location \\(i\\):\nNumerator: How different is location \\(i\\) from mean?\nDenominator: Variance of all locations\nWeight: How different are neighbors from mean?\nResult: Local statistic for each location\nCritical: Must test for statistical significance!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-moran-scatterplot",
    "href": "weekly-notes/week-09/week-09-slides.html#the-moran-scatterplot",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Moran Scatterplot",
    "text": "The Moran Scatterplot\nVisualizes relationship between:\n\nX-axis: Standardized value at location \\(i\\)\nY-axis: Spatial lag (weighted average of neighbors)\n\nQuadrant II         |    Quadrant I\n(Low-High)          |    (High-High)\nOutliers            |    HOTSPOTS\n                    |\n--|--\n                    |\nQuadrant III        |    Quadrant IV\n(Low-Low)           |    (High-Low)\nCOLDSPOTS           |    Outliers\nInterpretation:\n\nQuadrant I (High-High): High crime surrounded by high crime → Hotspot\nQuadrant II (Low-High): Low crime in high crime area → Outlier\nQuadrant III (Low-Low): Low crime surrounded by low crime → Coldspot\nQuadrant IV (High-Low): High crime in low crime area → Outlier\n\n\n\n\nHigh-Low Cluster of Halloween Enthusiasm"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#statistical-significance-testing",
    "href": "weekly-notes/week-09/week-09-slides.html#statistical-significance-testing",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Statistical Significance Testing",
    "text": "Statistical Significance Testing\nProblem: Not all clusters are “real” - could be random chance\nSolution: Permutation test\nProcess:\n\nCalculate observed \\(I_i\\) for location \\(i\\)\nRandomly shuffle values across locations (999 times)\nRecalculate \\(I_i\\) for each permutation\nCompare observed vs. distribution of permuted values\nIf observed is extreme → statistically significant (p &lt; 0.05)\n\nResult: Only report clusters that are unlikely to occur by chance"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#four-types-of-significant-clusters",
    "href": "weekly-notes/week-09/week-09-slides.html#four-types-of-significant-clusters",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Four Types of Significant Clusters",
    "text": "Four Types of Significant Clusters\nType | Z-score | Location Value | Neighbor Values | Interpretation |\n|||-|–|-| | High-High | Positive | Above mean | Above mean | Hotspot | | Low-Low | Positive | Below mean | Below mean | Coldspot | | High-Low | Negative | Above mean | Below mean | Outlier (isolated high) | | Low-High | Negative | Below mean | Above mean | Outlier (isolated low) |"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#local-morans-i-in-r-step-by-step",
    "href": "weekly-notes/week-09/week-09-slides.html#local-morans-i-in-r-step-by-step",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Local Moran’s I in R: Step by Step",
    "text": "Local Moran’s I in R: Step by Step\nlibrary(spdep)\n\n# Step 1: Create spatial object\nfishnet_sp &lt;- as_Spatial(fishnet)\n\n# Step 2: Define neighbors (Queen contiguity)\nneighbors &lt;- poly2nb(fishnet_sp, queen = TRUE)\n\n# Step 3: Create spatial weights (row-standardized)\nweights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n\n# Step 4: Calculate Local Moran's I\nlocal_moran &lt;- localmoran(\n  fishnet$abandoned_cars,  # Variable of interest\n  weights,                  # Spatial weights\n  zero.policy = TRUE       # Handle cells with no neighbors\n)\n\n# Step 5: Extract components\nfishnet$local_I &lt;- local_moran[, \"Ii\"]      # Local I statistic\nfishnet$p_value &lt;- local_moran[, \"Pr(z != E(Ii))\"]  # P-value\nfishnet$z_score &lt;- local_moran[, \"Z.Ii\"]    # Z-score"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#identifying-high-high-clusters-hotspots",
    "href": "weekly-notes/week-09/week-09-slides.html#identifying-high-high-clusters-hotspots",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Identifying High-High Clusters (Hotspots)",
    "text": "Identifying High-High Clusters (Hotspots)\n# Standardize the variable for quadrant classification\nfishnet$standardized_value &lt;- scale(fishnet$abandoned_cars)\n\n# Calculate spatial lag (weighted mean of neighbors)\nfishnet$spatial_lag &lt;- lag.listw(weights, fishnet$abandoned_cars)\nfishnet$standardized_lag &lt;- scale(fishnet$spatial_lag)\n\n# Identify High-High clusters\nfishnet$hotspot &lt;- 0  # Default: not a hotspot\n\n# Criteria: \n# 1. Value above mean (standardized &gt; 0)\n# 2. Neighbors above mean (spatial lag &gt; 0)\n# 3. Statistically significant (p &lt; 0.05)\n\nfishnet$hotspot[\n  fishnet$standardized_value &gt; 0 & \n  fishnet$standardized_lag &gt; 0 & \n  fishnet$p_value &lt; 0.05\n] &lt;- 1\n\n# Count hotspots\nsum(fishnet$hotspot)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#critical-interpretation-of-hotspots",
    "href": "weekly-notes/week-09/week-09-slides.html#critical-interpretation-of-hotspots",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Critical Interpretation of Hotspots",
    "text": "Critical Interpretation of Hotspots\nWhat you’ve identified:\nStatistically significant clusters of high abandoned car reports\nWhat this MIGHT mean:\n\nActual concentrations of abandoned cars\nAreas where residents report more (higher civic engagement?)\nAreas with more 311 system awareness\nAreas with more code enforcement presence\nGentrifying areas with new residents calling on existing conditions\n\nCritical question:\nAre these “high abandoned car” areas or “high reporting” areas?\nFor modeling:\nWe’re using this as a proxy for “disorder” but recognize it’s a flawed proxy"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-count-regression-the-problem-with-ols",
    "href": "weekly-notes/week-09/week-09-slides.html#why-count-regression-the-problem-with-ols",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why Count Regression? The Problem with OLS",
    "text": "Why Count Regression? The Problem with OLS\nOur outcome: Burglary counts per grid cell (0, 1, 2, 3, …)\nLinear regression assumes:\n\\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\varepsilon_i\\]\nwhere \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\)\nProblems for counts:\n\nCan predict negative values (impossible for counts)\nAssumes constant variance (counts often have variance ≠ mean)\nAssumes continuous outcome (counts are discrete)\nAssumes normal errors (count data is skewed)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#distribution-of-crime-counts",
    "href": "weekly-notes/week-09/week-09-slides.html#distribution-of-crime-counts",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Distribution of Crime Counts",
    "text": "Distribution of Crime Counts\n\n\nCode\n# Typical pattern for crime data\nggplot(fishnet, aes(x = countBurglaries)) +\n  geom_histogram(binwidth = 1, fill = \"#440154FF\", color = \"white\") +\n  labs(\n    title = \"Distribution of Burglary Counts\",\n    subtitle = \"Most cells have 0-2 burglaries, few have many\",\n    x = \"Burglaries per Cell\",\n    y = \"Number of Cells\"\n  ) +\n  theme_minimal()\n\n\nCharacteristics:\n\nRight-skewed: Long tail to the right\nMany zeros: Most cells have no burglaries\nDiscrete: Only integer values\nVariance &gt; Mean: Overdispersion common"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-poisson-distribution",
    "href": "weekly-notes/week-09/week-09-slides.html#the-poisson-distribution",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Poisson Distribution",
    "text": "The Poisson Distribution\nAppropriate for count data:\n\\[P(Y = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\nWhere:\n\n\\(Y\\) = count outcome (0, 1, 2, …)\n\\(\\lambda\\) = expected count (mean = variance)\n\\(k\\) = observed count\n\nKey property: Mean = Variance = \\(\\lambda\\)\nExamples:\n\nNumber of crimes per week\nNumber of 911 calls per hour\nNumber of accidents per intersection"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#poisson-regression-model",
    "href": "weekly-notes/week-09/week-09-slides.html#poisson-regression-model",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Poisson Regression Model",
    "text": "Poisson Regression Model\nLink \\(\\lambda\\) to predictors via log link:\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_p X_{pi}\\]\nEquivalently:\n\\[\\lambda_i = \\exp(\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + ... + \\beta_p X_{pi})\\]\nWhy log link?\n\nEnsures \\(\\lambda_i &gt; 0\\) (counts can’t be negative)\nLinear relationship on log scale\nMultiplicative effects on count scale"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#interpreting-poisson-coefficients",
    "href": "weekly-notes/week-09/week-09-slides.html#interpreting-poisson-coefficients",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Interpreting Poisson Coefficients",
    "text": "Interpreting Poisson Coefficients\nOn log scale:\n\\(\\beta_1\\) = change in log(expected count) per unit increase in \\(X_1\\)\nOn count scale (exponentiate):\n\\(\\exp(\\beta_1)\\) = multiplicative effect on expected count\nExamples:\n\\(\\beta\\) | \\(\\exp(\\beta)\\) | Interpretation |\n|||-| | 0.14 | 1.15 | 15% increase per unit of X | | -0.22 | 0.80 | 20% decrease per unit of X | | 0.00 | 1.00 | No effect | | 0.69 | 2.00 | Doubling per unit of X |"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#poisson-regression-in-r",
    "href": "weekly-notes/week-09/week-09-slides.html#poisson-regression-in-r",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Poisson Regression in R",
    "text": "Poisson Regression in R\n# Fit Poisson model\nmodel_poisson &lt;- glm(\n  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,\n  data = fishnet,\n  family = poisson(link = \"log\")\n)\n\n# View results\nsummary(model_poisson)\n\n# Exponentiate coefficients for interpretation\nexp(coef(model_poisson))\n\n# Example output:\n#                        exp(coef)\n# (Intercept)            0.234\n# Abandoned_Cars         1.151\n# Abandoned_Cars.nn      0.998\n# abandoned.isSig.dist   0.999\n\n# Interpretation:\n# - Each additional abandoned car → 15.1% increase in expected burglaries\n# - Each meter from nearest abandoned car → 0.2% decrease in expected burglaries"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-overdispersion-problem",
    "href": "weekly-notes/week-09/week-09-slides.html#the-overdispersion-problem",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Overdispersion Problem",
    "text": "The Overdispersion Problem\nPoisson assumption: Variance = Mean\nReality with crime data: Variance &gt; Mean (often MUCH larger)\nWhy overdispersion occurs:\n\nUnobserved heterogeneity: Some areas have unmeasured crime attractors\nContagion effects: One crime leads to others (not independent)\nMeasurement error: Counting issues, data quality\nModel misspecification: Missing important variables\n\nCheck for overdispersion:\n\\[\\text{Dispersion} = \\frac{\\text{Residual Deviance}}{\\text{Degrees of Freedom}}\\]\n\nIf ≈ 1: Poisson is fine\nIf &gt; 1: Overdispersion (common!)\nIf &gt; 2-3: Serious overdispersion → Use Negative Binomial"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#checking-overdispersion-in-r",
    "href": "weekly-notes/week-09/week-09-slides.html#checking-overdispersion-in-r",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Checking Overdispersion in R",
    "text": "Checking Overdispersion in R\n# Fit Poisson model\nmodel_pois &lt;- glm(\n  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,\n  data = fishnet,\n  family = poisson\n)\n\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_pois, type = \"pearson\")^2) / \n               model_pois$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 3), \"\\n\")\n\n# Rule of thumb:\n# &lt; 1.5: Poisson OK\n# 1.5 - 3: Mild overdispersion, NegBin recommended\n# &gt; 3: Serious overdispersion, NegBin essential\n\n# Example output:\n# Dispersion parameter: 4.523\n# Warning: Serious overdispersion detected!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#negative-binomial-regression",
    "href": "weekly-notes/week-09/week-09-slides.html#negative-binomial-regression",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\nRelaxes the variance = mean assumption\nAdds dispersion parameter (\\(\\alpha\\)):\n\\[\\text{Var}(Y_i) = \\mu_i + \\alpha \\mu_i^2\\]\nWhere:\n\n\\(\\mu_i\\) = expected count (same as Poisson)\n\\(\\alpha\\) = dispersion parameter\n\nIf \\(\\alpha = 0\\): Reduces to Poisson\nIf \\(\\alpha &gt; 0\\): Allows extra variance (overdispersion)\nInterpretation: Coefficients interpreted same way as Poisson!"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#negative-binomial-in-r",
    "href": "weekly-notes/week-09/week-09-slides.html#negative-binomial-in-r",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Negative Binomial in R",
    "text": "Negative Binomial in R\nlibrary(MASS)\n\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,\n  data = fishnet\n)\n\n# View results\nsummary(model_nb)\n\n# Compare to Poisson\nAIC(model_pois)  # e.g., 8234.5\nAIC(model_nb)    # e.g., 6721.3\n\n# Lower AIC = better fit\n# If NegBin AIC much lower → use NegBin\n\n# Extract dispersion parameter (theta)\nmodel_nb$theta  # e.g., 2.47\n\n# Interpretation: Significant overdispersion confirmed"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#comparing-poisson-vs.-negative-binomial",
    "href": "weekly-notes/week-09/week-09-slides.html#comparing-poisson-vs.-negative-binomial",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Comparing Poisson vs. Negative Binomial",
    "text": "Comparing Poisson vs. Negative Binomial\nAspect | Poisson | Negative Binomial |\n|–||-| | Variance assumption | Var = Mean | Var = μ + αμ² | | Overdispersion | Cannot handle | Accommodates | | Standard errors | Underestimated if overdispersed | Correctly estimated | | When to use | Count data, no overdispersion | Count data with overdispersion | | Crime data | Rarely appropriate | Usually better |\nFor today’s lab: We’ll fit both, compare, and use the better model"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#model-diagnostics-for-count-models",
    "href": "weekly-notes/week-09/week-09-slides.html#model-diagnostics-for-count-models",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Model Diagnostics for Count Models",
    "text": "Model Diagnostics for Count Models\nUnlike OLS, we don’t use residual plots the same way\nKey diagnostics:\n\nDispersion test (already covered)\nDeviance residuals: Should be roughly normal\nPearson residuals: Check for outliers\nCook’s distance: Influential observations\nPredicted vs. observed: Visual check\n\n# Deviance residuals\nplot(model_nb, which = 1)  # Residuals vs. Fitted\n\n# Identify outliers\noutliers &lt;- which(abs(residuals(model_nb, type = \"deviance\")) &gt; 3)\n\n# Influence\ninfluence &lt;- cooks.distance(model_nb)\ninfluential &lt;- which(influence &gt; 4/length(influence))"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-add-spatial-features",
    "href": "weekly-notes/week-09/week-09-slides.html#why-add-spatial-features",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why Add Spatial Features?",
    "text": "Why Add Spatial Features?\nBasic model:\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 (\\text{Abandoned Cars}_i)\\]\nProblem: Ignores spatial context\nEnhanced model with spatial features:\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 (\\text{Abandoned Cars}_i) + \\beta_2 (\\text{Distance to Nearest}_i) + \\beta_3 (\\text{Distance to Hotspot}_i)\\]\nBenefits:\n\nCaptures spillover effects\nAccounts for spatial dependence\nImproves predictions\nMore realistic model of spatial crime processes"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#distance-to-nearest-feature",
    "href": "weekly-notes/week-09/week-09-slides.html#distance-to-nearest-feature",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Distance to Nearest Feature",
    "text": "Distance to Nearest Feature\nConcept: Proximity matters for risk\nImplementation: k-Nearest Neighbors (k=1)\nFor each grid cell:\n\nFind location of all abandoned cars\nCalculate distance to each\nKeep minimum distance\n\nIn R:\nlibrary(FNN)\n\n# Calculate distance to nearest abandoned car\nnn_dist &lt;- get.knnx(\n  data = st_coordinates(abandoned_cars),      # \"To\" locations\n  query = st_coordinates(st_centroid(fishnet)), # \"From\" locations\n  k = 1                                          # Nearest 1\n)\n\n# Extract distances\nfishnet$abandoned_car_nn &lt;- nn_dist$nn.dist[, 1]"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#distance-to-hotspot",
    "href": "weekly-notes/week-09/week-09-slides.html#distance-to-hotspot",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Distance to Hotspot",
    "text": "Distance to Hotspot\nTwo-step process:\nStep 1: Identify hotspots (Local Moran’s I High-High clusters)\nStep 2: Calculate distance from each cell to nearest hotspot\n# Step 1: Identify hotspots (we did this earlier)\nhotspot_cells &lt;- filter(fishnet, hotspot == 1)\n\n# Step 2: Calculate distances\nhotspot_dist &lt;- get.knnx(\n  data = st_coordinates(st_centroid(hotspot_cells)),\n  query = st_coordinates(st_centroid(fishnet)),\n  k = 1\n)\n\nfishnet$hotspot_nn &lt;- hotspot_dist$nn.dist[, 1]\nWhy useful? Captures multi-scale spatial dependence:\n\nLocal (within cell)\nNeighborhood (distance to nearest)\nRegional (distance to hotspot cluster)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#visualizing-distance-features",
    "href": "weekly-notes/week-09/week-09-slides.html#visualizing-distance-features",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Visualizing Distance Features",
    "text": "Visualizing Distance Features\n# Create comparison maps\np1 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = abandoned_car_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\") +\n  labs(title = \"Distance to Nearest Abandoned Car\") +\n  theme_void()\n\np2 &lt;- ggplot(fishnet) +\n  geom_sf(aes(fill = hotspot_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"magma\") +\n  labs(title = \"Distance to Nearest Hotspot\") +\n  theme_void()\n\ngrid.arrange(p1, p2, ncol = 2)\nLook for: Concentric patterns around features/hotspots"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-use-a-fishnet-grid",
    "href": "weekly-notes/week-09/week-09-slides.html#why-use-a-fishnet-grid",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why Use a Fishnet Grid?",
    "text": "Why Use a Fishnet Grid?\nProblem: Crime is points, but we need areas for modeling\nOptions:\n\nExisting boundaries (census tracts, neighborhoods)\n\nPro: Align with administrative data\nCon: Arbitrary, unequal sizes, Modifiable Areal Unit Problem\n\nFishnet grid (regular cells)\n\nPro: Consistent size, no boundary bias\nCon: Arbitrary, may split “natural” areas\n\n\nWe use fishnet because:\n\nStandard approach in predictive policing\nEasier spatial operations\nConsistent unit of analysis"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#creating-a-fishnet-grid",
    "href": "weekly-notes/week-09/week-09-slides.html#creating-a-fishnet-grid",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Creating a Fishnet Grid",
    "text": "Creating a Fishnet Grid\nlibrary(sf)\n\n# Step 1: Define cell size (in map units - meters for our projection)\ncell_size &lt;- 500  # 500m x 500m cells\n\n# Step 2: Create grid over study area\nfishnet &lt;- st_make_grid(\n  chicago_boundary,\n  cellsize = cell_size,\n  square = TRUE,\n  what = \"polygons\" #you could change to hexagons if you wanted to be fancy.\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n# Step 3: Clip to study area (remove cells outside boundary)\nfishnet &lt;- fishnet[chicago_boundary, ]\n\n# Check results\nnrow(fishnet)  # Number of cells\nst_area(fishnet[1, ])  # Area of one cell (should be 250,000 m²)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#grid-cell-size-a-critical-choice",
    "href": "weekly-notes/week-09/week-09-slides.html#grid-cell-size-a-critical-choice",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Grid Cell Size: A Critical Choice",
    "text": "Grid Cell Size: A Critical Choice\nCommon sizes:\n\n250m × 250m: Fine-grained, many cells, computationally intensive\n500m × 500m: Standard, balance detail and computation\n1000m × 1000m: Coarse, faster, loses local detail\n\nSmaller cells:\n\n✓ More spatial detail\n✓ Better capture local patterns\n✗ More zeros (sparse data)\n✗ Computational cost\n\nLarger cells:\n\n✓ Fewer zeros\n✓ More stable estimates\n✗ Lose local variation\n✗ May obscure hotspots\n\nChoice affects results! No “correct” answer."
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#aggregating-points-to-grid",
    "href": "weekly-notes/week-09/week-09-slides.html#aggregating-points-to-grid",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Aggregating Points to Grid",
    "text": "Aggregating Points to Grid\nProcess:\n\nSpatial join between crimes (points) and fishnet (polygons)\nCount crimes per cell\nHandle cells with zero crimes\n\n# Count burglaries per cell\nburglary_counts &lt;- st_join(burglaries, fishnet) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet %&gt;%\n  left_join(burglary_counts, by = \"uniqueID\") %&gt;%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary\nsummary(fishnet$countBurglaries)\n#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#      0       0       1    2.3       3      47"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#handling-zeros-in-count-data",
    "href": "weekly-notes/week-09/week-09-slides.html#handling-zeros-in-count-data",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Handling Zeros in Count Data",
    "text": "Handling Zeros in Count Data\nCrime data typically has MANY zeros\nExample distribution: - 40% of cells: 0 burglaries - 30% of cells: 1 burglary - 20% of cells: 2-3 burglaries - 10% of cells: 4+ burglaries\nImplications:\n\nPoisson handles zeros naturally (built into distribution)\nZero-inflation: If &gt;60% zeros, consider Zero-Inflated Poisson (ZIP)\nFor today: Standard Negative Binomial handles our zeros fine\n\nCritical interpretation: Are zeros “true zeros” (no crime) or “missing data” (unreported)?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-standard-cross-validation-fails-for-spatial-data",
    "href": "weekly-notes/week-09/week-09-slides.html#why-standard-cross-validation-fails-for-spatial-data",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why Standard Cross-Validation Fails for Spatial Data",
    "text": "Why Standard Cross-Validation Fails for Spatial Data\nStandard k-fold CV:\n\nRandomly split data into k folds\nTrain on k-1 folds, test on 1\nRepeat k times\n\nProblem with spatial data:\n\nNearby observations are correlated\nTraining set includes cells adjacent to test cells\nSpatial leakage: Model learns from neighbors of test set\nOverly optimistic performance estimates\n\nSolution: Spatial cross-validation"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#leave-one-group-out-cross-validation-logo-cv",
    "href": "weekly-notes/week-09/week-09-slides.html#leave-one-group-out-cross-validation-logo-cv",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Leave-One-Group-Out Cross-Validation (LOGO-CV)",
    "text": "Leave-One-Group-Out Cross-Validation (LOGO-CV)\nPrinciple: Hold out entire spatial groups, not individual cells\nProcess:\n\nDivide study area into groups (e.g., police districts)\nHold out all cells in District 1\nTrain model on Districts 2-N\nPredict for District 1\nRepeat for each district\n\nWhy better:\n\nTests generalization to truly new areas\nNo spatial leakage between train/test\nMore realistic deployment scenario\nConservative performance estimates"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#logo-cv-implementation",
    "href": "weekly-notes/week-09/week-09-slides.html#logo-cv-implementation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "LOGO-CV Implementation",
    "text": "LOGO-CV Implementation\n# Get unique districts\ndistricts &lt;- unique(fishnet$District)\n\n# Initialize results\ncv_results &lt;- list()\n\n# Loop through districts\nfor (dist in districts) {\n  # Split data\n  train_data &lt;- fishnet %&gt;% filter(District != dist)\n  test_data &lt;- fishnet %&gt;% filter(District == dist)\n  \n  # Fit model on training data\n  model_cv &lt;- glm.nb(\n    countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,\n    data = train_data\n  )\n  \n  # Predict on test data\n  test_data$prediction &lt;- predict(model_cv, test_data, type = \"response\")\n  \n  # Store results\n  cv_results[[dist]] &lt;- test_data\n}\n\n# Combine all predictions\nall_predictions &lt;- bind_rows(cv_results)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#evaluating-cv-performance",
    "href": "weekly-notes/week-09/week-09-slides.html#evaluating-cv-performance",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Evaluating CV Performance",
    "text": "Evaluating CV Performance\nCommon metrics for count models:\nMean Absolute Error (MAE): \\[MAE = \\frac{1}{n}\\sum_i |y_i - \\hat{y}_i|\\]\nRoot Mean Squared Error (RMSE): \\[RMSE = \\sqrt{\\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2}\\]\nMean Error (Bias): \\[ME = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)\\]\n# Calculate metrics by district\ncv_metrics &lt;- all_predictions %&gt;%\n  group_by(District) %&gt;%\n  summarize(\n    MAE = mean(abs(countBurglaries - prediction)),\n    RMSE = sqrt(mean((countBurglaries - prediction)^2)),\n    ME = mean(countBurglaries - prediction)\n  )"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#interpreting-prediction-errors",
    "href": "weekly-notes/week-09/week-09-slides.html#interpreting-prediction-errors",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Interpreting Prediction Errors",
    "text": "Interpreting Prediction Errors\nWhat do errors tell us?\nMAE = 1.5 burglaries per cell:\n\nOn average, we’re off by 1.5 burglaries\nInterpretable in original units\n\nRMSE = 2.3 burglaries per cell:\n\nPenalizes large errors more\nIf RMSE &gt;&gt; MAE, we have some very bad predictions\n\nME = -0.3 burglaries per cell:\n\nNegative: We tend to under-predict\nPositive: We tend to over-predict\nClose to zero: Unbiased predictions\n\nCritical question: Are errors evenly distributed across neighborhoods?"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#spatial-patterns-in-prediction-errors",
    "href": "weekly-notes/week-09/week-09-slides.html#spatial-patterns-in-prediction-errors",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Spatial Patterns in Prediction Errors",
    "text": "Spatial Patterns in Prediction Errors\n# Map prediction errors\nall_predictions &lt;- all_predictions %&gt;%\n  mutate(\n    error = countBurglaries - prediction,\n    abs_error = abs(error),\n    pct_error = (prediction - countBurglaries) / (countBurglaries + 1) * 100\n  )\n\n# Visualize\nggplot(all_predictions) +\n  geom_sf(aes(fill = error), color = NA) +\n  scale_fill_gradient2(\n    low = \"blue\", mid = \"white\", high = \"red\",\n    midpoint = 0,\n    name = \"Error\"\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Red = Over-prediction, Blue = Under-prediction\") +\n  theme_void()\nLook for: Systematic patterns by neighborhood"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#why-compare-to-kernel-density-estimation",
    "href": "weekly-notes/week-09/week-09-slides.html#why-compare-to-kernel-density-estimation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Why Compare to Kernel Density Estimation?",
    "text": "Why Compare to Kernel Density Estimation?\nKDE is the simplest spatial prediction method:\n\nPlace smooth “bump” over each crime location\nSum all bumps to create risk surface\nNo predictors, no model, just past locations\n\nBenefits of comparison:\n\nBenchmark: Does our complex model beat a simple baseline?\nValue added: What do spatial features contribute?\nPractical: KDE is computationally simpler\nCritical: Maybe spatial patterns alone explain everything?\n\nIf our model doesn’t beat KDE, we’re wasting complexity"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#kernel-density-estimation-explained",
    "href": "weekly-notes/week-09/week-09-slides.html#kernel-density-estimation-explained",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Kernel Density Estimation Explained",
    "text": "Kernel Density Estimation Explained\nConceptually:\nImagine placing a “hill” over each crime location:\n\nPeak at the crime location\nHeight decreases with distance\nControlled by bandwidth parameter\n\nMathematical:\n\\[\\hat{f}(s) = \\frac{1}{n\\tau^2}\\sum_{i=1}^n K\\left(\\frac{d(s, s_i)}{\\tau}\\right)\\]\nWhere:\n\n\\(s\\) = location where we estimate density\n\\(s_i\\) = location of crime \\(i\\)\n\\(\\tau\\) = bandwidth (smoothing parameter)\n\\(K\\) = kernel function (usually Gaussian)\n\\(d(s, s_i)\\) = distance between \\(s\\) and \\(s_i\\)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#bandwidth-selection-a-critical-choice",
    "href": "weekly-notes/week-09/week-09-slides.html#bandwidth-selection-a-critical-choice",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Bandwidth Selection: A Critical Choice",
    "text": "Bandwidth Selection: A Critical Choice\nSmall bandwidth (e.g., 250m):\n\nNarrow bumps\nPreserves local detail\nCan be “noisy” (overfit to specific locations)\n\nLarge bandwidth (e.g., 2000m):\n\nWide bumps\nSmooth, generalized patterns\nMay miss important local hotspots\n\nCommon choices for urban crime:\n\n500m - 1000m (balance local and regional)\n\nOur approach:\n\nUse 1000m (1 km) bandwidth\nStandard in predictive policing literature"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#calculating-kde-in-r",
    "href": "weekly-notes/week-09/week-09-slides.html#calculating-kde-in-r",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Calculating KDE in R",
    "text": "Calculating KDE in R\nlibrary(spatstat)\n\n# Step 1: Convert to point pattern (ppp) object\nburglary_ppp &lt;- as.ppp(\n  X = st_coordinates(burglaries),\n  W = as.owin(st_bbox(chicago_boundary))\n)\n\n# Step 2: Calculate KDE\nkde_surface &lt;- density.ppp(\n  burglary_ppp,\n  sigma = 1000,  # Bandwidth in meters\n  edge = TRUE    # Edge correction\n)\n\n# Step 3: Extract values to fishnet cells\nfishnet$kde_risk &lt;- raster::extract(\n  raster(kde_surface),\n  st_centroid(fishnet)\n)\n\n# Standardize to 0-1 scale for comparison\nfishnet$kde_risk &lt;- (fishnet$kde_risk - min(fishnet$kde_risk, na.rm=T)) / \n                     (max(fishnet$kde_risk, na.rm=T) - min(fishnet$kde_risk, na.rm=T))"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#creating-risk-categories",
    "href": "weekly-notes/week-09/week-09-slides.html#creating-risk-categories",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Creating Risk Categories",
    "text": "Creating Risk Categories\nBoth KDE and our model produce continuous risk scores\nFor evaluation, convert to categories:\n# Create quintiles (5 equal groups)\nfishnet$model_risk_category &lt;- cut(\n  fishnet$prediction,\n  breaks = quantile(fishnet$prediction, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\n\nfishnet$kde_risk_category &lt;- cut(\n  fishnet$kde_risk,\n  breaks = quantile(fishnet$kde_risk, probs = seq(0, 1, 0.2)),\n  labels = c(\"1st (Lowest)\", \"2nd\", \"3rd\", \"4th\", \"5th (Highest)\"),\n  include.lowest = TRUE\n)\nEach category contains 20% of cells"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#testing-on-hold-out-data-2018-burglaries",
    "href": "weekly-notes/week-09/week-09-slides.html#testing-on-hold-out-data-2018-burglaries",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Testing on Hold-Out Data (2018 Burglaries)",
    "text": "Testing on Hold-Out Data (2018 Burglaries)\nCritical: Test on data the model has NEVER seen\nProcess:\n\nTrain model on 2017 data\nCreate risk predictions for all cells\nLoad 2018 burglaries (new data)\nCount how many 2018 burglaries fall in each risk category\nCompare model vs. KDE\n\nQuestion: Which method captures more crimes in high-risk areas?\n# Spatial join: 2018 burglaries to fishnet with risk categories\nresults_2018 &lt;- st_join(fishnet, burglaries_2018) %&gt;%\n  group_by(model_risk_category) %&gt;%\n  summarize(burglaries_2018 = n()) %&gt;%\n  mutate(pct_of_total = 100 * burglaries_2018 / sum(burglaries_2018))"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-hit-rate-evaluation",
    "href": "weekly-notes/week-09/week-09-slides.html#the-hit-rate-evaluation",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Hit Rate Evaluation",
    "text": "The Hit Rate Evaluation\nGoal: Capture maximum crimes with minimum resources\nIdeal result:\nIf we patrol the highest-risk 20% of cells (Category 5):\n\nModel: Captures 55% of 2018 burglaries\nKDE: Captures 48% of 2018 burglaries\n\nInterpretation:\n\nModel is better (55% &gt; 48%)\nBut still imperfect (missing 45% of burglaries)\nSpatial features add value over location alone\n\nRandom patrol: Would capture ~20% (by chance)"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#visualizing-model-vs.-kde-performance",
    "href": "weekly-notes/week-09/week-09-slides.html#visualizing-model-vs.-kde-performance",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Visualizing Model vs. KDE Performance",
    "text": "Visualizing Model vs. KDE Performance"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#critical-interpretation-of-performance",
    "href": "weekly-notes/week-09/week-09-slides.html#critical-interpretation-of-performance",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Critical Interpretation of Performance",
    "text": "Critical Interpretation of Performance\nStatistical Success:\n✓ Model beats KDE\n✓ Better than random\n✓ Statistically significant difference\nBut ask:\n\nWhich neighborhoods are classified as “highest risk”?\nWhat happens when police patrol those areas?\nWho experiences increased surveillance?\nAre we predicting crime or policing patterns?\nDoes “better prediction” = “better public safety”?\n\nA technically superior model can still be ethically problematic"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#the-complete-modeling-workflow",
    "href": "weekly-notes/week-09/week-09-slides.html#the-complete-modeling-workflow",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "The Complete Modeling Workflow",
    "text": "The Complete Modeling Workflow\nStep 1: Data Preparation\n\nCreate fishnet grid\nAggregate crimes to cells\nCalculate spatial features (Local Moran’s I, distances)\n\nStep 2: Exploratory Analysis\n\nVisualize crime patterns\nCheck for spatial autocorrelation\nIdentify hotspots\n\nStep 3: Model Fitting\n\nFit Poisson (check overdispersion)\nFit Negative Binomial (if overdispersed)\nInterpret coefficients"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#workflow-continued",
    "href": "weekly-notes/week-09/week-09-slides.html#workflow-continued",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Workflow (Continued)",
    "text": "Workflow (Continued)\nStep 4: Validation\n\nSpatial cross-validation (LOGO-CV)\nCalculate error metrics\nMap prediction errors\n\nStep 5: Comparison\n\nCalculate KDE baseline\nTest both on hold-out data (2018)\nCompare hit rates by risk category\n\nStep 6: Critical Analysis\n\nInterrogate data sources\nIdentify potential biases\nAssess harms and benefits\nPropose alternatives"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#what-if-we-predicted-differently",
    "href": "weekly-notes/week-09/week-09-slides.html#what-if-we-predicted-differently",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "What If We Predicted Differently?",
    "text": "What If We Predicted Differently?\nInstead of:\n\nWhere will crimes occur? → Deploy police\n\nConsider:\n\nWhere is social services need highest? → Deploy resources\nWhere is community trust lowest? → Invest in legitimacy\nWhere is opportunity lowest? → Economic development\nWhere is housing instability highest? → Stabilize residents\n\nSame technical methods, different normative goal"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#predictive-models-for-justice-not-policing",
    "href": "weekly-notes/week-09/week-09-slides.html#predictive-models-for-justice-not-policing",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Predictive Models for Justice, Not Policing",
    "text": "Predictive Models for Justice, Not Policing\nExamples:\n\nPredict eviction risk → Provide legal aid proactively\nPredict health crisis → Deploy community health workers\nPredict school dropout → Intensive support services\nPredict food insecurity → Expand access programs\n\nKey differences:\n\nPredictions lead to help, not punishment\nFalse positives = extra support (relatively low harm)\nBuilds trust instead of eroding it\nAddresses root causes"
  },
  {
    "objectID": "weekly-notes/week-09/week-09-slides.html#key-takeaways",
    "href": "weekly-notes/week-09/week-09-slides.html#key-takeaways",
    "title": "CRITICAL PERSPECTIVES ON PREDICTIVE POLICING",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantRemember:\n\n\n\n\nAll crime data is socially constructed - It reflects policing, not just crime\nHistorical harms persist in data - Dirty data trains biased systems\nVendors are not accountable - Black boxes prevent oversight\nFeedback loops amplify inequality - Predictions become self-fulfilling\nTechnical accuracy ≠ social justice - A “good” model can be ethically terrible\nAlternatives exist - We could predict need instead of threat\nYour choices matter - How you build models has real consequences"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner.\n\n\n\n\n\n\nRun the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC?\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#your-role",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#your-role",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "You are policy analysts hired by the Georgia Department of Corrections. They are considering deploying a recidivism prediction model to inform parole decisions. Your team must analyze the model and make a GO/NO-GO recommendation to the Commissioner."
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#instructions",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#instructions",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "",
    "text": "Run the provided R script (week10_exercise.R) and note:\n\nWhat’s the model’s AUC?\nAt threshold 0.50, what’s the sensitivity and specificity?\nWhich racial group has the highest false positive rate?\nWhich group has the highest false negative rate?\nWhat happens if we change the threshold to 0.30 or 0.70?\n\n\n\n\nAs a table or half table team, discuss your findings and complete the template below. Prepare to present your recommendation.\n\n\n\nPresent your recommendation to the “Commissioner” (instructor)."
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#consulting-team-information",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#consulting-team-information",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "Consulting Team Information",
    "text": "Consulting Team Information\nClever Team Name: _____________\nTeam Members:"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#technical-assessment",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#technical-assessment",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "1. TECHNICAL ASSESSMENT",
    "text": "1. TECHNICAL ASSESSMENT\n\nModel Performance Metrics\nAUC (Area Under ROC Curve): __________\nAt threshold = 0.50:\n\nSensitivity (True Positive Rate): __________\nSpecificity (True Negative Rate): __________\nPrecision (Positive Predictive Value): __________\nOverall Accuracy: __________\n\n\n\nTechnical Quality Rating\nSelect one:\n\nExcellent (AUC &gt; 0.90)\nGood (AUC 0.80-0.90)\nAcceptable (AUC 0.70-0.80)\nPoor (AUC &lt; 0.70)\n\n\n\nBrief Technical Summary (2-3 sentences)\nIs the model accurate enough for high-stakes decision-making?"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#equity-analysis",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#equity-analysis",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "2. EQUITY ANALYSIS",
    "text": "2. EQUITY ANALYSIS\n\nFalse Positive Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Positive Rate\nSample Size\n\n\n\n\nGroup 1:\n\n\n\n\nGroup 2:\n\n\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nFalse Negative Rates by Race (at threshold 0.50)\n\n\n\nRacial Group\nFalse Negative Rate\nSample Size\n\n\n\n\nGroup 1:\n\n\n\n\nGroup 2:\n\n\n\n\nGroup 3:\n\n\n\n\nGroup 4:\n\n\n\n\n\n\n\nDisparity Analysis\nLargest disparity identified:\nGroup _____________ has ______% higher false positive rate than Group _____________\nOR\nGroup _____________ has ______% higher false negative rate than Group _____________\n\n\nEquity Concerns Summary (3-4 sentences)\nWhat are the implications of these disparities? Who is harmed?"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#threshold-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "3. THRESHOLD RECOMMENDATION",
    "text": "3. THRESHOLD RECOMMENDATION\n\nIf we deploy this model, we recommend:\nSelect one:\n\nThreshold = 0.30 (Aggressive - prioritize catching recidivists)\nThreshold = 0.50 (Balanced - default)\nThreshold = 0.70 (Conservative - minimize false accusations)\nOther: ________\n\n\n\nRationale for Threshold Choice (3-4 sentences)\nWhy this threshold? What does it optimize for? What are the trade-offs?\n\n\n\n\n\n\nThis threshold prioritizes:\nSelect one:\n\nHigh Sensitivity - Catch more people who will reoffend (accept more false positives)\nHigh Specificity - Avoid false accusations (accept more false negatives)\nBalance - Try to minimize both types of errors"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#deployment-recommendation",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "4. DEPLOYMENT RECOMMENDATION",
    "text": "4. DEPLOYMENT RECOMMENDATION\n\nOur recommendation to Georgia DOC:\nSelect one:\n\nDEPLOY - Use this model to inform parole decisions\nDO NOT DEPLOY - Do not use this model\nCONDITIONAL DEPLOY - Deploy only with specific safeguards in place\n\n\n\nKey Reasons for Our Recommendation\nProvide 3-5 bullet points supporting your decision:\n\n\n\n\n\n\n\n\n\nWhat about the equity concerns?\nHow do you justify your recommendation given the disparate impact you identified?"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#safeguards-or-alternatives",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "5. SAFEGUARDS OR ALTERNATIVES",
    "text": "5. SAFEGUARDS OR ALTERNATIVES\n\nIf DEPLOY - Required Safeguards\nWhat protections must be in place before deployment?\n\n\n\n\n\n\nOR\n\n\nIf DO NOT DEPLOY - Alternative Approaches\nWhat should Georgia DOC do instead?"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#limitations-uncertainties",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "6. LIMITATIONS & UNCERTAINTIES",
    "text": "6. LIMITATIONS & UNCERTAINTIES\n\nWhat we don’t know (but wish we did)\nWhat additional information would strengthen your recommendation?\n\n\n\n\n\nWeaknesses in our recommendation\nWhat’s the strongest argument AGAINST your recommendation?"
  },
  {
    "objectID": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#bottom-line",
    "href": "weekly-notes/week-10/In_Class_Exercise_Instruction.html#bottom-line",
    "title": "Georgia DOC Policy Advisory Challenge",
    "section": "7. BOTTOM LINE",
    "text": "7. BOTTOM LINE\n\nOne-Sentence Recommendation\nIf the Commissioner only reads one thing, what should it be?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html",
    "href": "weekly-notes/week-10/week-10-class-lab.html",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "",
    "text": "This script demonstrates how to:\n\nBuild a logistic regression model for binary classification\nEvaluate model performance using multiple metrics\nAnalyze disparate impact across demographic groups\nTest different decision thresholds\nMake evidence-based policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#week-10-musa-5080-public-policy-analytics",
    "href": "weekly-notes/week-10/week-10-class-lab.html#week-10-musa-5080-public-policy-analytics",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "",
    "text": "This script demonstrates how to:\n\nBuild a logistic regression model for binary classification\nEvaluate model performance using multiple metrics\nAnalyze disparate impact across demographic groups\nTest different decision thresholds\nMake evidence-based policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#confusion-matrix-at-default-threshold-0.5",
    "href": "weekly-notes/week-10/week-10-class-lab.html#confusion-matrix-at-default-threshold-0.5",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "6.1: Confusion Matrix at Default Threshold (0.5)",
    "text": "6.1: Confusion Matrix at Default Threshold (0.5)\n\n\nCode\ntest_data &lt;- test_data %&gt;%\n  mutate(predicted_class_50 = ifelse(predicted_prob &gt; 0.5, 1, 0))\n\n# Create confusion matrix\n\ncm_50 &lt;- confusionMatrix(as.factor(test_data$predicted_class_50),\n  as.factor(test_data$recidivism), positive = \"1\") # \"1\" is the positive class (recidivism)\n\nprint(cm_50)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1294  719\n         1 1334 3204\n                                          \n               Accuracy : 0.6866          \n                 95% CI : (0.6752, 0.6978)\n    No Information Rate : 0.5988          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.3215          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n                                          \n            Sensitivity : 0.8167          \n            Specificity : 0.4924          \n         Pos Pred Value : 0.7060          \n         Neg Pred Value : 0.6428          \n             Prevalence : 0.5988          \n         Detection Rate : 0.4891          \n   Detection Prevalence : 0.6927          \n      Balanced Accuracy : 0.6546          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nCode\n# Extract key metrics\n\ncat(\"nKey Metrics at Threshold = 0.5:n\")\n\n\nnKey Metrics at Threshold = 0.5:n\n\n\nCode\ncat(\"Sensitivity (Recall):\", round(cm_50$byClass[\"Sensitivity\"], 3),\n    \"- Proportion of actual recidivists correctly identifiedn\")\n\n\nSensitivity (Recall): 0.817 - Proportion of actual recidivists correctly identifiedn\n\n\nCode\ncat(\"Specificity:\", round(cm_50$byClass[\"Specificity\"], 3), \"- Proportion of non-recidivists correctly identifiedn\")\n\n\nSpecificity: 0.492 - Proportion of non-recidivists correctly identifiedn\n\n\nCode\ncat(\"Precision (PPV):\", round(cm_50$byClass[\"Precision\"], 3),\n    \"- Proportion of predicted recidivists who actually recidivatedn\")\n\n\nPrecision (PPV): 0.706 - Proportion of predicted recidivists who actually recidivatedn\n\n\nCode\ncat(\"Accuracy:\", round(cm_50$overall[\"Accuracy\"], 3), \"nn\")\n\n\nAccuracy: 0.687 nn"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#roc-curve-and-auc",
    "href": "weekly-notes/week-10/week-10-class-lab.html#roc-curve-and-auc",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "6.2: ROC Curve and AUC",
    "text": "6.2: ROC Curve and AUC\n\n\nCode\n# Generate ROC curve\n\nroc_obj &lt;- roc(response = test_data$recidivism,\n  predictor = test_data$predicted_prob )\n\n# Plot ROC curve\n\nggroc(roc_obj,\n      color = \"steelblue\",\n      size = 1.5\n      ) +\n  geom_abline(slope = 1,\n              intercept = 1,\n              linetype = \"dashed\",\n              color = \"gray50\"\n              ) +\n  labs(title = \"ROC Curve: Recidivism Prediction Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\"\n       ) +\n  annotate(\"text\",\n           x = 0.5,\n           y = 0.3,\n           label = \"Random Classifiern(AUC = 0.5)\",\n           color = \"gray50\",\n           size = 3\n           ) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nCode\n# Print AUC\n\ncat(\"Area Under the Curve (AUC):\", round(auc(roc_obj), 3), \"n\")\n\n\nArea Under the Curve (AUC): 0.732 n\n\n\nCode\ncat(\"Interpretation: AUC of\", round(auc(roc_obj), 2), \"indicates\", ifelse(auc(roc_obj) &gt; 0.8, \"good\", \"acceptable\"), \"discriminationnn\")\n\n\nInterpretation: AUC of 0.73 indicates acceptable discriminationnn"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#performance-across-multiple-thresholds",
    "href": "weekly-notes/week-10/week-10-class-lab.html#performance-across-multiple-thresholds",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "6.3: Performance Across Multiple Thresholds",
    "text": "6.3: Performance Across Multiple Thresholds\n\n\nCode\n# Test three different thresholds\n\nthresholds_to_test &lt;- c(0.3, 0.5, 0.7)\n\nthreshold_results &lt;- map_df(thresholds_to_test, function(thresh) { # Generate predictions at this threshold\n  preds &lt;- ifelse(test_data$predicted_prob &gt; thresh, 1, 0)\n\n# Calculate confusion matrix\n  cm &lt;- confusionMatrix(as.factor(preds),\n                        as.factor(test_data$recidivism),\n                        positive = \"1\")\n\n# Extract metrics\n  data.frame(threshold = thresh,\n             accuracy = cm$overall[\"Accuracy\"],\n             sensitivity = cm$byClass[\"Sensitivity\"],\n             specificity = cm$byClass[\"Specificity\"],\n             precision = cm$byClass[\"Precision\"],\n             f1_score = cm$byClass[\"F1\"],\n             # Calculate false positive and false negative rates\n             fpr = 1 - cm$byClass[\"Specificity\"],\n             fnr = 1 - cm$byClass[\"Sensitivity\"] ) })\n\n# Display results\n\nprint(threshold_results)\n\n\n             threshold  accuracy sensitivity specificity precision  f1_score\nAccuracy...1       0.3 0.6476874   0.9686464   0.1685693 0.6349206 0.7670569\nAccuracy...2       0.5 0.6866127   0.8167219   0.4923896 0.7060379 0.7573573\nAccuracy...3       0.7 0.6209739   0.5034412   0.7964231 0.7868526 0.6140215\n                   fpr        fnr\nAccuracy...1 0.8314307 0.03135356\nAccuracy...2 0.5076104 0.18327810\nAccuracy...3 0.2035769 0.49655876\n\n\nCode\n# Visualize threshold trade-offs\n\nthreshold_results %&gt;%\n  select(threshold, sensitivity, specificity, precision) %&gt;%\n  pivot_longer(cols = c(sensitivity, specificity, precision),\n               names_to = \"metric\",\n               values_to = \"value\") %&gt;%\n  ggplot(aes(x = threshold,\n             y = value,\n             color = metric,\n             group = metric)\n         ) +\n  geom_line(size = 1.2\n            ) +\n  geom_point(size = 3\n             ) +\n  scale_color_brewer(palette = \"Set1\",\n                     labels = c(\"Precision\", \"Sensitivity\", \"Specificity\")\n                     ) +\n  scale_x_continuous(breaks = thresholds_to_test\n                     ) +\n  labs(title = \"Performance Metrics Across Thresholds\",\n       subtitle = \"The threshold-performance trade-off\",\n       x = \"Probability Threshold\",\n       y = \"Metric Value\",\n       color = \"Metric\"\n       ) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#critical-insight",
    "href": "weekly-notes/week-10/week-10-class-lab.html#critical-insight",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "CRITICAL INSIGHT:",
    "text": "CRITICAL INSIGHT:\nAs threshold increases:\n\nFewer people flagged as high-risk (more conservative)\nSensitivity decreases (miss more actual recidivists)\nSpecificity increases (fewer false accusations)\nPrecision usually increases (predictions more accurate when made)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#calculate-performance-metrics-by-race",
    "href": "weekly-notes/week-10/week-10-class-lab.html#calculate-performance-metrics-by-race",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "7.1: Calculate Performance Metrics by Race",
    "text": "7.1: Calculate Performance Metrics by Race\n\n\nCode\n# First, calculate overall metrics for comparison\n\noverall_metrics &lt;- test_data %&gt;%\n  summarise(Group = \"Overall\",\n            N = n(),\n            Base_Rate = mean(recidivism),\n            Sensitivity = sum(predicted_class_50 == 1 & recidivism == 1) / sum(recidivism == 1),\n            Specificity = sum(predicted_class_50 == 0 & recidivism == 0) / sum(recidivism == 0),\n            Precision = sum(predicted_class_50 == 1 & recidivism == 1) / sum(predicted_class_50 == 1),\n            FPR = sum(predicted_class_50 == 1 & recidivism == 0) / sum(recidivism == 0),\n            FNR = sum(predicted_class_50 == 0 & recidivism == 1) / sum(recidivism == 1))\n\n# Calculate metrics by race\n\nrace_metrics &lt;- test_data %&gt;%\n  group_by(Race) %&gt;%\n  summarise(N = n(),\n            Base_Rate = mean(recidivism),\n            Sensitivity = sum(predicted_class_50 == 1 & recidivism == 1) / sum(recidivism == 1),\n            Specificity = sum(predicted_class_50 == 0 & recidivism == 0) / sum(recidivism == 0),\n            Precision = sum(predicted_class_50 == 1 & recidivism == 1) / sum(predicted_class_50 == 1),\n            FPR = sum(predicted_class_50 == 1 & recidivism == 0) / sum(recidivism == 0),\n            FNR = sum(predicted_class_50 == 0 & recidivism == 1) / sum(recidivism == 1) ) %&gt;%\n  rename(Group = Race)\n\n# Combine overall and by-group metrics\n\nequity_analysis &lt;- bind_rows(overall_metrics, race_metrics)\n\n# Display results\n\nprint(\"=\" %&gt;% rep(80) %&gt;% paste0(collapse = \"\"))\n\n\n[1] \"================================================================================\"\n\n\nCode\nprint(\"EQUITY ANALYSIS: Model Performance by Race (Threshold = 0.5)\")\n\n\n[1] \"EQUITY ANALYSIS: Model Performance by Race (Threshold = 0.5)\"\n\n\nCode\nprint(\"=\" %&gt;% rep(80) %&gt;% paste0(collapse = \"\"))\n\n\n[1] \"================================================================================\"\n\n\nCode\nprint(equity_analysis, width = Inf)\n\n\n# A tibble: 3 × 8\n  Group       N Base_Rate Sensitivity Specificity Precision   FPR   FNR\n  &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Overall  6551     0.599       0.817       0.492     0.706 0.508 0.183\n2 BLACK    3931     0.598       0.846       0.438     0.691 0.562 0.154\n3 WHITE    2620     0.600       0.773       0.575     0.732 0.425 0.227\n\n\nCode\nprint(\"=\" %&gt;% rep(80) %&gt;% paste0(collapse = \"\"))\n\n\n[1] \"================================================================================\""
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#visualize-disparate-impact",
    "href": "weekly-notes/week-10/week-10-class-lab.html#visualize-disparate-impact",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "7.2: Visualize Disparate Impact",
    "text": "7.2: Visualize Disparate Impact\n\n\nCode\n# Create bar chart comparing key metrics across groups\n\nrace_metrics %&gt;%\n  select(Group, FPR, FNR, Sensitivity, Specificity) %&gt;%\n  pivot_longer(cols = c(FPR, FNR, Sensitivity, Specificity),\n               names_to = \"Metric\", values_to = \"Rate\") %&gt;%\n  ggplot(aes(x = Group, y = Rate, fill = Metric)\n         ) +\n  geom_col(position = \"dodge\",\n           alpha = 0.8\n           ) +\n  scale_fill_brewer(palette = \"Set2\"\n                    ) +\n  labs(title = \"Model Performance Disparities Across Racial Groups\",\n       subtitle = \"Using threshold = 0.5\",\n       x = \"Race\",\n       y = \"Rate\",\n       fill = \"Metric\",\n       caption = \"FPR = False Positive Rate, FNR = False Negative Rate\"\n       ) +\n  theme(axis.text.x = element_text(angle = 45,\n                                   hjust = 1))"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#testing-threshold-adjustments-for-equity",
    "href": "weekly-notes/week-10/week-10-class-lab.html#testing-threshold-adjustments-for-equity",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "7.3: Testing Threshold Adjustments for Equity",
    "text": "7.3: Testing Threshold Adjustments for Equity\n\n\nCode\n# Can we achieve more equitable outcomes by adjusting thresholds?\n\n# Function to calculate key rates at a given threshold\n\ncalc_rates_by_threshold &lt;- function(data, threshold) {\n  data %&gt;%\n    mutate(pred = ifelse(predicted_prob &gt; threshold, 1, 0)) %&gt;%\n    summarise( threshold = threshold,\n               FPR = sum(pred == 1 & recidivism == 0) / sum(recidivism == 0),\n               FNR = sum(pred == 0 & recidivism == 1) / sum(recidivism == 1),\n               Sensitivity = sum(pred == 1 & recidivism == 1) / sum(recidivism == 1),\n               Specificity = sum(pred == 0 & recidivism == 0) / sum(recidivism == 0) ) }\n\n# Test range of thresholds for each racial group\n\nthreshold_range &lt;- seq(0.3, 0.7, by = 0.05)\n\nthreshold_by_race &lt;- test_data %&gt;%\n  nest_by(Race) %&gt;%\n  reframe(map_df(threshold_range,\n                 ~calc_rates_by_threshold(data, .x))) %&gt;%\n  ungroup()\n\n# Visualize FPR across thresholds by race\n\nggplot(threshold_by_race,\n       aes(x = threshold, y = FPR,color = Race)\n       ) +\n  geom_line(size = 1.2\n            ) +\n  geom_point(size = 2\n            ) +\n  geom_vline(xintercept = 0.5,\n             linetype = \"dashed\",\n             alpha = 0.5\n             ) +\n  labs(title = \"False Positive Rate by Threshold and Race\",\n       subtitle = \"Could different thresholds equalize FPR?\",\n       x = \"Probability Threshold\",\n       y = \"False Positive Rate\",\n       color = \"Race\")\n\n\n\n\n\n\n\n\n\nCode\n# Visualize FNR across thresholds by race\n\nggplot(threshold_by_race,\n       aes(x = threshold, y = FNR, color = Race)\n       ) +\n  geom_line(size = 1.2\n            ) +\n  geom_point(size = 2\n             ) +\n  geom_vline(xintercept = 0.5,\n             linetype = \"dashed\",\n             alpha = 0.5\n             ) +\n  labs(title = \"False Negative Rate by Threshold and Race\",\n       subtitle = \"The trade-off: equalizing FPR may worsen FNR disparities\",\n       x = \"Probability Threshold\",\n       y = \"False Negative Rate\",\n       color = \"Race\")"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#extension-1-calibration-analysis",
    "href": "weekly-notes/week-10/week-10-class-lab.html#extension-1-calibration-analysis",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "Extension 1: Calibration Analysis",
    "text": "Extension 1: Calibration Analysis\n\n\nCode\n# Check if predicted probabilities match actual outcomes\n\ntest_data %&gt;%\n  mutate(prob_bin = cut(predicted_prob,\n                        breaks = seq(0, 1, by = 0.1))\n         ) %&gt;%\n  group_by(prob_bin) %&gt;%\n  summarise(mean_predicted = mean(predicted_prob),\n            mean_observed = mean(recidivism),\n            n = n() ) %&gt;%\n  filter(n &gt; 10) %&gt;%\n  ggplot(aes(x = mean_predicted, y = mean_observed, size = n)\n         ) +\n  geom_point(alpha = 0.6, color = \"steelblue\"\n             ) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\"\n              ) +\n  labs(title = \"Calibration Plot\",\n       subtitle = \"Are predicted probabilities well-calibrated?\",\n       x = \"Mean Predicted Probability\",\n       y = \"Mean Observed Recidivism Rate\",\n       size = \"N in bin\"\n       ) +\n  coord_fixed()"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-class-lab.html#extension-2-feature-importance",
    "href": "weekly-notes/week-10/week-10-class-lab.html#extension-2-feature-importance",
    "title": "LOGISTIC REGRESSION WITH EQUITY ANALYSIS",
    "section": "Extension 2: Feature Importance",
    "text": "Extension 2: Feature Importance\n\n\nCode\n# Which variables matter most?\n\ncoef_df &lt;- summary(logit_model)$coefficients %&gt;%\n  as.data.frame() %&gt;% rownames_to_column(\"variable\") %&gt;%\n  mutate(odds_ratio = exp(Estimate)) %&gt;%\n  filter(variable != \"(Intercept)\") %&gt;%\n  arrange(desc(abs(Estimate)))\n\nggplot(coef_df,\n       aes(x = reorder(variable, Estimate),\n           y = odds_ratio)\n       ) +\n  geom_point(size = 3, color = \"steelblue\"\n             ) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\"\n             ) +\n  coord_flip() +\n  scale_y_continuous(trans = \"log10\") +\n  labs(title = \"Feature Importance (Odds Ratios)\",\n       subtitle = \"OR &gt; 1 increases recidivism risk; OR &lt; 1 decreases risk\",\n       x = \"Variable\",\n       y = \"Odds Ratio (log scale)\")"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html",
    "href": "weekly-notes/week-10/week-10-slides.html",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "",
    "text": "Scenario: A state corrections department asks you to help predict:\nWill someone released from prison be arrested again within 3 years?\nNot asking: How many times? How long until?\nAsking: Yes or no? Will it happen or not?\nDiscussion question (1 minute):\n\nHow is this different from predicting home prices?\nWhy might they want this prediction?\nWhat could go wrong?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#a-decision-with-real-consequences",
    "href": "weekly-notes/week-10/week-10-slides.html#a-decision-with-real-consequences",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "",
    "text": "Scenario: A state corrections department asks you to help predict:\nWill someone released from prison be arrested again within 3 years?\nNot asking: How many times? How long until?\nAsking: Yes or no? Will it happen or not?\nDiscussion question (1 minute):\n\nHow is this different from predicting home prices?\nWhy might they want this prediction?\nWhat could go wrong?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#where-weve-been",
    "href": "weekly-notes/week-10/week-10-slides.html#where-weve-been",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Where We’ve Been",
    "text": "Where We’ve Been\nWeeks 1-7: Linear regression\n\nPredicting continuous outcomes: home prices, income, population\nY = β₀ + β₁X₁ + β₂X₂ + … + ε\nUsed RMSE to evaluate predictions\n\nLast week: Poisson regression\n\nPredicting count outcomes: number of crimes\nDifferent distribution, but still predicting quantities\n\nToday: A fundamentally different question\n\nNot “how much?” but “will it happen?”\nBinary outcomes: yes/no, 0/1, success/failure\nThis requires a completely different approach"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#what-makes-binary-outcomes-different",
    "href": "weekly-notes/week-10/week-10-slides.html#what-makes-binary-outcomes-different",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "What Makes Binary Outcomes Different?",
    "text": "What Makes Binary Outcomes Different?\nThe problem with linear regression for binary outcomes:\n\n\n\n\n\n\n\n\n\nProblems:\n\nPredictions can be &gt; 1 or &lt; 0 (makes no sense for probability!)\nAssumes constant effect across range (not realistic)\nViolates regression assumptions (errors aren’t normal)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#enter-logistic-regression",
    "href": "weekly-notes/week-10/week-10-slides.html#enter-logistic-regression",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Enter: Logistic Regression",
    "text": "Enter: Logistic Regression\nThe solution: Transform the problem!\nInstead of predicting Y directly, predict the probability that Y = 1\nThe logistic function constrains predictions between 0 and 1:\n\\[p(X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k)}}\\]"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#logistic-vs.-linear-visual-comparison",
    "href": "weekly-notes/week-10/week-10-slides.html#logistic-vs.-linear-visual-comparison",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Logistic vs. Linear: Visual Comparison",
    "text": "Logistic vs. Linear: Visual Comparison\n\n\n\n\n\n\n\n\n\nKey difference: Logistic regression produces valid probabilities!"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#when-do-we-use-logistic-regression",
    "href": "weekly-notes/week-10/week-10-slides.html#when-do-we-use-logistic-regression",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "When Do We Use Logistic Regression?",
    "text": "When Do We Use Logistic Regression?\nPerfect for binary classification problems in policy:\nCriminal Justice:\n\nWill someone reoffend? (recidivism)\nWill someone appear for court? (flight risk)\n\nHealth:\n\nWill patient develop disease? (risk assessment)\nWill treatment be successful? (outcome prediction)\n\nEconomics:\n\nWill loan default? (credit risk)\nWill person get hired? (employment prediction)\n\nUrban Planning:\n\nWill building be demolished? (blight prediction)\nWill household participate in program? (uptake prediction)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-logit-transformation",
    "href": "weekly-notes/week-10/week-10-slides.html#the-logit-transformation",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The Logit Transformation",
    "text": "The Logit Transformation\nBehind the scenes: We work with log-odds, not probabilities directly\nOdds: \\(\\text{Odds} = \\frac{p}{1-p}\\)\nLog-Odds (Logit): \\(\\text{logit}(p) = \\ln\\left(\\frac{p}{1-p}\\right)\\)\nThis creates a linear relationship: \\[\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ...\\]\nWhy this matters:\n\nCoefficients are log-odds (like linear regression!)\nBut we interpret as odds ratios when exponentiated: \\(e^{\\beta}\\)\nOR &gt; 1: predictor increases odds of outcome\nOR &lt; 1: predictor decreases odds of outcome"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#example-email-spam-detection",
    "href": "weekly-notes/week-10/week-10-slides.html#example-email-spam-detection",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Example: Email Spam Detection",
    "text": "Example: Email Spam Detection\nLet’s build a simple spam detector to understand the mechanics.\nGoal: Predict whether email is spam (1) or legitimate (0)\nPredictors: - Number of exclamation marks - Contains word “free” - Email length\n\n\nCode\n# Create example spam detection data\nset.seed(123)\nn_emails &lt;- 1000\n\nspam_data &lt;- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Look at the data\nhead(spam_data)\n\n\n  exclamation_marks contains_free    length is_spam\n1                 4             1 150.21006       1\n2                 7             1 148.00225       1\n3                 4             1 199.10099       1\n4                 8             0 193.39124       1\n5                 9             0  72.53286       1\n6                 2             1 252.02867       1"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#fitting-the-logistic-model",
    "href": "weekly-notes/week-10/week-10-slides.html#fitting-the-logistic-model",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Fitting the Logistic Model",
    "text": "Fitting the Logistic Model\nIn R, we use glm() with family = \"binomial\"\n\n\nCode\n# Fit logistic regression\nspam_model &lt;- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\n\n\n\nCall:\nglm(formula = is_spam ~ exclamation_marks + contains_free + length, \n    family = \"binomial\", data = spam_data)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)         233.048  15015.736   0.016    0.988\nexclamation_marks    55.945  53708.285   0.001    0.999\ncontains_free        46.055  49298.975   0.001    0.999\nlength               -1.273     81.369  -0.016    0.988\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.5017e+02  on 999  degrees of freedom\nResidual deviance: 7.5863e-07  on 996  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#interpreting-coefficients",
    "href": "weekly-notes/week-10/week-10-slides.html#interpreting-coefficients",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\n\nCode\n# Extract coefficients\ncoefs &lt;- coef(spam_model)\nprint(coefs)\n\n\n      (Intercept) exclamation_marks     contains_free            length \n       233.048051         55.944824         46.055006         -1.272668 \n\n\nCode\n# Convert to odds ratios\nodds_ratios &lt;- exp(coefs)\nprint(odds_ratios)\n\n\n      (Intercept) exclamation_marks     contains_free            length \n    1.627356e+101      1.979376e+24      1.003310e+20      2.800833e-01 \n\n\nInterpretation:\n\nexclamation_marks: Each additional ! multiplies odds of spam by 1.9793761^{24}\ncontains_free: Having “free” multiplies odds by 1.0033099^{20}\n\nlength: Each additional character multiplies odds by 0.2801 (shorter = more likely spam)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#making-predictions",
    "href": "weekly-notes/week-10/week-10-slides.html#making-predictions",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Making Predictions",
    "text": "Making Predictions\nThe model outputs probabilities:\n\n\nCode\n# Predict probability for a new email\nnew_email &lt;- data.frame(\n  exclamation_marks = 3,\n  contains_free = 1,\n  length = 150\n)\n\npredicted_prob &lt;- predict(spam_model, newdata = new_email, type = \"response\")\ncat(\"Predicted probability of spam:\", round(predicted_prob, 3))\n\n\nPredicted probability of spam: 1\n\n\nBut now what?\n\nIf probability = 0.723, is this spam or not?\nWe need to choose a threshold (cutoff)\nThreshold = 0.5 is common default, but is it the right choice?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-fundamental-challenge",
    "href": "weekly-notes/week-10/week-10-slides.html#the-fundamental-challenge",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The Fundamental Challenge",
    "text": "The Fundamental Challenge\nThis is where logistic regression gets interesting (and complicated):\nThe model gives us probabilities, but we need to make binary decisions.\nQuestion: What probability threshold should we use to classify?\n\nThreshold = 0.5? (common default)\nThreshold = 0.3? (more aggressive - flag more as spam)\nThreshold = 0.7? (more conservative - only flag obvious spam)\n\nThe answer depends on:\n\nCost of false positives (marking legitimate email as spam)\nCost of false negatives (missing actual spam)\nThese costs are rarely equal!\n\nThe rest of today: How do we evaluate these predictions and choose thresholds?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#from-probabilities-to-decisions",
    "href": "weekly-notes/week-10/week-10-slides.html#from-probabilities-to-decisions",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "From Probabilities to Decisions",
    "text": "From Probabilities to Decisions\nWe now have a model that predicts probabilities.\nBut policy decisions require binary choices: spam/not spam, approve/deny, intervene/don’t intervene.\nThis requires two steps:\n\nChoose a threshold to convert probabilities → binary predictions\nEvaluate how good those predictions are\n\nThe confusion matrix helps us with step 2"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-four-outcomes",
    "href": "weekly-notes/week-10/week-10-slides.html#the-four-outcomes",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The Four Outcomes",
    "text": "The Four Outcomes\nWhen we make binary predictions, four things can happen:\n\n\nModel says “Yes”:\n\nTrue Positive (TP): Correct! ✓\nFalse Positive (FP): Wrong - Type I error\n\nModel says “No”:\n\nTrue Negative (TN): Correct! ✓\n\nFalse Negative (FN): Wrong - Type II error\n\n\n\n\n\nConfusion Matrix Structure\n\n\n\n\nRemember: The model predicts probabilities. WE choose the threshold that converts probabilities to yes/no predictions."
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#quick-example-covid-testing",
    "href": "weekly-notes/week-10/week-10-slides.html#quick-example-covid-testing",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Quick Example: COVID Testing",
    "text": "Quick Example: COVID Testing\nScenario: Testing for COVID-19\n\n\n\nCOVID Test Outcomes\n\n\n\n\n\n\n\n\nTrue_Status\nTest_Result\nOutcome\nConsequence\n\n\n\n\nPositive\nPositive\nTrue Positive (TP)\nQuarantine (correct)\n\n\nPositive\nNegative\nFalse Negative (FN)\nGoes to work, spreads virus\n\n\nNegative\nPositive\nFalse Positive (FP)\nQuarantines unnecessarily\n\n\nNegative\nNegative\nTrue Negative (TN)\nGoes to work (correct)\n\n\n\n\n\nWhich error is worse?\n\nFalse Negative → Virus spreads\nFalse Positive → Unnecessary quarantine\n\nThe answer depends on context! (And changes our threshold choice)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#calculating-performance-metrics",
    "href": "weekly-notes/week-10/week-10-slides.html#calculating-performance-metrics",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Calculating Performance Metrics",
    "text": "Calculating Performance Metrics\nFrom the confusion matrix, we derive metrics that emphasize different trade-offs:\nSensitivity (Recall, True Positive Rate): \\[\\text{Sensitivity} = \\frac{TP}{TP + FN}\\] “Of all actual positives, how many did we catch?”\nSpecificity (True Negative Rate): \\[\\text{Specificity} = \\frac{TN}{TN + FP}\\] “Of all actual negatives, how many did we correctly identify?”\nPrecision (Positive Predictive Value): \\[\\text{Precision} = \\frac{TP}{TP + FP}\\] “Of all our positive predictions, how many were correct?”"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#interactive-example-spam-detection",
    "href": "weekly-notes/week-10/week-10-slides.html#interactive-example-spam-detection",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Interactive Example: Spam Detection",
    "text": "Interactive Example: Spam Detection\nLet’s say we have an email spam filter:\n\n100 actual spam emails\n900 actual legitimate emails\nOur model makes predictions…\n\n\n\nCode\n# Create example predictions\nset.seed(123)\nspam_data &lt;- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %&gt;%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data &lt;- spam_data %&gt;%\n  mutate(predicted_spam = ifelse(predicted_prob &gt; 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat &lt;- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#spam-filter-results",
    "href": "weekly-notes/week-10/week-10-slides.html#spam-filter-results",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Spam Filter Results",
    "text": "Spam Filter Results\n\n\n          Reference\nPrediction   0   1\n         0 760  14\n         1 140  86\n\n\n\nSensitivity: 0.86 - We caught 86 % of spam\n\n\nSpecificity: 0.844 - We correctly identified 84.4 % of legitimate emails\n\n\nPrecision: 0.381 - Of emails marked spam, 38.1 % actually were spam\n\n\nQuestion: What happens if we change the threshold?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#why-threshold-choice-matters",
    "href": "weekly-notes/week-10/week-10-slides.html#why-threshold-choice-matters",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Why Threshold Choice Matters",
    "text": "Why Threshold Choice Matters\nRemember: The model gives us probabilities. We decide what probability triggers action.\nThreshold = 0.3 (low bar) - More emails marked as spam - Higher sensitivity (catch more spam) - Lower specificity (more false alarms)\nThreshold = 0.7 (high bar) - Fewer emails marked as spam - Lower sensitivity (miss some spam) - Higher specificity (fewer false alarms)\nThere is no “right” answer - it depends on the costs of each type of error"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-great-sensitivity-specificity-trade-off",
    "href": "weekly-notes/week-10/week-10-slides.html#the-great-sensitivity-specificity-trade-off",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The Great Sensitivity-Specificity Trade-off",
    "text": "The Great Sensitivity-Specificity Trade-off\n\n\nCode\n# Calculate metrics at different thresholds\nthresholds &lt;- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold &lt;- map_df(thresholds, function(thresh) {\n  preds &lt;- ifelse(spam_data$predicted_prob &gt; thresh, 1, 0)\n  cm &lt;- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#two-policy-scenarios",
    "href": "weekly-notes/week-10/week-10-slides.html#two-policy-scenarios",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Two Policy Scenarios",
    "text": "Two Policy Scenarios\nScenario A: Rare, deadly disease screening\n\nDisease is rare but fatal if untreated\nTreatment is safe with minor side effects\nGoal: Don’t miss any cases (high sensitivity)\nAcceptable: Some false positives (low threshold)\n\nScenario B: Identifying “high-risk” individuals for intervention\n\nLimited intervention slots\nFalse positives waste resources\nFalse negatives miss opportunities to help\nGoal: Use resources efficiently (high precision)\nDecision depends on: Cost of intervention vs. cost of missed case\n\nClass discussion: Which metrics matter most for each scenario?"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-roc-curve-visualizing-all-thresholds",
    "href": "weekly-notes/week-10/week-10-slides.html#the-roc-curve-visualizing-all-thresholds",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The ROC Curve: Visualizing All Thresholds",
    "text": "The ROC Curve: Visualizing All Thresholds\nROC = Receiver Operating Characteristic\n(Originally developed for radar signal detection in WWII)\nWhat it shows:\n\nEvery possible threshold\nTrade-off between True Positive Rate (Sensitivity) and False Positive Rate (1 - Specificity)\nOverall model discrimination ability\n\nHow to read it:\n\nX-axis: False Positive Rate (1 - Specificity)\nY-axis: True Positive Rate (Sensitivity)\nDiagonal line: Random guessing\nTop-left corner: Perfect prediction"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#creating-an-roc-curve",
    "href": "weekly-notes/week-10/week-10-slides.html#creating-an-roc-curve",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Creating an ROC Curve",
    "text": "Creating an ROC Curve\n\n\nCode\n# Create ROC curve for our spam example\nroc_obj &lt;- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nCode\n# Print AUC\nauc_value &lt;- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n\n\n\nArea Under the Curve (AUC): 0.938"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#interpreting-auc",
    "href": "weekly-notes/week-10/week-10-slides.html#interpreting-auc",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Interpreting AUC",
    "text": "Interpreting AUC\nAUC (Area Under the Curve) summarizes overall model performance:\n\nAUC = 1.0: Perfect classifier\nAUC = 0.9-1.0: Excellent\nAUC = 0.8-0.9: Good\nAUC = 0.7-0.8: Acceptable\nAUC = 0.6-0.7: Poor\nAUC = 0.5: No better than random guessing\nAUC &lt; 0.5: Worse than random (your model is backwards!)\n\nOur spam filter AUC = 0.938\nInterpretation: The model has good discrimination ability, but…\n\nAUC doesn’t tell us which threshold to use\nAUC doesn’t account for class imbalance\nAUC doesn’t show us equity implications"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#understanding-the-roc-curve-points",
    "href": "weekly-notes/week-10/week-10-slides.html#understanding-the-roc-curve-points",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Understanding the ROC Curve Points",
    "text": "Understanding the ROC Curve Points"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#the-core-problem-disparate-impact",
    "href": "weekly-notes/week-10/week-10-slides.html#the-core-problem-disparate-impact",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "The Core Problem: Disparate Impact",
    "text": "The Core Problem: Disparate Impact\nA model can be “accurate” overall but perform very differently across groups\nExample metrics from a recidivism model:\n\n\n\nGroup\nSensitivity\nSpecificity\nFalse Positive Rate\n\n\n\n\nOverall\n0.72\n0.68\n0.32\n\n\nGroup A\n0.78\n0.74\n0.26\n\n\nGroup B\n0.64\n0.58\n0.42\n\n\n\nGroup B experiences:\n\nLower sensitivity (more people who will reoffend are missed)\nLower specificity (more people who won’t reoffend are flagged)\nHigher false positive rate (more unjust interventions)\n\nThis is algorithmic bias in action"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#real-world-case-compas",
    "href": "weekly-notes/week-10/week-10-slides.html#real-world-case-compas",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Real-World Case: COMPAS",
    "text": "Real-World Case: COMPAS\nCOMPAS: Commercial algorithm used in criminal justice to predict recidivism\nProPublica investigation (2016) found:\n\nSimilar overall accuracy for Black and White defendants\nBUT: False positive rates differed dramatically\n\nBlack defendants: 45% false positive rate\nWhite defendants: 23% false positive rate\n\nBlack defendants twice as likely to be incorrectly labeled “high risk”\n\nResult:\n\nDifferent threshold needed for different groups to achieve equity\nBut single-threshold systems are the norm\nKey insight: Overall accuracy masks disparate impact"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#framework-for-threshold-selection",
    "href": "weekly-notes/week-10/week-10-slides.html#framework-for-threshold-selection",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Framework for Threshold Selection",
    "text": "Framework for Threshold Selection\nStep 1: Understand the consequences\n\nWhat happens with a false positive?\nWhat happens with a false negative?\nAre costs symmetric or asymmetric?\n\nStep 2: Consider stakeholder perspectives\n\nWho is affected by each type of error?\nDo all groups experience consequences equally?\n\nStep 3: Choose your metric priority\n\nMaximize sensitivity? (catch all positives)\nMaximize specificity? (minimize false alarms)\nBalance precision and recall? (F1 score)\nEqualize across groups?\n\nStep 4: Test multiple thresholds\n\nEvaluate performance across thresholds\nLook at group-wise performance\nConsider sensitivity analysis"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#cost-benefit-analysis-approach",
    "href": "weekly-notes/week-10/week-10-slides.html#cost-benefit-analysis-approach",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Cost-Benefit Analysis Approach",
    "text": "Cost-Benefit Analysis Approach\nAssign concrete costs to errors:\nExample: Disease screening\n\nTrue Positive: Treatment cost $1000, prevent $50,000 in complications\nFalse Positive: Unnecessary treatment $1000\nTrue Negative: No cost\nFalse Negative: Miss disease, $50,000 in complications later\n\nCalculate expected cost at each threshold: \\[E[\\text{Cost}] = C_{FP} \\times FP + C_{FN} \\times FN\\]\nChoose threshold that minimizes expected cost\nNote: This assumes we can quantify all costs, which is often impossible for justice/equity concerns"
  },
  {
    "objectID": "weekly-notes/week-10/week-10-slides.html#practical-recommendations",
    "href": "weekly-notes/week-10/week-10-slides.html#practical-recommendations",
    "title": "LOGISTIC REGRESSION FOR BINARY OUTCOMES",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\n\nReport multiple metrics - not just accuracy\nShow the ROC curve - demonstrates trade-offs\nTest multiple thresholds - document your choice\nEvaluate by sub-group - check for disparate impact\nDocument assumptions - explain why you chose your threshold\nConsider context - what are the real-world consequences?\nProvide uncertainty - confidence intervals, not just point estimates\nEnable recourse - can predictions be challenged?\n\nMost importantly: Be transparent about limitations and potential harms"
  }
]