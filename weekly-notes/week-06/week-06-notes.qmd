---
title: "SPATIAL MACHINE LEARNING & ADVANCED REGRESSION"
subtitle: WEEK 6 NOTES
date: 2025-10-13
author:
  - name: Tess Vu
    email:
      - tessavu@proton.me
      - tessavu@upenn.edu
    corresponding: TRUE
affiliation:
  - name: University of Pennsylvania
    department: Urban Spatial Analytics (MUSA)
    city: Philadelphia
    state: PA
    url: https://www.design.upenn.edu/urban-spatial-analytics
format:
  html:
    code-fold: show
    toc: true
    toc-location: left
    toc-expand: true
    smooth-scroll: true
    embed-resources: true
    title-block-style: default
execute:
  warning: false
  message: false
---

## Key Concepts Learned

### Baseline Model

```{r}
# Load packages and data
library(tidyverse)
library(sf)
library(here)

# Load Boston housing data
boston <- read_csv(here("data/boston.csv"))

# Quick look at the data
glimpse(boston)
```

```{r}
# Simple model: Predict price from living area
# SalePrice is y (response) and LivingArea is x (predictor).
baseline_model <- lm(SalePrice ~ LivingArea, data = boston)
summary(baseline_model)
```

- Interpretation
  
  - Intercept's Estimate: Base Sale Price at 0 sq. ft. is ~$157,968.32. Not useful in practice.
  
  - Living Area's Estimate: Each additional square foot of Living Area adds ~$216 to Sale Price.
  
  - Relationship is statistically significant (p < 0.001). If p is small, reject the null hypothesis.
  
  - R^2 is 0.13, so 13% of variation is explained by regression model output.
  
    - Most of the 87% variation is unexplained, so what's the problem?
    
      - Limitations
      
        - Ignores location (North End vs. Roxbury vs. Back Bay); proximity to downtown, waterfront, parks, etc.; nearby crime levels; school quality; neighborhood characteristics.
        
        - Might fail because 1,000 sq ft in Back Bay is not equivalent to 1,000 sq ft in Roxbury; same house, different locations, means very different prices; location!
        
        - Could improve by adding spatial features like crime and distance to amenities; control for neighborhood (fixed effects); include interactions (does size matter more in wealthy areas)?
        
        - Just doing a linear regression is a limit, so that's where spatial features come in.
        
```{r}
# Add number of bathrooms
better_model <- lm(SalePrice ~ LivingArea + R_FULL_BTH, data = boston)
summary(better_model)
```

```{r}
# Compare models
cat("Baseline R²:", summary(baseline_model)$r.squared, "\n")
```

```{r}
cat("With bathrooms R²:", summary(better_model)$r.squared, "\n")
```

```{r}
# Convert boston data to sf object
boston.sf <- boston %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326) %>%
  st_transform('ESRI:102286')  # MA State Plane (feet)

# Check it worked
head(boston.sf)
```

```{r}
class(boston.sf)  # Should show "sf" and "data.frame"
```

```{r}
# Load neighborhood boundaries
nhoods <- read_sf(here("data/BPDA_Neighborhood_Boundaries.geojson")) %>%
  st_transform('ESRI:102286')  # Match CRS!

# Check the neighborhoods
head(nhoods)
```

```{r}
nrow(nhoods)  # How many neighborhoods?
```

```{r}
# Spatial join: Assign each house to its neighborhood
boston.sf <- boston.sf %>%
  st_join(nhoods, join = st_intersects)

# Check results
boston.sf %>%
  st_drop_geometry() %>%
  count(`name`) %>%
  arrange(desc(n))
```

```{r}
#| eval: false
#| include: false
# Which neighborhoods are most expensive?
price_by_nhood %>%
  arrange(desc(median_price)) %>%
  head(5)
```

```{r}
#| eval: false
#| include: false
# Which have most sales?
price_by_nhood %>%
  arrange(desc(n_sales)) %>%
  head(5)
```

- Why do you think certain neighborhoods command higher prices? Proximity to downtown? Historical character? School quality? Safety? All of the above? This is why we need spatial features and neighborhood controls!

### Part 1: Expanding Toolkit

#### Categorical Variables

- Continuous Variables

  - Square footage
  
  - Age of house
  
  - Income levels
  
  - Distance to downtown
  
- Categorical Variables

  - Neighborhood
  
    - **n-1 Rule:** One neighborhood is chosen as the reference category (omitted). R picks the first alphabetically unless specified otherwise.
  
  - School district
  
  - Building type
  
  - Has garage
  
```{r}
# Ensure name is a factor
boston.sf <- boston.sf %>%
  mutate(name = as.factor(name))

# Check which is reference (first alphabetically)
levels(boston.sf$name)[1]
```

```{r}
# Fit model with neighborhood fixed effects
model_neighborhoods <- lm(SalePrice ~ LivingArea + name, 
                          data = boston.sf)

# Show just first 10 coefficients
summary(model_neighborhoods)$coef[1:10, ]
```

- Interperetation

  - **Reference Category:** Allston was automatically chosen as the intercept because it's the first alphabetically.
  
  - **Structural Variables**
  
    - **Living Area:** Each additional sq. ft. adds this amount (same for all neighborhoods).
    
    - **Bedrooms:** Effect of one or more full bathroom (same for all neighborhoods).
    
  - **Neighborhood Dummies**
  
    - **Positive Coefficient:** This neighborhood is *more* expensive than Allston.
    
    - **Negative Coefficient:** This neighborhood is *less* expensive than Allston.
    
    - All other variables constant and held equal, same size and same bathrooms.
    
      - House A in Back Bay
      
        - Living Area: 1,500 sq. ft.
        
        - Baths: 2
        
        - Neighborhood: Back Bay
        
        - Predicted Price: $8,458,873
        
      - House B in Roxbury
      
        - Living Area: 1,500 sq. ft.
        
        - Baths: 2
        
        - Neighborhood: Roxbury
        
        - Predicted Price: $311,066
        
      - Neighborhood Effect Price Difference: $8,127,807
        
        - Same house, different location, but huge price difference. This is what neighborhood dummies capture.
        
          - R automatically handles dummy variables as booleans, so 1 is true for Back Bay and 0 is false for Back Bay when creating neighborhood variables.
  
#### Interactions: When Relationships Depend

- **The Question:** Doess the effect of one variable *depend on* the level of another variable?

  - Example Scenarios
  
    - **Housing:** Does square footage matter more in wealthy neighborhoods?
    
    - **Education:** Do tutoring effects vary by initial skill level?
    
    - **Public Health:** Do pollution effects differ by age?
    
  - **Example for this study:** Is the value of square footage the same across all Boston neighborhoods?
  
    - SalePrice = β₀ + β₁(LivingArea) + β₂(WealthyNeighborhood) + β₃(LivingArea × WealthyNeighborhood) + ε
    
- **Theory: Luxury Premium Hypothesis**

  - **In Wealthy Neighborhoods (Back Bay, Beacon Hill, South End)**
  
    - High-end buyers pay premium for space.
    
    - Luxury finishes, location prestige.
    
    - Each sq. ft. adds substantial value.
    
    - Steep slope.
    
    - **Hypothesis:** $300+ per sq. ft.
    
  - **In Working-Class Neighborhoods (Dorchester, Mattapan, East Boston)**
  
    - Buyers value function over luxury.
    
    - More price-sensitive market.
    
    - Space matters, but less premium.
    
    - Flatter slope.
    
    - **Hypothesis:** $100-150 per sq. ft.
    
  - **Key Question:** If we assume one slope for all neighborhoods, are we misunderstanding the market?

```{r}
# Define wealthy neighborhoods based on median prices
wealthy_hoods <- c("Back Bay", "Beacon Hill", "South End", "Bay Village")

# Create binary indicator
boston.sf <- boston.sf %>%
  mutate(
    wealthy_neighborhood = ifelse(name %in% wealthy_hoods, "Wealthy", "Not Wealthy"),
    wealthy_neighborhood = as.factor(wealthy_neighborhood)
  )

# Check the split
boston.sf %>%
  st_drop_geometry() %>%
  count(wealthy_neighborhood)
```

Model 1: No Interaction (Parallel Slopes)

```{r}
# Model assumes same slope everywhere
model_no_interact <- lm(SalePrice ~ LivingArea + wealthy_neighborhood, 
                        data = boston.sf)

summary(model_no_interact)$coef
```

- Assumes living area has the same effect in all neighborhoods. Only the intercept differs (wealthy areas start higher). Parallel lines on a plot.

Model 2: With Interaction (Different Slopes)

```{r}
# Model allows different slopes
model_interact <- lm(SalePrice ~ LivingArea * wealthy_neighborhood, 
                     data = boston.sf)

summary(model_interact)$coef
```

- Allows living are to have different effects in different neighborhoods. Both intercept *and* slope differ. Non-parallel lines on a plot.

- We get the un-intuitive negative premium here because that is an intercept adjustment (applies at 0 sqft). The slope difference (+985sq/ft) is huge - we can calculate when wealthy areas become more expensive at what sq. ft. = 384 (358,542 / 985).

  - Not Wealthy Areas Equation: $Price = 358,542 + 96 × LivingArea$
  
    - Interpretation is at base price $358,542, each sq. ft. adds $96.
  
  - Wealthy Areas Equation: $Price = -19,395 + 1,081 × LivingArea$
  
    - Interpretation is at base price -$19,395, each sq. ft. adds $1,081.
    
  - The Interaction Effect. Wealthy areas value each sq ft $985 more than non-wealthy areas!

- Key Observation: The lines are NOT parallel when plotted - that’s the interaction!

Comparing Model Performance

```{r}
# Compare R-squared
cat("Model WITHOUT interaction R²:", round(summary(model_no_interact)$r.squared, 4), "\n")
```

```{r}
cat("Model WITH interaction R²:", round(summary(model_interact)$r.squared, 4), "\n")
```

```{r}
cat("Improvement:", round(summary(model_interact)$r.squared - summary(model_no_interact)$r.squared, 4), "\n")
```

- Model Improvement: Adding the interaction improves R² by 0.1635 (a 26.6% relative improvement)

  - Interpretation: We explain 16.35% more variation in prices by allowing different slopes!

#### Polynomial Terms: Non-Linear Relationships When Straight Lines Don't Fit

- Signs of Non-Linearity:

  - Curved residual plots.
  
  - Diminishing returns.
  
  - Accelerating effects.
  
  - U-shaped or inverted-U patterns.
  
  - Theoretical reasons.
  
- Examples:

  - House Age: Depreciation, then vintage premium.
  
  - Test Scores: Plateau after studying.
  
  - Advertising: Diminishing returns.
  
  - Crime Prevention: Early gains, then plateaus.
  
- Polynomial Regression

  - SalePrice = β₀ + β₁(Age) + β₂(Age²) + ε
  
    - Allows for *curved relationship*.
    
- Age's Non-Linear Effect

  - New Houses (0-20 Years)
  
    - Modern amenities.
    
    - Move-in ready.
    
    - No repairs needed.
    
    - **High value.**
    
    - Steep depreciation initially.
  
  - Middle-Aged (20-80 Years)
  
    - Needs updates.
    
    - Wear and tear.
    
    - Not yet "historic".
    
    - **Lowest value.**
    
    - Trough of the curve.
  
  - Historic/Vintage (80+ Years)
  
    - Architectural character.
    
    - Historic districts.
    
    - Prestige value.
    
    - **Rising value.**
    
    - "Vintage premium".
    
- Boston's Context: The city has LOTS of historic homes (Back Bay, Beacon Hill built 1850s-1900s). Does age create a U-shaped curve?

```{r}
# Calculate age from year built
boston.sf <- boston.sf %>%
  mutate(Age = 2025 - YR_BUILT)%>% filter(Age <2000)


# Check the distribution of age
summary(boston.sf$Age)
```

```{r}
# Visualize age distribution
ggplot(boston.sf, aes(x = Age)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white", alpha = 0.7) +
  labs(title = "Distribution of House Age in Boston",
       x = "Age (years)",
       y = "Count") +
  theme_minimal()
```

Linear Model (Baseline)

```{r}
# Simple linear relationship
model_age_linear <- lm(SalePrice ~ Age + LivingArea, data = boston.sf)

summary(model_age_linear)$coef
```

- Each additional year of age changes price by $2834.01 (assumed constant rate).

Add Polynomial Term: Age Squared

```{r}
# Quadratic model (Age²)
model_age_quad <- lm(SalePrice ~ Age + I(Age^2) + LivingArea, data = boston.sf)

summary(model_age_quad)$coef
```

- Important: The I() Function Why I(Age^2) instead of just Age^2? In R formulas, ^ has special meaning. I() tells R: “interpret this literally, compute Age²” Without I(): R would interpret it differently in the formula.

- Model Equation: $Price = 570,397 + - 13,008 × Age + 80 × Age² + 204 × LivingArea$

  - Warning: Can't interpret coefficients directly. With Age², the effect of age is no longer constant. You need to calculate the marginal effect. Marginal effect of $Age = β₁ + 2 × β₂ × Age$ This means the effect changes at every age!

Compare Models

```{r}
# R-squared comparison
r2_linear <- summary(model_age_linear)$r.squared
r2_quad <- summary(model_age_quad)$r.squared

cat("Linear model R²:", round(r2_linear, 4), "\n")
```

```{r}
cat("Quadratic model R²:", round(r2_quad, 4), "\n")
```

```{r}
cat("Improvement:", round(r2_quad - r2_linear, 4), "\n\n")
```

```{r}
# F-test: Is the Age² term significant?
anova(model_age_linear, model_age_quad)
```

Check Residual Plot

```{r}
# Compare residual plots
par(mfrow = c(1, 2))

# Linear model residuals
plot(fitted(model_age_linear), residuals(model_age_linear),
     main = "Linear Model Residuals",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

# Quadratic model residuals  
plot(fitted(model_age_quad), residuals(model_age_quad),
     main = "Quadratic Model Residuals",
     xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)
```

### Part 2: Why Space Matters

#### Hedonic Model Framework

#### Tobler's First Law

*“Everything is related to everything else, but near things are more related than distant things.”*

*- Waldo Tobler, 1970*

  - For house prices, crime nearby matters more than crime across the city.
  
  - Parks within walking distance affect value.
  
  - Immediate neighborhood defines market.

#### Spatial Autocorrelation

### Part 3: Creating Spatial Features

#### Buffer Aggregation

- **Count or sum** events within a defined distance.

  - Example: Number of crimes within 500'.

```{r}
neighborhood_boundaries <- st_read("data/BPDA_Neighborhood_Boundaries.geojson")

crimes.sf <- read_csv("data/bostonCrimes.csv")
crimes.sf <- crimes.sf %>%
  na.omit()

neighborhood_boundaries <- st_transform(neighborhood_boundaries, 102286)
boston.sf <- st_transform(boston.sf, 102286)
crimes.sf <- st_as_sf(crimes.sf, coords = c("Long", "Lat"), crs = 4326)
crimes.sf <- st_transform(crimes.sf, 102286)
```

```{r}
# Create buffer features - these will work now that CRS is correct
boston.sf <- boston.sf %>%
  mutate(
    crimes.Buffer = lengths(st_intersects(
      st_buffer(geometry, 660),
      crimes.sf
    )),
    crimes_500ft = lengths(st_intersects(
      st_buffer(geometry, 500),
      crimes.sf
    ))
  )
```

#### K-Nearest Neighbors

- Average distance to k closest events.

  - Example: **Average distance** to 3 nearest violent crimes.

```{r}
# Calculate distance matrix (houses to crimes)
dist_matrix <- st_distance(boston.sf, crimes.sf)

# Function to get mean distance to k nearest neighbors
get_knn_distance <- function(dist_matrix, k) {
  apply(dist_matrix, 1, function(distances) {
    # Sort and take first k, then average
    mean(as.numeric(sort(distances)[1:k]))
  })
}

# Create multiple kNN features
boston.sf <- boston.sf %>%
  mutate(
    crime_nn1 = get_knn_distance(dist_matrix, k = 1),
    crime_nn3 = get_knn_distance(dist_matrix, k = 3),
    crime_nn5 = get_knn_distance(dist_matrix, k = 5)
  )

# Check results
summary(boston.sf %>% st_drop_geometry() %>% select(starts_with("crime_nn")))
```

- crime_nn3 = 83.29 means the average distance to the 3 nearest crimes is 83.29 feet.

```{r}
# Which k value correlates most with price?
boston.sf %>%
  st_drop_geometry() %>%
  select(SalePrice, crime_nn1, crime_nn3, crime_nn5) %>%
  cor(use = "complete.obs") %>%
  as.data.frame() %>%
  select(SalePrice)
```

- Finding the kNN feature with the strongest correlation tells us the relevant “zone of influence” for crime perception!

#### Distance to Specific Points

- **Straight-line (Euclidian) distance** to important locations.

  - Example: Distance to downtown, nearest T-Station.
  
```{r}
# Define downtown Boston (Boston Common: 42.3551° N, 71.0656° W)
downtown <- st_sfc(st_point(c(-71.0656, 42.3551)), crs = "EPSG:4326") %>%
  st_transform('ESRI:102286')

# Calculate distance from each house to downtown
boston.sf <- boston.sf %>%
  mutate(
    dist_downtown_ft = as.numeric(st_distance(geometry, downtown)),
    dist_downtown_mi = dist_downtown_ft / 5280
  )

# Summary
summary(boston.sf$dist_downtown_mi)
```

```{r}
# Summary of all spatial features created
spatial_summary <- boston.sf %>%
  st_drop_geometry() %>%
  select(crimes.Buffer, crimes_500ft, crime_nn3, dist_downtown_mi) %>%
  summary()

spatial_summary
```

- crimes.Buffer (660ft) is a buffer count measuring number of crimes near a house.

- crimes_500ft is a buffer county measuring crimes within 500ft.

- crime_nn3 is a kNN distance measuring average distance to 3 nearest crimes in feet.

- dist_downtown_mi is a point distance measuring miles from downtown boston.

```{r}
boston.sf <- boston.sf %>%
  mutate(Age = 2015 - YR_BUILT)  

# Model 1: Structural only
model_structural <- lm(SalePrice ~ LivingArea + R_BDRMS + Age, 
                       data = boston.sf)

# Model 2: Add spatial features
model_spatial <- lm(SalePrice ~ LivingArea + R_BDRMS + Age +
                    crimes_500ft + crime_nn3 + dist_downtown_mi,
                    data = boston.sf)

# Compare
cat("Structural R²:", round(summary(model_structural)$r.squared, 4), "\n")
```

```{r}
cat("With spatial R²:", round(summary(model_spatial)$r.squared, 4), "\n")
```

```{r}
cat("Improvement:", round(summary(model_spatial)$r.squared - 
                          summary(model_structural)$r.squared, 4), "\n")
```

### Part 4: Fixed Effects

- Categorical variables that capture all unmeasured characteristics of a group.

- In hedonic models:

  - Each neighborhood gets its own dummy variable.
  
  - Captures everything unique about that neighborhood we didn't explicitly measure.
  
    - Technically done when went over categorical data.
    
    - What's captured:
    
      - School quality.
      
      - Prestige or reputation.
      
      - Walkability.
      
      - Access to jobs.
      
      - Cultural amenities.
      
      - **Things we can't easily measure.**
    
```{r}
#| eval: false
#| include: false
# Add neighborhood fixed effects
reg5 <- lm(
  SalePrice ~ LivingArea + Age + 
              crimes_500ft + 
              parks_nn3 + 
              as.factor(name),  # FE
  data = boston.sf
)
```

```{r}
# Behind the scenes, R creates dummies:
# is_BackBay = 1 if Back Bay, 0 otherwise
# is_Beacon = 1 if Beacon Hill, 0 otherwise
# is_Allston = 1 if Allston, 0 otherwise
# ... (R drops one as reference category)
```

Each coefficient is equal to the price premium/discount for that neighborhood (holding all else constant).

- Why use fixed effects?

  - Dramatically improves prediction.
  
    - In current data it's because neighborhoods bundle many unmeasured factors like school districts, job access, amenities, and "cool factor".
    
  - Coefficients change.
  
    - Crime coefficient without FE: -$125/crime
    
    - Crime coefficient with FE: -$85/crime
    
      - Without FE: Captured confounders too.
      
      - With FE: Neighborhoods "absorb" other differences.
      
      - Now just the crime effect.
      
    - Trade-Off: FEs are powerful, but they're a **black box**, we don't know **why** Back Bay commands a premium.
    
Compare All Models

```{r}
#| eval: false
#| include: false
# Model 3: Structural Only
reg3 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH, 
           data = boston.sf)

# Model 4: Add Spatial Features  
reg4 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +
                       crimes_500ft + crime_nn3+ dist_downtown_mi,
           data = boston.sf)

boston.sf <- boston.sf %>%
  st_join(nhoods, join = st_intersects)

# Model 5: Add Fixed Effects
reg5 <- lm(SalePrice ~ LivingArea + Age + R_FULL_BTH +
                       crimes_500ft + crime_nn3+ dist_downtown_mi +
                       as.factor(name),
           data = boston.sf)
library(stargazer)
# Compare in-sample fit
stargazer(reg3, reg4, reg5, type = "text")
```

### Part 5: Cross-Validation with Categorical Variables

#### CV Recap From Last Week

- Three common validation approaches:

  - **Train/Test Split:** 80/20 training/testing split, simple but unstable.
  
  - **k-Fold Cross-Validation:** Split into k-folds, train on k-1, test on 1, repeat.
  
  - **LOOCV:** Leave one observation out at a time (special case of k-fold).
  
Use k-fold CV to compare hedonic models.

  - CV tells us how well model predicts new data.
  
  - More honest than in-sample R^2.
  
  - Helps detect overfitting.

```{r}
#| eval: false
#| include: false
library(caret)

ctrl <- trainControl(
  method = "cv",
  number = 10  # 10-fold CV
)

model_cv <- train(
  SalePrice ~ LivingArea + Age,
  data = boston.sf,
  method = "lm",
  trControl = ctrl
)
```

```{r}
#| eval: false
#| include: false
library(caret)
ctrl <- trainControl(method = "cv", number = 10)

# Model 1: Structural
cv_m1 <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH,
  data = boston.sf, method = "lm", trControl = ctrl
)

# Model 2: + Spatial
cv_m2 <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,
  data = boston.sf, method = "lm", trControl = ctrl
)

# Model 3: + Fixed Effects (BUT WAIT - there's a (potential) problem!)
cv_m3 <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + 
              as.factor(name),
  data = boston.sf, method = "lm", trControl = ctrl
)

# Compare
data.frame(
  Model = c("Structural", "Spatial", "Fixed Effects"),
  RMSE = c(cv_m1$results$RMSE, cv_m2$results$RMSE, cv_m3$results$RMSE)
)
```

```{r}
#| eval: false
#| include: false
# ALWAYS run this before CV with categorical variables
category_check %>%
  st_drop_geometry() %>%
  count(name) %>%
  arrange(n)

print(category_check)
```

**Rule of Thumb:** Categories with n < 10 will likely cause CV problems.

- Solution: Group small neighborhoods.

```{r}
#| eval: false
#| include: false
# Step 1: Add count column
boston.sf <- boston.sf %>%
  add_count(name)

# Step 2: Group small neighborhoods
boston.sf <- boston.sf %>%
  mutate(
    name_cv = if_else(
      n < 10,                       # If fewer than 10 sales
      "Small_Neighborhoods",        # Group them
      as.character(name)            # Keep original
    ),
    name_cv = as.factor(name_cv)
  )

# Step 3: Use grouped version in CV
cv_model_fe <- train(
  SalePrice ~ LivingArea + Age + crimes_500ft + 
              as.factor(name_cv),   # Use name_cv, not name!
  data = boston.sf,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)
```

  - Trade-Off: Lose granularity for small neighborhoods, but avoid CV crashes.

- Alternative: Drop sparse categories.

```{r}
#| eval: false
#| include: false
# Remove neighborhoods with < 10 sales
neighborhood_counts %
  st_drop_geometry() %>%
  count(name)

keep_neighborhoods %
  filter(n >= 10) %>%
  pull(name)

boston_filtered %
  filter(name %in% keep_neighborhoods)

cat("Removed", nrow(boston.sf) - nrow(boston_filtered), "observations")
```

  - Warning: Consider carefully, which neighborhoods are you excluding? Often those with less data are marginalized communities. Document what you removed and why.

RECOMMMENDED WORKFLOW

```{r}
#| eval: false
#| include: false
# 1. Check category sizes
boston.sf %>%
  st_drop_geometry() %>%
  count(name) %>%
  arrange(n) %>%
  print()
```

```{r}
#| eval: false
#| include: false
# 2. Group if needed
boston.sf <- boston.sf %>%
  add_count(name) %>%
  mutate(
    name_cv = if_else(n < 10, "Small_Neighborhoods", as.character(name)),
    name_cv = as.factor(name_cv)
  )

# 3. Set up CV
ctrl <- trainControl(method = "cv", number = 10)

# 4. Use grouped neighborhoods in ALL models with FE
model <- train(
  SalePrice ~ LivingArea + Age + crimes_500ft + as.factor(name_cv),
  data = boston.sf,
  method = "lm",
  trControl = ctrl
)

# 5. Report
cat("10-fold CV RMSE:", round(model$results$RMSE, 0), "\n")
```

```{r}
#| eval: false
#| include: false
# Full model comparison with CV.
library(caret)

# Prep data
boston.sf <- boston.sf %>%
  add_count(name) %>%
  mutate(name_cv = if_else(n < 10, "Small_Neighborhoods", as.character(name)),
         name_cv = as.factor(name_cv))

ctrl <- trainControl(method = "cv", number = 10)

# Compare models
cv_structural <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH,
  data = boston.sf, method = "lm", trControl = ctrl
)

cv_spatial <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3,
  data = boston.sf, method = "lm", trControl = ctrl
)

cv_fixedeffects <- train(
  SalePrice ~ LivingArea + Age + R_FULL_BTH + crimes_500ft + crime_nn3 + 
              as.factor(name_cv),
  data = boston.sf, method = "lm", trControl = ctrl
)

# Results
data.frame(
  Model = c("Structural", "+ Spatial", "+ Fixed Effects"),
  RMSE = c(cv_structural$results$RMSE, 
           cv_spatial$results$RMSE, 
           cv_fixedeffects$results$RMSE)
)
```

- Note: These values are kind of ginormous - remember RMSE squares big errors, so outliers can have a really large impact.

- Key Insight: Each layer improves out-of-sample prediction, with fixed effects providing the biggest boost.

- Why? Neighborhoods bundle many unmeasured factors (schools, amenities, prestige) that we can’t easily quantify individually.

- Look for: - Prices over $2-3 million (could be luxury condos or errors) - Prices near 0 (data errors) - Long right tail in histogram

  - Log Transform the skewed dependent variable.
  
    - Interpreting log models:
    
      - RMSE is now in log-dollars (hard to interpret).
      
      - To convert back: exp(predictions) gives actual dollars.
      
      - Coefficients now represent percentage changes, not dollar changes.
      
      - This is standard practice in hedonic modeling!

## Coding Techniques

- sum(is.na())

- glimpse()

- summary()

- st_drop_geometry()

## Questions & Challenges

- Do *NOT* use interactions when:

  - There are small samples, sufficient data is needed in each group.
  
  - There are already too many interactions, it makes models unstable and causes overfitting.
  
- How do we quantify "nearness" in a way a regression model can use?

  - Must creaate spaatial features that measure proximity to amenities/disamenities.
  
- When CV fails with categorical variables, the problem tends to be sparse categories.

  - In example:
  
    - Random split created 10 folds.
    
    - All "West End" sales ended up in one fold, the test fold.
    
    - Training folds never saw "West End".
    
    - Model can't predict for a category it never learned.
    
  - Issue: When neighborhoods have very few sales (<10), random CV splits can put all instances in the saame fold, breaking the model.

## Connections to Policy

- Boston's Housing Market

  - **Market Segmentation:** Boston operates as *two* distinct housing markets.
  
    - Luxury Market: Every sq. ft. is premium at $1,081/sq. ft.
    
    - Standard Market: Space is valued, but lower premium at $96 / sq. ft.
    
  - **Affordability Crisis:** The interaction amplifies inequality.
  
    - Large homes in wealthy areas become exponentially more expensive.
    
    - Creates barriers to mobility between neighborhoods.
    
  - **Policy Design:** One-size-fits-all policies likely will fail.
  
    - Property tax assessments should account for neighborhood-specific valuation.
    
    - Housing assistance needs vary dramatically by area.

## Reflection

Tips for Success:

- Do:

  - Start simple, add complexity.
  
  - Check for NAs: sum(is.na()).
  
  - Test on small subset first.
  
  - Commment code.
  
  - Check coefficient signs.
  
  - Use glimpse() and summary().
  
- Don't:

  - Add 50 variables at once.
  
  - Ignore errors.
  
  - Forget st_drop_geometry() for non-spatial operations.
  
  - Skip sparse category check.
  
- Common Errors:

  - "Factor has new levels" -> Group sparse categories.
  
  - "Computationally singular" -> Remove collinear variables.
  
  - "Very high RMSE" -> Check outliers, scale.
  
  - "CV takes forever" -> Simplify model or reduce folds.
  
  - "Negative R^2" -> Model worse than mean, rethink variables.