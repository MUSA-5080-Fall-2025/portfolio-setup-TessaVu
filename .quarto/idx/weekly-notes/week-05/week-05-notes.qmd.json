{"title":"PREDICTIVE MODELING WITH LINEAR REGRESSION","markdown":{"yaml":{"title":"PREDICTIVE MODELING WITH LINEAR REGRESSION","subtitle":"WEEK 5 NOTES","date":"2025-10-06","author":[{"name":"Tess Vu","email":["tessavu@proton.me","tessavu@upenn.edu"],"corresponding":true}],"affiliation":[{"name":"University of Pennsylvania","department":"Urban Spatial Analytics (MUSA)","city":"Philadelphia","state":"PA","url":"https://www.design.upenn.edu/urban-spatial-analytics"}],"format":{"html":{"code-fold":"show","toc":true,"toc-location":"left","toc-expand":true,"smooth-scroll":true,"embed-resources":true,"title-block-style":"default"}},"execute":{"warning":false,"message":false}},"headingText":"Key Concepts Learned","containsRefs":false,"markdown":"\n\n\n1.  STATISTICAL LEARNING FRAMEWORK\n\n    -   **Statistical Learning:** Set of approaches for estimating relationships.\n\n    -   Formalizing relationships, where:\n\n        -   *Y = f(X) + ε*\n\n        -   **f:** The systematic information X provides about Y.\n\n        -   **ε:** Random error (irreducible).\n\n    -   **f represents the TRUE relationship between predictors and outcome.**\n\n        -   It's **FIXED** but **UNKNOWN**.\n\n        -   It's what people try to estimate.\n\n        -   Different X values produce different Y values through f.\n\n    -   Two reasons to estimate f:\n\n        1.  **Prediction**\n\n            -   Estimate Y for new observations.\n\n            -   Don't necessarily care about the exact form of f.\n\n            -   **Focus is on accuracy of predictions.**\n\n        2.  **Inference**\n\n            -   Understand how X affects Y.\n\n            -   *Which predictions matter?*\n\n            -   *What is the nature of the relationship?*\n\n            -   **Focus is on interpreting the model.**\n\n    -   How to estimate f? With two broad approaches:\n\n        -   **Parametric Methods**\n\n            -   Make an assumption about functional form (e.g. linear).\n\n            -   Reduces problem to estimating a few parameters.\n\n            -   Easier to interpret.\n\n            -   More common.\n\n        -   **Non-Parametric Methods**\n\n            -   Don't assume a specific form.\n\n            -   More flexible.\n\n            -   Requires more data.\n\n            -   Harder to interpret.\n\n        -   **KEY DIFFERENCE:** In *parametric* we assume f is linear, then estimate β₀ and β₁, etc. In *non-parametric* we let the data determine the shape of f.\n\n            -   **Deep Learning:** Neural networks are technically *parametric* (millions of parameters), but achieve flexibility through parameter quantity rather than assuming a rigid form.\n\n        -   **Linear Regression: Parametric**\n\n            -   **Assumption:** Relationship between X and Y is linear.\n\n            -   **Task:** Estimate the β coefficients using sample data.\n\n            -   **Method:** Ordinary Least Squares (OLS).\n\n            -   **Advantages:**\n\n                -   Simple and interpretable.\n\n                -   Well-understood properties.\n\n                -   Works remarkably well for many problems.\n\n                -   Foundation for more complex methods.\n\n            -   **Disadvantages:**\n\n                -   Assumes linearity.\n\n                -   Sensitive to outliers.\n\n                -   Makes several assumptions.\n\n2.  TWO GOALS\n\n-   Understanding Relationships vs. Making Predictions: Same model serves different purposes.\n\n    -   **Inference**\n\n        -   \"Does education affect income?\"\n\n        -   Focus on coefficients.\n\n        -   Statistical significance matters.\n\n        -   Understand mechanisms and what the coefficients, or predictors, tell us.\n\n    -   **Prediction**\n\n        -   \"What's county Y's income?\"\n\n        -   Focus on accuracy.\n\n        -   Prediction intervals matter.\n\n        -   Don't need to understand why certain relationships exist.\n\n3.  MODEL BUILDING\n\n-   Considerable scatter can generally mean the relationships aren't deterministic, but rather more probablistic or stochastic.\n\n-   Interpreting coefficients example:\n\n    -   Intercept (β₀) = \\$62,855\n\n        -   Expected income when population = 0.\n\n        -   Not usually meaningful in practice.\n\n    -   Slope (β₁) = \\$0.02\n\n        -   For each additional person, income increases by \\$0.02.\n\n        -   This is more useful, because for every 1,000 people, income increases by \\$20.\n\n    -   p-value \\<0.001 means it's very unlikely to see this if true, when β₁ = 0.\n\n    -   Can reject the null hypothesis.\n\n-   **Holy Grail Concept:** Estimates are just estimates of the true unknown parameters.\n\n-   Different samples produce different linear regression lines, but no matter how many samples and statistical tests, the true relationship is unknowable.\n\n-   Standard errors quantify the above uncertainty.\n\n-   Statistical significance:\n\n    1.  **Null Hypothesis (H₀):** β₁ = 0 (no relationship).\n\n    2.  **Our Estimate:** β₁ = 0.02.\n\n    3.  **Question:** Could we get \\$0.02 just by chance if H₀ is true?\n\n    -   **t-statistic:** How many standard errors away from 0?\n\n        -   **Bigger \\|t\\| means more confidence that the relationship is real.**\n\n    -   **p-value:** Probability of seeing our estimate if H₀ is true.\n\n        -   **Small p, reject H₀, conclude relationship exists.**\n\n        -   **Large p, fail to reject H₀, conclude that there's a possibility no relationship exists.**\n\n4.  MODEL EVALUATION\n\n-   Two key questions:\n\n    1.  **How well does the data used fit?** (in-sample fit)\n\n    2.  **How well would it predict new data?** (out-of-sample performance)\n\n    -   NOT THE SAME.\n\n    -   **In-Simple Fit: R\\^2**\n\n        -   R\\^2 = 0.208 means \"21% of variation in income is explained by population.\"\n\n        -   **Is this good? It depends on the goal. For prediction, it's moderately good. For inference, it shows population matters, but other factors exist.**\n\n        -   **R\\^2 alone doesn't tell if the model is trustworthy.**\n\n    -   **Overfitting Problem**\n\n        1.  **Underfitting:** Model is too simple (high bias).\n\n        2.  **Good Fit:** Captures pattern without noise.\n\n        3.  **Overfitting:** Memorizes training data (high variance).\n\n        -   **DANGER: High R\\^2 doesn't mean good predictions!**\n\n        -   Overfit regression means it can't be applied to other samples and is too aligned with the data samples it was tested and derived from.\n\n        -   70% training, so fit on training data only. 30% testing, so predict on test data after getting the fit on the training data.\n\n    -   **RMSE:** A number of 9,536 means that on new data (test set), predictions are off by \\~\\$9,500 on average. Is this level of error acceptable for policy decisions?\n\n    -   **Cross-Validation:** A better approach doing multiple training and testing splits that gives more stable estimate of true prediction performance. Average the multiple RMSEs.\n\n5.  CHECKING ASSUMPTIONS\n\n-   Linear regression makes assumptions, if violated:\n\n    -   Coefficients may be biased.\n\n    -   Standard errors are wrong.\n\n    -   Predictions unreliable.\n\n    -   **Check diagnostics before trusting any model.**\n\n-   Assumption 1: Linearity.\n\n    -   Assume relationship is linear.\n\n    -   Check with residual plot.\n\n        -   Good plot will have random scatter, points around 0, and constant spread.\n\n        -   Bad plot will have curved pattern (parabolic relationship), model is missing something, predictions are biased.\n\n        -   Linearity violations hurt predictions, not just inference:\n\n            -   If the true relationship is curved and a straight line is fitted, there will be a systematic underprediction in some regions and overprediction in otherss.\n\n            -   Biased predictions in predictable ways (not random errors).\n\n            -   Residual plots should show random scatter, any pattern means the model is missing something systematic.\n\n-   Assumption 2: Constant Variance.\n\n    -   **Heteroskedasticity:** Variance changes across X.\n\n    -   **Impact:** Standard errors are wrong, so p-values are misleading.\n\n    -   Heteroskedasticity is often a sympton of model misspecification.\n\n        -   Model fits well for some values, but poorly for other values (aggregation matters).\n\n        -   May indicate missing variables that matter more at certain X values.\n\n        -   Ask what's different about observations with large residuals.\n\n        -   Example: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\n    -   Key Insight: Large counties vary widely in income because some have high education and others have low education. Adding education as a predictor accounts for variation.\n\n    -   Key Insight: Adding the right predictor can fix heteroskedasticity.\n\n-   Formal Test: Breusch-Pagan\n\n    -   p \\> 0.05 means constant variance assumption is okay.\n\n    -   p \\< 0.05 means there's evidence of heteroskedasticity.\n\n    -   Solutions for heteroskedasticity:\n\n        1.  Transform Y (like log(income)).\n\n        2.  Robust standard errors.\n\n        3.  Add missing variables.\n\n        4.  Accept it (point predictions are still okay for prediction goals).\n\n-   Assumption 3: Normality of Residuals.\n\n    -   Assume residuals are normally distributed.\n\n    -   Matters because it's less critical for point predictions (unbiased regardless). It's important for confidence intervals and prediction intervals. It's needed for valid hypothesis tests (t-tests, F-tests).\n\n    -   Q-Q (Quantile-Quantile) plot of residuals to check.\n\n        -   Theoretical Quantiles on x and Sample Quantiles on y.\n\n-   Assumption 4: No Multicollinearity.\n\n    -   For multiple regression, predictors shouldn't be too correlated.\n\n    -   It matters because coefficients become unstable, and difficult to interpret.\n\n-   Assumption 5: No Influential Outliers.\n\n    -   **Not all outliers are problems, only those with high leverage AND large residuals.**\n\n        -   Pulls regression line.\n\n    -   Dealing with influential points:\n\n        1.  **Investigate** why this observation is unusual. Is it a data error or truly unique?\n\n        2.  **Report** the influential observations in analysis.\n\n        3.  **Sensitivity check.** Refit the model without the outlier(s), do the conclusions change?\n\n        4.  **Don't automatically remove outliers,** they might represent real, important cases.\n\n        For policy, an influential county might need special attention, *NOT* exclusion.\n\n6.  IMPROVING PREDICTIONS\n\n-   If a relationship is curved, try log transformations.\n\n    -   Log models show percentage relationships.\n\n-   Categorical variables, R creates dummy variables automatically.\n\nSUMMARY OF REGRESSION WORKFLOW\n\n```         \n1. **Understand the framework.** What's f? What's the goal?\n\n2. **Visualize first.** Does a linear model make sense?\n\n3. **Fit the model.** Estimate coefficients.\n\n4. **Evaluate performance.** Train/test split, cross-validation.\n\n5. **Check assumptions.** Residual plots, VIF, outliers.\n\n6. **Improve if needed.** Transformations, more variables.\n\n7. **Consider ethics.** Who could be harmed by this model?\n```\n\n## Coding Techniques\n\n-   **Train/Test Split**\n\n```{r}\n#| eval: false\n#| include: false\n\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n\n-   **Evaluate Predictions**\n\n```{r}\n#| eval: false\n#| include: false\n# Calculate prediction error (RMSE)\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\n\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n\n-   **Cross-Validation**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n```\n\n-   **Residual Plot**\n\n```{r}\n#| eval: false\n#| include: false\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n\n-   **Breusch-Pagan Formal Test**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(lmtest)\nbptest(model1)\n```\n\n-   **Multicollinearity**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n\n-   **Log Transformations**\n\n```{r}\n#| eval: false\n#| include: false\n# Compare linear vs log\nmodel_linear <- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n```\n\n-   **Categorical Variables**\n\n```{r}\n#| eval: false\n#| include: false\n# Create metro/non-metro indicator\npa_data <- pa_data %>%\n  mutate(metro = ifelse(total_popE > 500000, 1, 0))\n\nmodel3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n```\n\n## Questions & Challenges\n\n-   Public policy is very much a balancing act between technically and statistically good models vs. actually applying them to real-world policy without the regression causing any ethical consequences.\n\n## Connections to Policy\n\n-   Real-life healthcare algorithm model discriminated despite being technically good with good R\\^2 and low prediction error with a good fit. Ethically this ended up amplifying existing discrimination. **A model can be statistically \"good\" while being ethically terrible for decision-making.**\n\n-   Influential points, or outliers, have connections to algorithmic bias. High-influence observations could represent marginalized communities or unique populations. Removing them can erase important populations from analysis and lead to biased policy decisions.\n\n-   **ALWAYS INVESTIGATE OUTLIERS BEFORE REMOVING.**\n\n## Reflection\n\nKEY TAKEAWAYS\n\n-   Statistical Learning:\n\n    -   Estimating f(X), the systematic relationship.\n\n    -   Parametric methods assume a form (most choose linear).\n\n-   Two Purposes:\n\n    -   *Inference:* Understand relationships.\n\n    -   *Prediction:* Forecast new values.\n\n-   Model Evaluation:\n\n    -   In-sample fit is NOT equivalent to out-of-sample performance.\n\n    -   Beware of overfitting!\n\n-   Diagnostics Matter:\n\n    -   Always check assumptions.\n\n    -   Plots reveal what R\\^2 hides.\n","srcMarkdownNoYaml":"\n\n## Key Concepts Learned\n\n1.  STATISTICAL LEARNING FRAMEWORK\n\n    -   **Statistical Learning:** Set of approaches for estimating relationships.\n\n    -   Formalizing relationships, where:\n\n        -   *Y = f(X) + ε*\n\n        -   **f:** The systematic information X provides about Y.\n\n        -   **ε:** Random error (irreducible).\n\n    -   **f represents the TRUE relationship between predictors and outcome.**\n\n        -   It's **FIXED** but **UNKNOWN**.\n\n        -   It's what people try to estimate.\n\n        -   Different X values produce different Y values through f.\n\n    -   Two reasons to estimate f:\n\n        1.  **Prediction**\n\n            -   Estimate Y for new observations.\n\n            -   Don't necessarily care about the exact form of f.\n\n            -   **Focus is on accuracy of predictions.**\n\n        2.  **Inference**\n\n            -   Understand how X affects Y.\n\n            -   *Which predictions matter?*\n\n            -   *What is the nature of the relationship?*\n\n            -   **Focus is on interpreting the model.**\n\n    -   How to estimate f? With two broad approaches:\n\n        -   **Parametric Methods**\n\n            -   Make an assumption about functional form (e.g. linear).\n\n            -   Reduces problem to estimating a few parameters.\n\n            -   Easier to interpret.\n\n            -   More common.\n\n        -   **Non-Parametric Methods**\n\n            -   Don't assume a specific form.\n\n            -   More flexible.\n\n            -   Requires more data.\n\n            -   Harder to interpret.\n\n        -   **KEY DIFFERENCE:** In *parametric* we assume f is linear, then estimate β₀ and β₁, etc. In *non-parametric* we let the data determine the shape of f.\n\n            -   **Deep Learning:** Neural networks are technically *parametric* (millions of parameters), but achieve flexibility through parameter quantity rather than assuming a rigid form.\n\n        -   **Linear Regression: Parametric**\n\n            -   **Assumption:** Relationship between X and Y is linear.\n\n            -   **Task:** Estimate the β coefficients using sample data.\n\n            -   **Method:** Ordinary Least Squares (OLS).\n\n            -   **Advantages:**\n\n                -   Simple and interpretable.\n\n                -   Well-understood properties.\n\n                -   Works remarkably well for many problems.\n\n                -   Foundation for more complex methods.\n\n            -   **Disadvantages:**\n\n                -   Assumes linearity.\n\n                -   Sensitive to outliers.\n\n                -   Makes several assumptions.\n\n2.  TWO GOALS\n\n-   Understanding Relationships vs. Making Predictions: Same model serves different purposes.\n\n    -   **Inference**\n\n        -   \"Does education affect income?\"\n\n        -   Focus on coefficients.\n\n        -   Statistical significance matters.\n\n        -   Understand mechanisms and what the coefficients, or predictors, tell us.\n\n    -   **Prediction**\n\n        -   \"What's county Y's income?\"\n\n        -   Focus on accuracy.\n\n        -   Prediction intervals matter.\n\n        -   Don't need to understand why certain relationships exist.\n\n3.  MODEL BUILDING\n\n-   Considerable scatter can generally mean the relationships aren't deterministic, but rather more probablistic or stochastic.\n\n-   Interpreting coefficients example:\n\n    -   Intercept (β₀) = \\$62,855\n\n        -   Expected income when population = 0.\n\n        -   Not usually meaningful in practice.\n\n    -   Slope (β₁) = \\$0.02\n\n        -   For each additional person, income increases by \\$0.02.\n\n        -   This is more useful, because for every 1,000 people, income increases by \\$20.\n\n    -   p-value \\<0.001 means it's very unlikely to see this if true, when β₁ = 0.\n\n    -   Can reject the null hypothesis.\n\n-   **Holy Grail Concept:** Estimates are just estimates of the true unknown parameters.\n\n-   Different samples produce different linear regression lines, but no matter how many samples and statistical tests, the true relationship is unknowable.\n\n-   Standard errors quantify the above uncertainty.\n\n-   Statistical significance:\n\n    1.  **Null Hypothesis (H₀):** β₁ = 0 (no relationship).\n\n    2.  **Our Estimate:** β₁ = 0.02.\n\n    3.  **Question:** Could we get \\$0.02 just by chance if H₀ is true?\n\n    -   **t-statistic:** How many standard errors away from 0?\n\n        -   **Bigger \\|t\\| means more confidence that the relationship is real.**\n\n    -   **p-value:** Probability of seeing our estimate if H₀ is true.\n\n        -   **Small p, reject H₀, conclude relationship exists.**\n\n        -   **Large p, fail to reject H₀, conclude that there's a possibility no relationship exists.**\n\n4.  MODEL EVALUATION\n\n-   Two key questions:\n\n    1.  **How well does the data used fit?** (in-sample fit)\n\n    2.  **How well would it predict new data?** (out-of-sample performance)\n\n    -   NOT THE SAME.\n\n    -   **In-Simple Fit: R\\^2**\n\n        -   R\\^2 = 0.208 means \"21% of variation in income is explained by population.\"\n\n        -   **Is this good? It depends on the goal. For prediction, it's moderately good. For inference, it shows population matters, but other factors exist.**\n\n        -   **R\\^2 alone doesn't tell if the model is trustworthy.**\n\n    -   **Overfitting Problem**\n\n        1.  **Underfitting:** Model is too simple (high bias).\n\n        2.  **Good Fit:** Captures pattern without noise.\n\n        3.  **Overfitting:** Memorizes training data (high variance).\n\n        -   **DANGER: High R\\^2 doesn't mean good predictions!**\n\n        -   Overfit regression means it can't be applied to other samples and is too aligned with the data samples it was tested and derived from.\n\n        -   70% training, so fit on training data only. 30% testing, so predict on test data after getting the fit on the training data.\n\n    -   **RMSE:** A number of 9,536 means that on new data (test set), predictions are off by \\~\\$9,500 on average. Is this level of error acceptable for policy decisions?\n\n    -   **Cross-Validation:** A better approach doing multiple training and testing splits that gives more stable estimate of true prediction performance. Average the multiple RMSEs.\n\n5.  CHECKING ASSUMPTIONS\n\n-   Linear regression makes assumptions, if violated:\n\n    -   Coefficients may be biased.\n\n    -   Standard errors are wrong.\n\n    -   Predictions unreliable.\n\n    -   **Check diagnostics before trusting any model.**\n\n-   Assumption 1: Linearity.\n\n    -   Assume relationship is linear.\n\n    -   Check with residual plot.\n\n        -   Good plot will have random scatter, points around 0, and constant spread.\n\n        -   Bad plot will have curved pattern (parabolic relationship), model is missing something, predictions are biased.\n\n        -   Linearity violations hurt predictions, not just inference:\n\n            -   If the true relationship is curved and a straight line is fitted, there will be a systematic underprediction in some regions and overprediction in otherss.\n\n            -   Biased predictions in predictable ways (not random errors).\n\n            -   Residual plots should show random scatter, any pattern means the model is missing something systematic.\n\n-   Assumption 2: Constant Variance.\n\n    -   **Heteroskedasticity:** Variance changes across X.\n\n    -   **Impact:** Standard errors are wrong, so p-values are misleading.\n\n    -   Heteroskedasticity is often a sympton of model misspecification.\n\n        -   Model fits well for some values, but poorly for other values (aggregation matters).\n\n        -   May indicate missing variables that matter more at certain X values.\n\n        -   Ask what's different about observations with large residuals.\n\n        -   Example: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\n    -   Key Insight: Large counties vary widely in income because some have high education and others have low education. Adding education as a predictor accounts for variation.\n\n    -   Key Insight: Adding the right predictor can fix heteroskedasticity.\n\n-   Formal Test: Breusch-Pagan\n\n    -   p \\> 0.05 means constant variance assumption is okay.\n\n    -   p \\< 0.05 means there's evidence of heteroskedasticity.\n\n    -   Solutions for heteroskedasticity:\n\n        1.  Transform Y (like log(income)).\n\n        2.  Robust standard errors.\n\n        3.  Add missing variables.\n\n        4.  Accept it (point predictions are still okay for prediction goals).\n\n-   Assumption 3: Normality of Residuals.\n\n    -   Assume residuals are normally distributed.\n\n    -   Matters because it's less critical for point predictions (unbiased regardless). It's important for confidence intervals and prediction intervals. It's needed for valid hypothesis tests (t-tests, F-tests).\n\n    -   Q-Q (Quantile-Quantile) plot of residuals to check.\n\n        -   Theoretical Quantiles on x and Sample Quantiles on y.\n\n-   Assumption 4: No Multicollinearity.\n\n    -   For multiple regression, predictors shouldn't be too correlated.\n\n    -   It matters because coefficients become unstable, and difficult to interpret.\n\n-   Assumption 5: No Influential Outliers.\n\n    -   **Not all outliers are problems, only those with high leverage AND large residuals.**\n\n        -   Pulls regression line.\n\n    -   Dealing with influential points:\n\n        1.  **Investigate** why this observation is unusual. Is it a data error or truly unique?\n\n        2.  **Report** the influential observations in analysis.\n\n        3.  **Sensitivity check.** Refit the model without the outlier(s), do the conclusions change?\n\n        4.  **Don't automatically remove outliers,** they might represent real, important cases.\n\n        For policy, an influential county might need special attention, *NOT* exclusion.\n\n6.  IMPROVING PREDICTIONS\n\n-   If a relationship is curved, try log transformations.\n\n    -   Log models show percentage relationships.\n\n-   Categorical variables, R creates dummy variables automatically.\n\nSUMMARY OF REGRESSION WORKFLOW\n\n```         \n1. **Understand the framework.** What's f? What's the goal?\n\n2. **Visualize first.** Does a linear model make sense?\n\n3. **Fit the model.** Estimate coefficients.\n\n4. **Evaluate performance.** Train/test split, cross-validation.\n\n5. **Check assumptions.** Residual plots, VIF, outliers.\n\n6. **Improve if needed.** Transformations, more variables.\n\n7. **Consider ethics.** Who could be harmed by this model?\n```\n\n## Coding Techniques\n\n-   **Train/Test Split**\n\n```{r}\n#| eval: false\n#| include: false\n\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n\n-   **Evaluate Predictions**\n\n```{r}\n#| eval: false\n#| include: false\n# Calculate prediction error (RMSE)\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\n\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n\n-   **Cross-Validation**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(caret)\n\n# 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\n\ncv_model$results\n```\n\n-   **Residual Plot**\n\n```{r}\n#| eval: false\n#| include: false\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n\n-   **Breusch-Pagan Formal Test**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(lmtest)\nbptest(model1)\n```\n\n-   **Multicollinearity**\n\n```{r}\n#| eval: false\n#| include: false\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n\n-   **Log Transformations**\n\n```{r}\n#| eval: false\n#| include: false\n# Compare linear vs log\nmodel_linear <- lm(median_incomeE ~ total_popE, data = pa_data)\nmodel_log <- lm(median_incomeE ~ log(total_popE), data = pa_data)\n\nsummary(model_log)\n```\n\n-   **Categorical Variables**\n\n```{r}\n#| eval: false\n#| include: false\n# Create metro/non-metro indicator\npa_data <- pa_data %>%\n  mutate(metro = ifelse(total_popE > 500000, 1, 0))\n\nmodel3 <- lm(median_incomeE ~ total_popE + metro, data = pa_data)\nsummary(model3)\n```\n\n## Questions & Challenges\n\n-   Public policy is very much a balancing act between technically and statistically good models vs. actually applying them to real-world policy without the regression causing any ethical consequences.\n\n## Connections to Policy\n\n-   Real-life healthcare algorithm model discriminated despite being technically good with good R\\^2 and low prediction error with a good fit. Ethically this ended up amplifying existing discrimination. **A model can be statistically \"good\" while being ethically terrible for decision-making.**\n\n-   Influential points, or outliers, have connections to algorithmic bias. High-influence observations could represent marginalized communities or unique populations. Removing them can erase important populations from analysis and lead to biased policy decisions.\n\n-   **ALWAYS INVESTIGATE OUTLIERS BEFORE REMOVING.**\n\n## Reflection\n\nKEY TAKEAWAYS\n\n-   Statistical Learning:\n\n    -   Estimating f(X), the systematic relationship.\n\n    -   Parametric methods assume a form (most choose linear).\n\n-   Two Purposes:\n\n    -   *Inference:* Understand relationships.\n\n    -   *Prediction:* Forecast new values.\n\n-   Model Evaluation:\n\n    -   In-sample fit is NOT equivalent to out-of-sample performance.\n\n    -   Beware of overfitting!\n\n-   Diagnostics Matter:\n\n    -   Always check assumptions.\n\n    -   Plots reveal what R\\^2 hides.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"embed-resources":true,"output-file":"week-05-notes.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"lux","title":"PREDICTIVE MODELING WITH LINEAR REGRESSION","subtitle":"WEEK 5 NOTES","date":"2025-10-06","author":[{"name":"Tess Vu","email":["tessavu@proton.me","tessavu@upenn.edu"],"corresponding":true}],"affiliation":[{"name":"University of Pennsylvania","department":"Urban Spatial Analytics (MUSA)","city":"Philadelphia","state":"PA","url":"https://www.design.upenn.edu/urban-spatial-analytics"}],"toc-location":"left","toc-expand":true,"smooth-scroll":true,"title-block-style":"default"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}