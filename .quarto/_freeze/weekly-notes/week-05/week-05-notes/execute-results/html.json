{
  "hash": "ceeaa577e53f2a03100c6220b90f078e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"WEEK 5 NOTES: PREDICTIVE MODELING WITH LINEAR REGRESSION\"\ndate: \"2025-10-06\"\n---\n\n## Key Concepts Learned\n\n1.  STATISTICAL LEARNING FRAMEWORK\n\n    -   **Statistical Learning:** Set of approaches for estimating relationships.\n\n    -   Formalizing relationships, where:\n\n        -   *Y = f(X) + ε*\n\n        -   **f:** The systematic information X provides about Y.\n\n        -   **ε:** Random error (irreducible).\n\n    -   **f represents the TRUE relationship between predictors and outcome.**\n\n        -   It's **FIXED** but **UNKNOWN**.\n\n        -   It's what people try to estimate.\n\n        -   Different X values produce different Y values through f.\n\n    -   Two reasons to estimate f:\n\n        1.  **Prediction**\n\n            -   Estimate Y for new observations.\n\n            -   Don't necessarily care about the exact form of f.\n\n            -   **Focus is on accuracy of predictions.**\n\n        2.  **Inference**\n\n            -   Understand how X affects Y.\n\n            -   *Which predictions matter?*\n\n            -   *What is the nature of the relationship?*\n\n            -   **Focus is on interpreting the model.**\n\n    -   How to estimate f? With two broad approaches:\n\n        -   **Parametric Methods**\n\n            -   Make an assumption about functional form (e.g. linear).\n\n            -   Reduces problem to estimating a few parameters.\n\n            -   Easier to interpret.\n\n            -   More common.\n\n        -   **Non-Parametric Methods**\n\n            -   Don't assume a specific form.\n\n            -   More flexible.\n\n            -   Requires more data.\n\n            -   Harder to interpret.\n\n        -   **KEY DIFFERENCE:** In *parametric* we assume f is linear, then estimate β₀ and β₁, etc. In *non-parametric* we let the data determine the shape of f.\n\n            -   **Deep Learning:** Neural networks are technically *parametric* (millions of parameters), but achieve flexibility through parameter quantity rather than assuming a rigid form.\n\n        -   **Linear Regression: Parametric**\n\n            -   **Assumption:** Relationship between X and Y is linear.\n\n            -   **Task:** Estimate the β coefficients using sample data.\n\n            -   **Method:** Ordinary Least Squares (OLS).\n\n            -   **Advantages:**\n\n                -   Simple and interpretable.\n\n                -   Well-understood properties.\n\n                -   Works remarkably well for many problems.\n\n                -   Foundation for more complex methods.\n\n            -   **Disadvantages:**\n\n                -   Assumes linearity.\n\n                -   Sensitive to outliers.\n\n                -   Makes several assumptions.\n\n2.  TWO GOALS\n\n-   Understanding Relationships vs. Making Predictions: Same model serves different purposes.\n\n    -   **Inference**\n\n        -   \"Does education affect income?\"\n\n        -   Focus on coefficients.\n\n        -   Statistical significance matters.\n\n        -   Understand mechanisms and what the coefficients, or predictors, tell us.\n\n    -   **Prediction**\n\n        -   \"What's county Y's income?\"\n\n        -   Focus on accuracy.\n\n        -   Prediction intervals matter.\n\n        -   Don't need to understand why certain relationships exist.\n\n3.  MODEL BUILDING\n\n-   Considerable scatter can generally mean the relationships aren't deterministic, but rather more probablistic or stochastic.\n\n-   Interpreting coefficients example:\n\n    -   Intercept (β₀) = \\$62,855\n\n        -   Expected income when population = 0.\n\n        -   Not usually meaningful in practice.\n\n    -   Slope (β₁) = \\$0.02\n\n        -   For each additional person, income increases by \\$0.02.\n\n        -   This is more useful, because for every 1,000 people, income increases by \\$20.\n\n    -   p-value \\<0.001 means it's very unlikely to see this if true, when β₁ = 0.\n\n    -   Can reject the null hypothesis.\n\n-   **Holy Grail Concept:** Estimates are just estimates of the true unknown parameters.\n\n-   Different samples produce different linear regression lines, but no matter how many samples and statistical tests, the true relationship is unknowable.\n\n-   Standard errors quantify the above uncertainty.\n\n-   Statistical significance:\n\n    1.  **Null Hypothesis (H₀):** β₁ = 0 (no relationship).\n\n    2.  **Our Estimate:** β₁ = 0.02.\n\n    3.  **Question:** Could we get \\$0.02 just by chance if H₀ is true?\n\n    -   **t-statistic:** How many standard errors away from 0?\n\n        -   **Bigger \\|t\\| means more confidence that the relationship is real.**\n\n    -   **p-value:** Probability of seeing our estimate if H₀ is true.\n\n        -   **Small p, reject H₀, conclude relationship exists.**\n\n        -   **Large p, fail to reject H₀, conclude that there's a possibility no relationship exists.**\n\n4.  MODEL EVALUATION\n\n-   Two key questions:\n\n    1.  **How well does the data used fit?** (in-sample fit)\n\n    2.  **How well would it predict new data?** (out-of-sample performance)\n\n    -   NOT THE SAME.\n\n    -   **In-Simple Fit: R\\^2**\n\n        -   R\\^2 = 0.208 means \"21% of variation in income is explained by population.\"\n\n        -   **Is this good? It depends on the goal. For prediction, it's moderately good. For inference, it shows population matters, but other factors exist.**\n\n        -   **R\\^2 alone doesn't tell if the model is trustworthy.**\n\n    -   **Overfitting Problem**\n\n        1.  **Underfitting:** Model is too simple (high bias).\n\n        2.  **Good Fit:** Captures pattern without noise.\n\n        3.  **Overfitting:** Memorizes training data (high variance).\n\n        -   **DANGER: High R\\^2 doesn't mean good predictions!**\n\n        -   Overfit regression means it can't be applied to other samples and is too aligned with the data samples it was tested and derived from.\n\n        -   70% training, so fit on training data only. 30% testing, so predict on test data after getting the fit on the training data.\n\n    -   **RMSE:** A number of 9,536 means that on new data (test set), predictions are off by \\~\\$9,500 on average. Is this level of error acceptable for policy decisions?\n\n    -   **Cross-Validation:** A better approach doing multiple training and testing splits that gives more stable estimate of true prediction performance. Average the multiple RMSEs.\n\n5.  CHECKING ASSUMPTIONS\n\n-   Linear regression makes assumptions, if violated:\n\n    -   Coefficients may be biased.\n\n    -   Standard errors are wrong.\n\n    -   Predictions unreliable.\n\n    -   **Check diagnostics before trusting any model.**\n\n-   Assumption 1: Linearity.\n\n    -   Assume relationship is linear.\n\n    -   Check with residual plot.\n\n        -   Good plot will have random scatter, points around 0, and constant spread.\n\n        -   Bad plot will have curved pattern (parabolic relationship), model is missing something, predictions are biased.\n\n        -   Linearity violations hurt predictions, not just inference:\n\n            -   If the true relationship is curved and a straight line is fitted, there will be a systematic underprediction in some regions and overprediction in otherss.\n\n            -   Biased predictions in predictable ways (not random errors).\n\n            -   Residual plots should show random scatter, any pattern means the model is missing something systematic.\n\n-   Assumption 2: Constant Variance.\n\n    -   **Heteroskedasticity:** Variance changes across X.\n\n    -   **Impact:** Standard errors are wrong, so p-values are misleading.\n\n    -   Heteroskedasticity is often a sympton of model misspecification.\n\n        -   Model fits well for some values, but poorly for other values (aggregation matters).\n\n        -   May indicate missing variables that matter more at certain X values.\n\n        -   Ask what's different about observations with large residuals.\n\n        -   Example: Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\n    -   Key Insight: Large counties vary widely in income because some have high education and others have low education. Adding education as a predictor accounts for variation.\n\n    -   Key Insight: Adding the right predictor can fix heteroskedasticity.\n\n-   Formal Test: Breusch-Pagan\n\n    -   p \\> 0.05 means constant variance assumption is okay.\n\n    -   p \\< 0.05 means there's evidence of heteroskedasticity.\n\n    -   Solutions for heteroskedasticity:\n\n        1.  Transform Y (like log(income)).\n\n        2.  Robust standard errors.\n\n        3.  Add missing variables.\n\n        4.  Accept it (point predictions are still okay for prediction goals).\n\n-   Assumption 3: Normality of Residuals.\n\n    -   Assume residuals are normally distributed.\n\n    -   Matters because it's less critical for point predictions (unbiased regardless). It's important for confidence intervals and prediction intervals. It's needed for valid hypothesis tests (t-tests, F-tests).\n\n    -   Q-Q (Quantile-Quantile) plot of residuals to check.\n\n        -   Theoretical Quantiles on x and Sample Quantiles on y.\n\n-   Assumption 4: No Multicollinearity.\n\n    -   For multiple regression, predictors shouldn't be too correlated.\n\n    -   It matters because coefficients become unstable, and difficult to interpret.\n\n-   Assumption 5: No Influential Outliers.\n\n    -   **Not all outliers are problems, only those with high leverage AND large residuals.**\n\n        -   Pulls regression line.\n\n    -   Dealing with influential points:\n\n        1.  **Investigate** why this observation is unusual. Is it a data error or truly unique?\n\n        2.  **Report** the influential observations in analysis.\n\n        3.  **Sensitivity check.** Refit the model without the outlier(s), do the conclusions change?\n\n        4.  **Don't automatically remove outliers,** they might represent real, important cases.\n\n        For policy, an influential county might need special attention, *NOT* exclusion.\n\n6.  IMPROVING PREDICTIONS\n\n-   If a relationship is curved, try log transformations.\n\n    -   Log models show percentage relationships.\n\n-   Categorical variables, R creates dummy variables automatically.\n\nSUMMARY OF REGRESSION WORKFLOW\n\n```         \n1. **Understand the framework.** What's f? What's the goal?\n\n2. **Visualize first.** Does a linear model make sense?\n\n3. **Fit the model.** Estimate coefficients.\n\n4. **Evaluate performance.** Train/test split, cross-validation.\n\n5. **Check assumptions.** Residual plots, VIF, outliers.\n\n6. **Improve if needed.** Transformations, more variables.\n\n7. **Consider ethics.** Who could be harmed by this model?\n```\n\n## Coding Techniques\n\n-   **Train/Test Split**\n\n\n\n-   **Evaluate Predictions**\n\n\n\n-   **Cross-Validation**\n\n\n\n-   **Residual Plot**\n\n\n\n-   **Breusch-Pagan Formal Test**\n\n\n\n-   **Multicollinearity**\n\n\n\n-   **Log Transformations**\n\n\n\n-   **Categorical Variables**\n\n\n\n## Questions & Challenges\n\n-   Public policy is very much a balancing act between technically and statistically good models vs. actually applying them to real-world policy without the regression causing any ethical consequences.\n\n## Connections to Policy\n\n-   Real-life healthcare algorithm model discriminated despite being technically good with good R\\^2 and low prediction error with a good fit. Ethically this ended up amplifying existing discrimination. **A model can be statistically \"good\" while being ethically terrible for decision-making.**\n\n-   Influential points, or outliers, have connections to algorithmic bias. High-influence observations could represent marginalized communities or unique populations. Removing them can erase important populations from analysis and lead to biased policy decisions.\n\n-   **ALWAYS INVESTIGATE OUTLIERS BEFORE REMOVING.**\n\n## Reflection\n\nKEY TAKEAWAYS\n\n-   Statistical Learning:\n\n    -   Estimating f(X), the systematic relationship.\n\n    -   Parametric methods assume a form (most choose linear).\n\n-   Two Purposes:\n\n    -   *Inference:* Understand relationships.\n\n    -   *Prediction:* Forecast new values.\n\n-   Model Evaluation:\n\n    -   In-sample fit is NOT equivalent to out-of-sample performance.\n\n    -   Beware of overfitting!\n\n-   Diagnostics Matter:\n\n    -   Always check assumptions.\n\n    -   Plots reveal what R\\^2 hides.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}